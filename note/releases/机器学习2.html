<!DOCTYPE html>
  <html>
    <head>
      <title>机器学习2</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="file:///C:\Users\qhy28\.atom\packages\markdown-preview-enhanced\node_modules\@shd101wyy\mume\dependencies\mathjax\MathJax.js"></script>
        
      

      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;padding:2em;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}@media screen and (min-width:914px){html body:not([data-presentation-mode]){width:980px;margin:10px auto}}@media screen and (max-width:400px){html body:not([data-presentation-mode]){font-size:14px;margin:0 auto;padding:15px}}html body .pagebreak,html body .newpage{page-break-before:always}html body pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}html body pre.line-numbers>code{position:relative}html body pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}html body pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}html body pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}html body .mathjax-exps .MathJax_Display{text-align:center !important}html body:not([for="preview"]) .code-chunk .btn-group{display:none}html body:not([for="preview"]) .code-chunk .status{display:none}html body:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px} 
       
      </style>
    </head>
    <body class="mume   ">
    <ul>
<li><a href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C">神经网络</a>
<ul>
<li><a href="#%E6%84%9F%E7%9F%A5%E6%9C%BA-perceptron">感知机 Perceptron</a></li>
<li><a href="#%E8%AF%AF%E5%B7%AE%E9%80%86%E4%BC%A0%E6%92%AD%E7%AE%97%E6%B3%95-bp%E7%AE%97%E6%B3%95">误差逆传播算法 BP算法</a></li>
</ul>
</li>
<li><a href="#%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8">自编码器</a>
<ul>
<li><a href="#%E6%AC%A0%E5%AE%8C%E5%A4%87%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8">欠完备自编码器</a></li>
<li><a href="#%E6%AD%A3%E5%88%99%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8">正则自编码器</a>
<ul>
<li><a href="#%E7%A8%80%E7%96%8F%E8%87%AA%E7%BC%96%E7%A0%81%E5%99%A8">稀疏自编码器</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<h1 class="mume-header" id="&#x795E;&#x7ECF;&#x7F51;&#x7EDC;">神经网络</h1>

<p>神经网络中的最基本的成分是 <strong>神经元</strong> 。<br>
多个神经元按一定层次连接起来,组成神经网络。</p>
<p>M-P神经元模型:<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02-ea5cd.png" alt="1"><br>
神经元接受来自n个其他神经元传递过来的输入信号,这些输入信号通过带权重的连接(connection)进行传递,神经元接受到的总输入值将与神经元的阈值进行比较,然后通过 <strong>激活函数</strong> 处理以产生神经元的输出。</p>
<p>常用 <strong>Sigmod</strong> 函数作为神经元的激活函数。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02-00782.png" alt="2"></p>
<h2 class="mume-header" id="&#x611F;&#x77E5;&#x673A;-perceptron">感知机 Perceptron</h2>

<p>感知机由两层神经元组成,接收层接受外界输入信号后传递给输出层,输出层是 M-P神经元。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02-accbb.png" alt="3"></p>
<h2 class="mume-header" id="&#x8BEF;&#x5DEE;&#x9006;&#x4F20;&#x64AD;&#x7B97;&#x6CD5;-bp&#x7B97;&#x6CD5;">误差逆传播算法 BP算法</h2>

<p>给定如图的神经网络:<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02-f61eb.png" alt="4"></p>
<p>输入层有d个神经元<br>
隐层维有q个神经元<br>
激活函数为<span class="mathjax-exps">$f(x) = sigmoid(x) = &#x5C;frac{1}{1 + e^{-x}}$</span></p>
<p>第i个输入层神经元和第h个隐层神经元的连接权为<span class="mathjax-exps">$v_{ih}$</span><br>
第h个隐层神经元的输入:<span class="mathjax-exps">$&#x5C;alpha_h = &#x5C;displaystyle{&#x5C;sum_{i = 1}^d} v_{ih}x_i$</span><br>
第h个隐层神经元的阈值为<span class="mathjax-exps">$&#x5C;gamma_{h}$</span><br>
则隐层神经元的输出为:<span class="mathjax-exps">$b_h = f(&#x5C;alpha_h - &#x5C;gamma_h)$</span></p>
<p>隐层第h个神经元和输出层第j个神经元的连接权为:<span class="mathjax-exps">$w_{hj}$</span><br>
第j个输出层神经元的输入为:<span class="mathjax-exps">$&#x5C;beta_j = &#x5C;displaystyle{&#x5C;sum_{h = 1}^q} w_{hj}b_h$</span><br>
第j个输出层神经元的阈值为:<span class="mathjax-exps">$&#x5C;theta_j$</span><br>
则输出层第j个神经元的输出为:<span class="mathjax-exps">$&#x5C;hat{y_j} = f(&#x5C;beta_j - &#x5C;theta_j)$</span></p>
<p>定义一个样本为(x,y),<span class="mathjax-exps">$x &#x5C;in &#x5C;mathbb{R}^d , y &#x5C;in &#x5C;mathbb{R}^l$</span><br>
我们使用均方误差作为损失函数,<span class="mathjax-exps">$&#x5C;hat y_j$</span>为输出层第j个神经元的输出结果,则<br>
<span class="mathjax-exps">$E = &#x5C;frac{1}{2}&#x5C;displaystyle{&#x5C;sum_{j = 1}^l} (&#x5C;hat y_j - y_j)^2$</span></p>
<p>这里的<span class="mathjax-exps">$&#x5C;frac{1}{2}$</span>是为了后面求导方便。<br>
我们的目标就是最小化E,<br>
使用梯度下降算法,即<span class="mathjax-exps">$v &#x5C;gets v + &#x5C;Delta v$</span></p>
<blockquote>
<p>sigmoid函数有一个很好的性质:<span class="mathjax-exps">$f&#x27;(x) = f(x)&#x5C;big (1 - f(x)&#x5C;big )$</span></p>
</blockquote>
<ol>
<li>求<span class="mathjax-exps">$&#x5C;Delta w_{hj}$</span><br>
<span class="mathjax-exps">$&#x5C;Delta w_{hj} = -&#x5C;eta &#x5C;frac{&#x5C;partial E}{&#x5C;partial w_{hj}} = &#x5C;eta g_j b_h$</span></li>
</ol>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial E}{&#x5C;partial w_{hj}} &amp;= &#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;hat y_j} &#x5C;frac{&#x5C;partial &#x5C;hat y_j}{&#x5C;partial &#x5C;beta_j} &#x5C;frac{&#x5C;partial &#x5C;beta_j}{&#x5C;partial w_{hj}} &amp;(1) &#x5C;&#x5C;&amp;= &#x5C;hat y_j (1 - &#x5C;hat y_j)(y_j - &#x5C;hat y_j) b_h &amp;(2)&#x5C;&#x5C;&amp;= -g_j b_h &amp;(3)&#x5C;end{align}$</span></p>
<p>(1):链式法则<br>
(2):展开导数</p>
<blockquote>
<p><span class="mathjax-exps">$&#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;hat y} = (&#x5C;hat y - y)$</span><br>
<span class="mathjax-exps">$&#x5C;frac{&#x5C;partial f(x)}{&#x5C;partial x} =  f(x)&#x5C;big (1 - f(x)&#x5C;big )$</span><br>
<span class="mathjax-exps">$&#x5C;frac{&#x5C;partial &#x5C;beta_j}{&#x5C;partial w_{hj}} = b_h$</span></p>
</blockquote>
<p>(3):定义符号<span class="mathjax-exps">$g_j$</span><br>
<span class="mathjax-exps">$&#x5C;begin{align}g_j &amp;= - &#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;hat y} &#x5C;frac{&#x5C;partial &#x5C;hat y}{&#x5C;partial &#x5C;beta_j}&#x5C;&#x5C;&amp;= -(&#x5C;hat y_j - y_j)f&#x27;(&#x5C;beta_j - &#x5C;theta_j)&#x5C;&#x5C;&amp;= &#x5C;hat y_j(1 - &#x5C;hat y_j)(y_j - &#x5C;hat y_j)&#x5C;end{align}$</span></p>
<p>则<span class="mathjax-exps">$&#x5C;Delta w_{hj} = &#x5C;eta g_j b_n$</span></p>
<ol start="2">
<li>求<span class="mathjax-exps">$&#x5C;Delta &#x5C;theta_j$</span><br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;theta_j} &amp;=&#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;hat y_j} &#x5C;frac{&#x5C;partial &#x5C;hat y_j}{&#x5C;partial &#x5C;theta_j} &#x5C;&#x5C;&amp;= g_j&#x5C;end{align}$</span></li>
</ol>
<p>则<span class="mathjax-exps">$&#x5C;Delta &#x5C;theta_j = -&#x5C;eta g_j$</span></p>
<ol start="3">
<li>求<span class="mathjax-exps">$&#x5C;Delta v_{ih}$</span><br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial E}{&#x5C;partial v_{ih}} &amp;=&#x5C;displaystyle{&#x5C;sum_{j = 1}^l} &#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;hat y_j} &#x5C;frac{&#x5C;partial &#x5C;hat y_j}{&#x5C;partial &#x5C;beta_j}&#x5C;frac{&#x5C;partial &#x5C;beta_j }{&#x5C;partial b_h} &#x5C;frac{&#x5C;partial b_h }{&#x5C;partial &#x5C;alpha_h}&#x5C;frac{&#x5C;partial &#x5C;alpha_h}{&#x5C;partial v_{ih}}&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^l} -g_j w_{hj} b_h(1-b_h)x_i &#x5C;&#x5C;&amp;= -x_ib_h(1-b_h) &#x5C;displaystyle{&#x5C;sum_{j = 1}^l} - g_j w_{hj} &#x5C;&#x5C;&amp;= -x_ie_h&#x5C;end{align}$</span></li>
</ol>
<p><span class="mathjax-exps">$e_h = b_h(1-b_h) &#x5C;displaystyle{&#x5C;sum_{j = 1}^l} -g_j w_{hj} = &#x5C;frac{&#x5C;partial E}{&#x5C;partial b_h}&#x5C;frac{&#x5C;partial b_h}{&#x5C;partial &#x5C;alpha_h}$</span></p>
<p>则<span class="mathjax-exps">$&#x5C;Delta v_{ih} = &#x5C;eta e_h x_i$</span></p>
<ol start="4">
<li>求<span class="mathjax-exps">$&#x5C;Delta &#x5C;gamma_h$</span><br>
<span class="mathjax-exps">$&#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;gamma_h} = &#x5C;displaystyle{&#x5C;sum_{j = 1}^l} &#x5C;frac{&#x5C;partial E}{&#x5C;partial &#x5C;hat y_j} &#x5C;frac{&#x5C;partial &#x5C;hat y_j}{&#x5C;partial &#x5C;beta_j}&#x5C;frac{&#x5C;partial &#x5C;beta_j }{&#x5C;partial b_h} &#x5C;frac{&#x5C;partial b_h }{&#x5C;partial &#x5C;gamma_h} = e_h$</span></li>
</ol>
<p>则<span class="mathjax-exps">$&#x5C;Delta &#x5C;gamma_h = - &#x5C;eta e_h$</span></p>
<p>算法过程为:<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02-39fc0.png" alt="5"><br>
第4行为正向传播,正向激活神经元<br>
第4第5步为误差反向传播<br>
第6步根据反向传播回来的误差进行参数的更新</p>
<p>由隐层神经元的梯度<span class="mathjax-exps">$e_h$</span>的表达式可以看出<span class="mathjax-exps">$e_h$</span>  和输出层神经元的梯度<span class="mathjax-exps">$g_j$</span>有关,那么进行计算的过程,可以看成是把输出层的误差,反向传播回到了隐层。</p>
<h1 class="mume-header" id="&#x81EA;&#x7F16;&#x7801;&#x5668;">自编码器</h1>

<p>自编码器也是神经网络的一种,经过训练后尝试将输入复制到输出。<br>
&quot;复制&quot;过程可以分解为&quot;编码&quot;和&quot;解码&quot;两个过程,希望经过这两个过程之后,提取出输入最显著的特征作为输出。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A02-39013.png" alt="6"></p>
<p>从代价函数的角度来看,我们的目标就是最小化一下这个代价函数:<br>
<span class="mathjax-exps">$L(x,g&#x5C;big (f(x) &#x5C;big ))$</span></p>
<p>一个特殊的情况是,把输入完全复制到了输出而不没有任何的变化。</p>
<p>这显然是无用,因此我们需要添加一些约束条件来使得输出r只能近似等于x,而不是完全等于x。</p>
<h2 class="mume-header" id="&#x6B20;&#x5B8C;&#x5907;&#x81EA;&#x7F16;&#x7801;&#x5668;">欠完备自编码器</h2>

<p>欠完备自编码器的思想是 限制隐层神经元的数目,使得中间结果h的维度小于样本的维度。这样就会丢失部分信息,而使得输出r不能完全等于x</p>
<h2 class="mume-header" id="&#x6B63;&#x5219;&#x81EA;&#x7F16;&#x7801;&#x5668;">正则自编码器</h2>

<p>若要求隐层的容量(h的维度)大于输入x,若不加以约束,更大可能的学习结果是完全复制,因此在代价函数中,还需要加入一个正则项加以约束。</p>
<h3 class="mume-header" id="&#x7A00;&#x758F;&#x81EA;&#x7F16;&#x7801;&#x5668;">稀疏自编码器</h3>


    </body>
    
    
    <script>
(function bindTaskListEvent() {
  var taskListItemCheckboxes = document.body.getElementsByClassName('task-list-item-checkbox')
  for (var i = 0; i < taskListItemCheckboxes.length; i++) {
    var checkbox = taskListItemCheckboxes[i]
    var li = checkbox.parentElement
    if (li.tagName !== 'LI') li = li.parentElement
    if (li.tagName === 'LI') {
      li.classList.add('task-list-item')
    }
  }
}())    
</script>
  </html>