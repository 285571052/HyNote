# 模式识别导论

## Bayesian Decision Theory

1. Prior probability:ignore present state

    $$p(x)$$

2. Conditional probability:also called likelihood

    $$p(x|w)$$

    ![](assets/moshishibiedaolun/2018-03-17-15-17-10.png)

3. Posterior probability

    Bayes Formula:

    $$p(w|x)= \frac{p(x | w)p(w)}{p(x)}$$

### Bayes decision rule

* Decide $w_1$ if $p(w_2 | x) > p(w_2|x)$
* Decide $w_2$ if $p(w_1 | x) > p(w_1|x)$

or equivalent to(ignore $p(x)$)

* Decide $w_1$ if $p(w_2 | x) > p(x | w_2)p(w_2)$
* Decide $w_2$ if $p(w_1 | x) > p(x | w_1)p(w_1)$

ispecial case when $p(w_1) = p(w_2)$

* Decide $w_1$ if $p(w_2 | x) > p(x | w_2)$
* Decide $w_2$ if $p(w_1 | x) > p(x | w_1)$

![](assets/moshishibiedaolun/2018-03-17-15-51-16.png)

![](assets/moshishibiedaolun/2018-03-17-16-49-30.png)

### Probability of Error

There are two possible errors
<img src="assets/moshishibiedaolun/2018-03-17-15-53-00.png" width = 40%>

Probability of error is
$p(error|x) = p(w_1|x)$ if decide $w_2$
$p(error|x) = p(w_2|x)$ if decide $w_1$

equivalent to

$p(error | x) = min[p(w_1|x) , p(w_2|x)]$

> Bayes rule or Bayes classifier:
> decide $w_1$,if $p(w_1 | x) > p(w_2|x)$ , otherwise decide $w_2$

> 这里错误的概率,是指当我们预测为$w_i$的时候,错误的概率,由上图可以看出,预测为第二类错误的概率是第二列红色部分
> 而我们预测为$w_2$是因为$p(w_2|x)$最大, 即预测为$w_2$的正确率最大
> 那么对应的错误率自然是$1 - p(x_2|x)$ , 即下面多分类问题中的$max$

### Multi-class problem

Bayes rule:
select $w_i$ if $p(w_i|x) > p(w_j|x)$ for all $j \neq i$

Probability of Error:
$$p(error|x) = 1 - max[p(w_1|x),...,p(w_c|x)]$$

### Bayes Error Rate

$$ \begin{array}{rl}
p(error) &= p(x\in R_2 , w_1) + p(x \in R_1 , w_2)\\
&= p(x\in R_2|w_1)p(w_1) + p(x\in R_2|w_2)p(w_2)\\
&= \int_{R_2} p(x\in R_2|w_1)p(w_1) dx + \int_{R_1} p(x\in R_2|w_2)p(w_2) dx
\end{array}$$

![](assets/moshishibiedaolun/2018-03-17-16-23-59.png)

> 从上图可以看出, 当我们采取Bayes decision rule(图中的$x_a'$ , 两条曲线的交点)的时候 , $p(error)$不一定是最小的
> 因此需要确定决策边界(threshold),使得$p(error)$最小
> 为此需要计算$p(x|w)$和$p(w)$

### Loss function

Loss : measure the cost for each action taken

Let
$\{w_1 , w_2 , ... , w_c\}$ be the set of $c$ categories
$\{\alpha_1 , \alpha_2 , ... , \alpha_a\}$ be the set of possible actions
$\lambda_{ij} = \lambda(\alpha_i|w_j)$ be the loss incurred for taking action $\alpha_i$ when the category is $w_j$

> 在真值为$w_j$下采取$\alpha_i$造成的损失

The expected loss associated with taking action $\alpha_i$ is Conditional risk
$$R(\alpha_i|x) = \sum_{j = 1}^c \lambda_{ij}p(w_j|x)$$

and overall risk
$$\mathcal{R} = \int R(a(x) | x)p(x) dx$$

Choose $\alpha_i$ so that $R(\alpha_i|x)$ is as small as possible for every $x$ , then the overall risk will be minimized

#### Minimum Risk Decision Rule

take action $\alpha_1$ if
$$R(\alpha_1|x)<R(\alpha_2|x)$$

equivalent to :

decide $w_1$ if
$$(\lambda_{21} - \lambda_{11})p(x|w_1)p(w_2) > (\lambda_{12} - \lambda_{22})p(x|w_2)p(w_2)$$
otherwise ,declde $w_2$

> 作差之后,可以看成是预测正确的损失为1, 而预测错误的时候 ,带权计算误差 , 下面可以看出 minimum Risk Decision Rule 范围更广

**Zero-one loss function** :
$$\lambda(\alpha_i|w_j) = \left \{ \begin{array}{rl}0 & i = j \\ 1 & i \neq j \end{array} \right .$$

* No loss to a correct decision
* A unit loss to any error
* All erroes are equally costly

Risk:
$$\begin{array}{rl}R(\alpha_i|x) &= \sum_{j = 1}^c \lambda_{ij}p(w_j|x)\\&= \sum_{j \neq i} p(w_j|x) \\&= 1-p(w_i|x) \end{array}$$

So minimizing the average probability of error requires maximization $p(w_i|x)$

> 前面bayes rule , 我们选择后验概率最大的$w_i$作为参数的估计(似然概率最大的$w_i$) , 而这个部分解释了为什么要这么做 , 因为这么取造成的损失的期望是最小的

> 而后面的决策边界 , 则是在确定了$w_i$后, 作决策所用 , 选择一个决策边界使得错误率最小 , 这一步的操作是不会对损失的期望造成影响,因为$w_i$下,已经是最小的

Practice:
![](assets/moshishibiedaolun/2018-03-17-16-49-55.png)

