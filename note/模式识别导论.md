# 模式识别导论

## Bayesian Decision Theory

1. Prior probability:ignore present state

    $$p(x)$$

2. Conditional probability:also called likelihood

    $$p(x|w)$$

    ![](assets/moshishibiedaolun/2018-03-17-15-17-10.png)

3. Posterior probability

    Bayes Formula:

    $$p(w|x)= \frac{p(x | w)p(w)}{p(x)}$$

### Bayes decision rule

* Decide $w_1$ if $p(w_1 | x) > p(w_2|x)$
* Decide $w_2$ if $p(w_2 | x) > p(w_1|x)$

or equivalent to(ignore $p(x)$)

* Decide $w_1$ if $ p(x | w_1)p(w_1) > p(x | w_2)p(w_2)$
* Decide $w_2$ if $ p(x | w_2)p(w_2) > p(x | w_1)p(w_1)$

ispecial case when $p(w_1) = p(w_2)$

* Decide $w_1$ if $p(x | w_1) > p(x | w_2)$
* Decide $w_2$ if $p(x | w_2)> p(x | w_1)$

![](assets/moshishibiedaolun/2018-03-17-15-51-16.png)

![](assets/moshishibiedaolun/2018-03-17-16-49-30.png)

### Probability of Error

There are two possible errors
<img src="assets/moshishibiedaolun/2018-03-17-15-53-00.png" width = 40%>

Probability of error is
$p(error|x) = p(w_1|x)$ if decide $w_2$
$p(error|x) = p(w_2|x)$ if decide $w_1$

equivalent to

$p(error | x) = min[p(w_1|x) , p(w_2|x)]$

> Bayes rule or Bayes classifier:
> decide $w_1$,if $p(w_1 | x) > p(w_2|x)$ , otherwise decide $w_2$

> 这里错误的概率,是指当我们预测为$w_i$的时候,错误的概率,由上图可以看出,预测为第二类错误的概率是第二列红色部分
> 而我们预测为$w_2$是因为$p(w_2|x)$最大, 即预测为$w_2$的正确率最大
> 那么对应的错误率自然是$1 - p(x_2|x)$ , 即下面多分类问题中的$max$

### Multi-class problem

Bayes rule:
select $w_i$ if $p(w_i|x) > p(w_j|x)$ for all $j \neq i$

Probability of Error:
$$p(error|x) = 1 - max[p(w_1|x),...,p(w_c|x)]$$

### Bayes Error Rate

$$ \begin{array}{rl}
p(error) &= p(x\in R_2 , w_1) + p(x \in R_1 , w_2)\\
&= p(x\in R_2|w_1)p(w_1) + p(x\in R_2|w_2)p(w_2)\\
&= \int_{R_2} p(x\in R_2|w_1)p(w_1) dx + \int_{R_1} p(x\in R_2|w_2)p(w_2) dx
\end{array}$$

![](assets/moshishibiedaolun/2018-03-17-16-23-59.png)

> 从上图可以看出, 当我们采取Bayes decision rule(图中的$x_a'$ , 两条曲线的交点)的时候 , $p(error)$不一定是最小的
> 因此需要确定决策边界(threshold),使得$p(error)$最小
> 为此需要计算$p(x|w)$和$p(w)$

### Loss function

Loss : measure the cost for each action taken

Let
$\{w_1 , w_2 , ... , w_c\}$ be the set of $c$ categories
$\{\alpha_1 , \alpha_2 , ... , \alpha_a\}$ be the set of possible actions
$\lambda_{ij} = \lambda(\alpha_i|w_j)$ be the loss incurred for taking action $\alpha_i$ when the category is $w_j$

> 在真值为$w_j$下采取$\alpha_i$造成的损失

The expected loss associated with taking action $\alpha_i$ is Conditional risk
$$R(\alpha_i|x) = \sum_{j = 1}^c \lambda_{ij}p(w_j|x)$$

and overall risk
$$\mathcal{R} = \int R(a(x) | x)p(x) dx$$

Choose $\alpha_i$ so that $R(\alpha_i|x)$ is as small as possible for every $x$ , then the overall risk will be minimized

#### Minimum Risk Decision Rule

take action $\alpha_1$ if
$$R(\alpha_1|x)<R(\alpha_2|x)$$

equivalent to :

decide $w_1$ if
$$(\lambda_{21} - \lambda_{11})p(x|w_1)p(w_2) > (\lambda_{12} - \lambda_{22})p(x|w_2)p(w_2)$$
otherwise ,declde $w_2$

> 作差之后,可以看成是预测正确的损失为1, 而预测错误的时候 ,带权计算误差 , 下面可以看出 minimum Risk Decision Rule 范围更广

**Zero-one loss function** :
$$\lambda(\alpha_i|w_j) = \left \{ \begin{array}{rl}0 & i = j \\ 1 & i \neq j \end{array} \right .$$

* No loss to a correct decision
* A unit loss to any error
* All erroes are equally costly

Risk:
$$\begin{array}{rl}R(\alpha_i|x) &= \sum_{j = 1}^c \lambda_{ij}p(w_j|x)\\&= \sum_{j \neq i} p(w_j|x) \\&= 1-p(w_i|x) \end{array}$$

So minimizing the average probability of error requires maximization $p(w_i|x)$

> 前面bayes rule , 我们选择后验概率最大的$w_i$作为参数的估计(似然概率最大的$w_i$) , 而这个部分解释了为什么要这么做 , 因为这么取造成的损失的期望是最小的

> 而后面的决策边界 , 则是在确定了$w_i$后, 作决策所用 , 选择一个决策边界使得错误率最小 , 这一步的操作是不会对损失的期望造成影响,因为$w_i$下,已经是最小的

Practice:
![](assets/moshishibiedaolun/2018-03-17-16-49-55.png)

### Classfiers

Discriminant functions :
$$g_(x) = , i = 1 , \cdots , c$$

Decision Rule: x will be assigned to class $w_i$ if $g_i(x) > g_j(x) , \forall j \neq i$

**For Minium-Error-Rate:**
$$g_i(x) = p(w_i|x)$$

if $G$ is a monotonically increasing functions , the decision rule can also be :
Decision Rule: x will be assigned to class $w_i$ if $G(g_i(x)) > G(g_j(x)) , \forall j \neq i$

for example $log$
$$G(g_i(x)) = \ln (g_i(x)) = \ln p(x|w_i) + \ln p(w_i) - ln p(x)$$

**For tow class problem**:
> 二分类问题 , 判别函数作差

$$g(x) = g_1(x) - g_2(x) = \ln \frac{p(x|w_1)}{p(x|w_2)} + \ln \frac{p(w_1)}{p(w_2)}$$

> 左边这个形式就是逻辑回归的表达式 , 即逻辑回归假设$p(w_1) = p(w_2)$
> 由于在样本少的情况下, 贝叶斯方法并不能准确估计 , 因此采用极大似然法(ML) , 即和逻辑回归的表示对应上

### 特例:正态分布

Advantage of Normal Distribute:

1. Analytically traceable continuous
2. A lot of processes are asymptotically Gaussian

Definition:
$$p(x) = \frac{1}{(2\pi)^{\frac{d}{2}} |\Sigma|^\frac{1}{2}}exp\left (-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right )$$

Assume $p(x|w_i) \sim N(\mu_i , \Sigma_i)$
and use $G(g_i(x)) = \ln (g_i(x)) = \ln p(x|w_i) + \ln p(w_i) - ln p(x)$ as discriminant function,
$$g_i(x) = -\frac{1}{2}(x - \mu_i)^T\Sigma_i^{-1}(x - \mu_i) - \frac{d}{2}\ln (2\pi) - \frac{1}{2} \ln |\Sigma_i| + \ln p(w_i)$$

改写成以下形式:
$$g_i(x) = x^TW_ix + w_ix + w_{i0}$$
其中,$W_i = -\frac{1}{2}\Sigma_i^{-1}$,
$w_i = \Sigma_i^{-1}\mu_i$,
$w_{i0} = -\frac{1}{2}\mu^T\Sigma_i^{-1}\mu - \frac{1}{2}\ln |\Sigma_i| + \ln p(w_i)$

#### Estimation on $p(x| w_i)$

how can we obtain them?

* can be estimated by using training samples
* No problem with estimation of $p(w_i)$
* Sample are often too small for the estimation of $p(x | w_i)$
* even worse for large dimension of feature space

one possible solution for estimating $p(x|w_i)$
assume known forms of distribution, such as normal distribution
$$p(x|w_i) \sim N(\mu_i , \Sigma_i)$$

the advantage of assuming a certain distribution form is that **the problem is now simplified from estimating unknown posterior function to one estimating parameters of a konw distribution function**

**ML Parameter Estimation**

> ML & Bayesian : two common parameter estimation methods
> Parameter such as $\mu_i$ and $\Sigma_i$ for normal distribution are fixed but unknown
> ML gives the best parameter estimation obtained by maximizing probability of obtaining observed samples
> Bayesian view parameter to be estimated as a random having some konwn prior distribution , and observing samples convert this to posterior density.

Assume:

* Sample $\{x_j\}$ in $D$ are i.i.d which are drawn independently according to the probability law $p(x|w_j)$
* $p(x | w_j) \sim N(\mu_j , \Sigma_j)$

$$p(D|\theta) = \displaystyle {\Pi_{k = 1}^n p(x_k | \theta)}$$

> According to beyes decision rule , maximize the posterior probability $p(x_k|\theta)$ will yield minimum error.

似然函数:$$l(\theta) = \ln p(D|\theta) = \sum_{k = 1}^n \ln p(x_k | \theta)$$

目标:$$\hat \theta = arg \max_\theta l(\theta)$$

令 $$\nabla_\theta l = 0$$
得
$$\begin{array}{rl}
\hat \mu &= \displaystyle  \frac{1}{n} \sum_{k = 1}^n x_k\\
\hat \sigma^2 &= \displaystyle \frac{1}{n} \sum_{k = 1}^n (x_k - \hat \mu)^2
\end{array}$$

**How to use ML to train a Classfier?**
Estimate $\mu_i$ and $\Sigma_i$ using $D_i$ for each class

### Tutorial

![](assets/moshishibiedaolun/2018-03-29-22-50-53.png)
![](assets/moshishibiedaolun/2018-03-29-22-51-22.png)
![](assets/moshishibiedaolun/2018-03-29-22-51-40.png)

![](assets/moshishibiedaolun/2018-03-29-22-52-08.png)
![](assets/moshishibiedaolun/2018-03-29-22-52-34.png)

## Linear Models for classification

Two class problem:

* Geometry of Linear Discriminants
* Least Squares Classification
* Perceptron Algorithm
* Generalized Linear Model

### Patametric vs Non-Parametric Method

* Patametric Methods
    * assume the form of the sample distribution is known
    * Trainning samples used to estimate distribution parameter
    * accurate if distribution assumption is correct
    * such as use Maximum Likelihood to train  a Classifier

* Non-Parametric Methods
    * Do not make assumption of sample distribution
    * assume the proper form for discriminant function
    * Trainning samples used to estimate value of parameter of the classifier
    * sub-optimal , but simple to use

### Linear Classification Models

#### Two-Class Problem

Decision surface is defined as $g(x) = 0$
$$g(x) = w^Tx + w_0$$
Since $g(x)$ is linear, this decision surface is hyperplane.

Assign $x$ to class $C_1$ if $g(x) \geq 0$, else class $C_2$

Defines decision boundary as $g(x)= 0$

#### Distance of arbitrary point $x$ to surface

![](assets/moshishibiedaolun/2018-04-08-15-15-40.png)

There are many solution of decision surface(optimal and sub-optimal), we want to choose the best one, So we need to calculate distance.

* distance of origin(原点) to surface
    * let $x_a$ and $x_b$ be points on the surface,we get $g(x_a) = g(x_b) = 0$, equivalent to
    $$w^T(x_a - x_b) = 0$$
    > 即$w$是法向量
    * let $x$ be the orthogonal projection onto the decision surface, we have $x = kw$
    * $g(x) = 0 = w^Tkw + w_0$
    $$k = -\frac{w_0}{\|w\|^2} \Rightarrow d = \|x\| = -\frac{w_0}{\|w\|}$$

* Distance of arbitrary point $x$ to surface
    <img src="assets/moshishibiedaolun/2018-04-08-15-09-11.png" width = 50%>

    * Let $x$ be the arbitrary point to surface, $x_\perp$ be the orthogonal projection onto the decision surface.
    * $$x = x_{\perp} + \gamma w$$
    * $$\begin{array}{rl}g(x_{\perp}) = 0 &= w^T(x - \gamma w) + w_0 = w^Tx - \gamma w^Tw + w_0\\
    &\Rightarrow \gamma = \frac{g(x)}{\|w\|^2}\\
    &\Rightarrow d = \|x - x_{\perp}\| = \frac{g(x)}{\|w\|}
    \end{array}
    $$
    > $\left \|\frac{w}{\|w\|}\right \| = 1$

#### Convexity of decision regions

![](assets/moshishibiedaolun/2018-04-08-15-53-45.png)

* any point $x$ on line $x_a$ and $x_b$ can be express as
    $$x = \lambda x_a + (1-\lambda)x_b$$,where $0\leq \lambda \leq 1$
* From linearity of discriminant functions
    $g_k(x) = x_k^Tx + w_{k0}$
* combining the two, we have
    $$g_k(x) = \lambda g_k(x_a) + (1-\lambda)g_k(x_b) $$
* because $x_a$ and $x_b$ lie inside $R_k$ it follow that
    $g_k(x_a) > g_j(x_a),g_k(x_b) > g_j(x_b)$ for all $j \neq k$
* Hence $x$ also lies inside $R_k$

#### Augmented Vector

* With dummy input $x_0 = 1$
* $x \leftarrow (x_0,x)$
* and $w \leftarrow (w_0,w)$
* then $g(x) = w^Tx$

#### Perceptron Algorithm
* Input vector $x$ transformed by a fixed nonlinear transformation to give feature vector $\phi(x)$
    $$g(x) = f(w^T\phi(x))$$
    where non-linear activation $f$ is a step function
    $$f(a) = \left \{\begin{array}{rl}+1 &, a\geq 0 \\-1&,a<0\end{array}\right .$$

* Use a target coding scheme
    $y = +1$, for class $C_1$
    $y = -1$, for class $C_2$

* all patterns need to satisfy
    $$w^T\phi(x_n)y_n > 0$$

* for each misclassfied sample, Perceptron Criterion tries to minimize:
    $$E(w) = -w^T\phi(x_n)y_n > 0$$

* Stochastic Gradient Descent
    $$g^{k + 1} = w^k - \eta\nabla E(w) = w^k + \eta\phi(x_n)y_n$$

#### Least Square for Classification

* Normalization
    * Simplify the setting: replace all labeled by $w_2$ by their 'negatives', such as
        $$x_i \leftarrow -x_i$$
        where $x_i$ is $w_2$
    * is the problem is linearly separable, all samples are
    $$w^Tx_i > 0$$
    as a result, labels can be ignored.

* Seek a hyperplane having a larger margin
* Sum-of-squared-error function handles the margin situation($w^Tx_i = b_i$)
    $$J_S(w) = \sum_{i = 1}^n (w^Tx_i - b_i)^2$$
    where $b_i$ are some arbitrarily specified positive constant.
* Matrix notation:
    $$J_S(w) = \|Xw - b\|^2$$

    $$w = \left ( \begin{array}{c}w_0\\w_1\\\vdots\\w_d\end{array}\right ),X = \left (\begin{array}{cccc}x_{10} & x_{11} & \cdots & x_{1d}\\x_{20} & x_{21} & \cdots & x_{2d}\\
    \vdots & \vdots & \ddots & \vdots\\
    x_{n0} & x_{n1} & \cdots & x_{nd}
    \end{array}\right ),b = \left (\begin{array}{c}b_0\\b_1\\\vdots\\b_d\end{array}\right )$$

* $$J_S(w) = 0 \Rightarrow w = X^{-1}b$$ if $X$ is nonsingular.
    however, usually, $X$ is rectangular:
    * More rows than columns
    * More samples than features
    * More equation than unkonwns

* Pseudoinverse Method
    $$\nabla J_S(w) = 0 = 2X^T(Xw - b)\Rightarrow w = (X^TX)^{-1}X^Tb$$

* Remark: $X^Tx$ is not always nonsingular
    $$w = \lim_{\epsilon\to 0} (X^TX + \epsilon I)^{-1}X^Tb$$

    > 既然是解析解, 是不是b可以设置成足够大?

* Gradient Descent can also be used, called Least-Mean-Squared(LMS)
    $$\nabla J_S(w) = 2\sum_{i = 1}^n (w^Tx_i - b_i)x_i = 2X^T(Xw - b)$$

    LMS needs not converge to a separating hyperplane(which can separate the samples perfectly), even if one exists.

    ![](assets/moshishibiedaolun/2018-04-08-16-39-20.png)

#### Gereralized Linear Model

* $G(X) = f(w^Tx + w_0)$
    ,$f$ is konwn as the activation function.
* The decision surfaces correspond to
    $g(x) = constant$
    or $w^Tx + w_0 = constant$
    so decision boundaries are linear in feature space even if $f$ is nonlinear. e.g. a step function
    > 就是说, 只有一层的话, 即使最后是非线性激活, 最后还是线性划分

* Generalized linear discriminant function
    $g(x) = \sum_{i = 1}^d w_i\phi_i(x)$
    $y$ is not linear in $x$, but linear in activation function $\phi_i$

* Quadratic Discriminant Function
    $g(x) = w_0 + w_1x + w_2x^2$
    mapping:$\phi(x) = \left ( \begin{array}{c}1\\x\\x^w\end{array}\right )$
    ![](assets/moshishibiedaolun/2018-04-08-16-53-44.png)

* this is the idea behind kernel trick

#### Related issue
* Size of learning rate $\eta$:
    * too small, convergence is nnedlessly slow
    * too large, the correction process will overshoot and can even diverge

* sub-optimal solution
    trapped by local minimum

### Extension to Multiple Classes
Two approaches:
* Using serveral two-class classifiers:
    Both result in ambiguous regions of input space
    * one-vs-the-rest
    <img src="assets/moshishibiedaolun/2018-04-08-16-58-20.png" width = 50%>
    * one-vs-one
    @import "assets/moshishibiedaolun/2018-04-08-16-58-59.png" {width=50%}

* Using $c$ linear discriminant function
    assign a point $x$ to class $C_k$ if $g_k(x) > g_j(k)$ for all $j \neq k$

    the decision boundary between class $C_k$ and $C_j$ is given by:
    $g_k(x) = g_j(x)$

    Advantage:
    * Avoid ambiguous region
    * Every decision region is singly connected
    * Low Complexity
    * $c$ classifiers are needed

### Tutorial
![](assets/moshishibiedaolun/2018-04-08-18-05-07.png)
![](assets/moshishibiedaolun/2018-04-08-18-05-26.png)
![](assets/moshishibiedaolun/2018-04-08-18-05-42.png)
![](assets/moshishibiedaolun/2018-04-08-18-06-01.png)
![](assets/moshishibiedaolun/2018-04-08-18-06-27.png)
![](assets/moshishibiedaolun/2018-04-08-18-06-42.png)

## Principal componment analysis

Data visualization:
* difficult to see the correlation between the features? Is there a representation better than the coordinate axes?
* difficult to see in 4 or higher dimensional space? Is it really necessary to show all dimension
* how could we find the **smallest** subspace that keeps the **most** information about the original data?

A solution: **Principal component analysis**

PAC: orthogonal projection of data onto lower dimanesion linear space that:
* maximize variance of prejected data(purple line)
* minimize mean squared distance between data point and prejections(sum of blue lines)
    <img src="assets/moshishibiedaolun/2018-05-31-14-19-50.png" width = 300px>

### Algorithm

* Algorithm I(Sequential): Give the centered data $\{x_!,\cdots,x_m\}$, compute the first principal vectors($1^st$ PAC vector):
    $$w_1 = \arg \max_{\|w\|=1} \frac{1}{m}  \sum_{i = 1}^m \{w^Tx_i^2\}$$

    We maximize the variance of prejection of $x$:
    $$w_k = \arg \max_{\|w\| = 1} \frac{1}{m} \sum_{i = 1}^m \{ [w^T(x_i - \sum_{j = 1}^{k - 1}w_jw_j^Tx_i)]^2 \}$$

    ![](assets/moshishibiedaolun/2018-05-31-14-35-33.png)

* Algorithm II (Sample Convariance Matrix)

    Give data $\{x_!,\cdots,x_m\}$ compute covariance matric $\Sigma$
    $$\Sigma = \frac{1}{m} (x_i - \bar x)(x_i - \bar x)^T \quad where \quad \bar x = \frac{1}{m} \sum_{i = 1}^m x_i$$

    PCA basis vectors = the eigenvectors of $\Sigma$

    Larger eigenvalue $\Rightarrow$ more importance eigenvectors
    1. substract mean from eachcolumen vector$x_i$
    $$x_i \leftarrow x_i - \bar x$$
    2. $\Sigma \leftarrow XX^T$, convariance matrix of $X$
    3. $\{\lambda_i,\mu_i\}$ = eigenvalues/eignvectors of $\Sigma$
    4. return top-k principle componment

### Application
* Face recognition
    * build a PAC subspace for each person and check while subspace can reconstruct the test image the best
        suppose m instance , each of size N, given $N\times N$ convariance matrix , can compute:
        * $all N eigenvectors in O(N^3)$
        * first $k$ eignvector in $O(kN^2)$
        expensive computational cost, a clever workaround:
        * note $m<<N$
        * use $L = X^TX$ isntead of $\Sigma = XX^T$
        * if $v$ is eigenvector of $L$, then $Xv$  is eigenvector of $\Sigma$
        $$\begin{align}
        Lv = \gamma v\\
        X^TXv = \gamma v\\
        X(X^Xv) = \gamma Xv\\
        (XX^T)Xv = \gamma (Xv)\\
        \Sigma (Xv) = \gamma (Xv)
        \end{align}$$


    * build one pac database for the whole dataset and then classify based on the weights.

    Shortcomings:
    * require carefully controlled data:
        * all faces centerd in frame
        * same size
        * some sensitivity to angle
    * Alternative:
        * 'learn' one set of PCA vector for each angle
        * use the one with lowest error
    * method is completely knowledge free
        * doesn't know that faces are wrapped around 3D objects
        * make no effort to preserve class distinction

* image compression
* Noise filtering

PCA SHORTCOMINGS:
PCA can not capture NON-Linear structure

### Justification of Algorithm II

* $x \in R^N, X = [x_1, x_2, \cdots , x_m] \in R^{N\times m}$

* $U = \left (\begin{array}{c}u_1^T\\\vdots \\u_N^T\end{array} \right ) \in R^{N\times N}$ orthogonal matrix, $UU^T = I_N, y \dot = Ux, x = U^Ty = \sum_{i = 1}^N u_i y_i$

* approximation of x using m basis vectors only:$\hat x \dot = \sum_{i = 1}^M u_iy_i, (M\leq N)$

* average error: $\epsilon^2 \dot = E\{\|x - \hat x\|\} = \frac{1}{m} \sum_{j =1}^m \|x_j-\hat x_j\|$

* goal: $\arg \min_{U} \epsilon^2$ ,s.t $U^TU = I^N$

* assume $x$ is centered:
    $$\begin{array}{rl}
    \epsilon^2 &= E\{\|x - \hat x\|\} = E\{ \| \sum_{i = 1}^Nu_iy_i - \sum_{i = 1}^M u_iy_i\|^2 \}\\
    &= E\{ \sum_{i = M + 1}^N y_iu_i^Tu_iy_i \} = \sum_{i = M + 1}^N E\{y_i^2\}\\
    & = \sum_{i = M + 1}^N E\{(u_i^Tx)(x^Tu_i)\}\\
    & = \sum_{i = M + 1}^N u_i^TE\{xx^T\}u_i\\
    &= \sum_{i = M + 1}^Nu_i^T\Sigma u_i
    \end{array}$$

* $\arg \min_{u_{M+1},\cdots,u_N} \epsilon^2$
    Use largrange-multiplier for constraint
    $$\begin{align}
    \begin{array}{rl}
    L &= \epsilon^2 - \sum_{i = M + 1}^N \lambda_i(u_i^Tu_i - 1)\\
    &= \sum{i = M + 1}^N u_i^T\Sigma u_i - \sum_{i = M + 1}^N \lambda_i(u_i^Tu_i - 1)\\
    \end{array}\\
\frac{\partial L}{\partial u_i} = [2\Sigma u_i - 2\lambda_i u_i] = 0\Rightarrow \Sigma u_i = \lambda_i u_i
    \end{align}$$

    $\Rightarrow [u_i, \lambda_i]$ is eigenvector and eigenvalue of $\Sigma$

    $$\epsilon^2 = \sum_{i = M + 1}^N u_i^T(\Sigma u_i) = \sum_{i = M + 1}^N  u_i^T\lambda_i u_i= \sum_{i = M + 1}^N \lambda_i$$

    The error $\epsilon^2$ is minimal if $\lambda^{M + 1} , \lambda_N$ are the smallest eigenvalue of $\Sigma$

### summary
PCA:
* find orthonrmal basis for data
* short dimensions in roder of importance
* discard low significance dimensions

## clustering

* supervised learning: discover patterns in the data that relate data attributies with a target atribute.
* sunsupervised learning: the data have no target attribute

What is cluter analysis?
Finding the groups of objects such that the objects in a group will be similar to one anthoer and different from the objects in other groups.

Type of clustering
* Partitional clustering: A division of data objects into non-overlapping subsets such that each data object is in exactly one subset
* Hierarchical clustering: A set of nested clusters organized as hierarchical tree

Partitional Clustering
* minimize squared error
* k-means algorithm

weakness of k-means
* problem with outliers
    * remove some data points in clustering process that are much further away from the centroids than other data points
    * perform random sampling
* sensitive to initial seeds
* not suitable for discovering clusters that are not hyper-ellipsoids

Fuzzy C-means clustering: a method of clustering which allow one piece of data to belong to two or more clusters.

> 多分配, 每类一个概率

target:
$$\min J_f(C,m) = \sum_{i = 1}^C\sum_{k = 1}^N (u_{k,i})^md_{k,i} $$ s.t. $\sum_{i = 1}^C u_{i,k} = 1$

The cluster centers $v_i$:
$$v_i = \frac{\sum_{k = 1}^N (u_{k,i})^mx_k}{\sum_{k = 1}^N (u_{k,i})^m}$$

Membership:
$$u_{i,k} = \frac{1}{\sum_{j = 1}^C \left (\frac{d_{k,i}}{d_{k,j}}\right )^{\frac{2}{m - 1}}}$$

Hierarchical Clustering: produces a set of nested clusters organized as a ierarchical tree

Strength:
* do not have to assume any particular number of clusters
* they may correspond to meaningful taxonomies

Two type:
1. Agglomerative:
    * start with the points as individual clusters
    * at each step, merge the closest pair of clusters until only cluster left
2. divisive:
    * start with one, all-inclusive cluster
    * at each step, split a cluster until each cluster contain a point

how to define inter-cluster similarity?
* min
    * can handle non-elliptical shape
    * sensitive to noise and outliers
* max
    * less susceptible to noise and outliers
    * trends to break large clusters
    * biased towards globular cluster
* group average
* distance between centroid
* other methods driven by an objective function

Problem and limitation:
* once a decision is made to combine two clusters, it cannot be undone
* no objective function is directly minimized
* different schemes have problem with one or more of following:
    * sensitivity to noise and outlier
    * difficulty handing different sized cluster and convex shape
    * breaking large clusters



## ensemble

ensemble learning: a machine learning paradigm where multiple learnier are used to solve the problem

If we split the data in random different ways, decision tree give different result , high variance

bagging:bootstrap aggregating is a method that result in low variance.
Say for each sample b, we calculate$f^b(x)$, then:
$$\hat f_{avg}(x) = \frac{1}{B} \sum_{b = 1}^B \hat f^b (x)$$

Bootstrap: construct B of trees, learning a classifier for each bootstrap sample and average them

bagging advantage:
* reduce overfitting
* normally use one type of classifier
* decision tree are popular
* easy to parallelize

Issues:
* the expectation of average of B such trees is the same as the expectation of any of them
* the bias of bagged trees if the same as that of the individual trees.
* suppose that there is one very strong predictor in the data set, along with a number of other moderately strong predictors.
 Then all bagged tree will select the strong predictor at the top of the tree and therefore all trees will look similar.

Solution: randomly select the subset

AdaBost: an algorithm for constructing a strong classifier as linear combination
