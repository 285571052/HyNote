# 模式识别导论

## Bayesian Decision Theory

1. Prior probability:ignore present state

    $$p(x)$$

2. Conditional probability:also called likelihood

    $$p(x|w)$$

    ![](assets/moshishibiedaolun/2018-03-17-15-17-10.png)

3. Posterior probability

    Bayes Formula:

    $$p(w|x)= \frac{p(x | w)p(w)}{p(x)}$$

### Bayes decision rule

* Decide $w_1$ if $p(w_1 | x) > p(w_2|x)$
* Decide $w_2$ if $p(w_2 | x) > p(w_1|x)$

or equivalent to(ignore $p(x)$)

* Decide $w_1$ if $ p(x | w_1)p(w_1) > p(x | w_2)p(w_2)$
* Decide $w_2$ if $ p(x | w_2)p(w_2) > p(x | w_1)p(w_1)$

ispecial case when $p(w_1) = p(w_2)$

* Decide $w_1$ if $p(x | w_1) > p(x | w_2)$
* Decide $w_2$ if $p(x | w_2)> p(x | w_1)$

![](assets/moshishibiedaolun/2018-03-17-15-51-16.png)

![](assets/moshishibiedaolun/2018-03-17-16-49-30.png)

### Probability of Error

There are two possible errors
<img src="assets/moshishibiedaolun/2018-03-17-15-53-00.png" width = 40%>

Probability of error is
$p(error|x) = p(w_1|x)$ if decide $w_2$
$p(error|x) = p(w_2|x)$ if decide $w_1$

equivalent to

$p(error | x) = min[p(w_1|x) , p(w_2|x)]$

> Bayes rule or Bayes classifier:
> decide $w_1$,if $p(w_1 | x) > p(w_2|x)$ , otherwise decide $w_2$

> 这里错误的概率,是指当我们预测为$w_i$的时候,错误的概率,由上图可以看出,预测为第二类错误的概率是第二列红色部分
> 而我们预测为$w_2$是因为$p(w_2|x)$最大, 即预测为$w_2$的正确率最大
> 那么对应的错误率自然是$1 - p(x_2|x)$ , 即下面多分类问题中的$max$

### Multi-class problem

Bayes rule:
select $w_i$ if $p(w_i|x) > p(w_j|x)$ for all $j \neq i$

Probability of Error:
$$p(error|x) = 1 - max[p(w_1|x),...,p(w_c|x)]$$

### Bayes Error Rate

$$ \begin{array}{rl}
p(error) &= p(x\in R_2 , w_1) + p(x \in R_1 , w_2)\\
&= p(x\in R_2|w_1)p(w_1) + p(x\in R_2|w_2)p(w_2)\\
&= \int_{R_2} p(x\in R_2|w_1)p(w_1) dx + \int_{R_1} p(x\in R_2|w_2)p(w_2) dx
\end{array}$$

![](assets/moshishibiedaolun/2018-03-17-16-23-59.png)

> 从上图可以看出, 当我们采取Bayes decision rule(图中的$x_a'$ , 两条曲线的交点)的时候 , $p(error)$不一定是最小的
> 因此需要确定决策边界(threshold),使得$p(error)$最小
> 为此需要计算$p(x|w)$和$p(w)$

### Loss function

Loss : measure the cost for each action taken

Let
$\{w_1 , w_2 , ... , w_c\}$ be the set of $c$ categories
$\{\alpha_1 , \alpha_2 , ... , \alpha_a\}$ be the set of possible actions
$\lambda_{ij} = \lambda(\alpha_i|w_j)$ be the loss incurred for taking action $\alpha_i$ when the category is $w_j$

> 在真值为$w_j$下采取$\alpha_i$造成的损失

The expected loss associated with taking action $\alpha_i$ is Conditional risk
$$R(\alpha_i|x) = \sum_{j = 1}^c \lambda_{ij}p(w_j|x)$$

and overall risk
$$\mathcal{R} = \int R(a(x) | x)p(x) dx$$

Choose $\alpha_i$ so that $R(\alpha_i|x)$ is as small as possible for every $x$ , then the overall risk will be minimized

#### Minimum Risk Decision Rule

take action $\alpha_1$ if
$$R(\alpha_1|x)<R(\alpha_2|x)$$

equivalent to :

decide $w_1$ if
$$(\lambda_{21} - \lambda_{11})p(x|w_1)p(w_2) > (\lambda_{12} - \lambda_{22})p(x|w_2)p(w_2)$$
otherwise ,declde $w_2$

> 作差之后,可以看成是预测正确的损失为1, 而预测错误的时候 ,带权计算误差 , 下面可以看出 minimum Risk Decision Rule 范围更广

**Zero-one loss function** :
$$\lambda(\alpha_i|w_j) = \left \{ \begin{array}{rl}0 & i = j \\ 1 & i \neq j \end{array} \right .$$

* No loss to a correct decision
* A unit loss to any error
* All erroes are equally costly

Risk:
$$\begin{array}{rl}R(\alpha_i|x) &= \sum_{j = 1}^c \lambda_{ij}p(w_j|x)\\&= \sum_{j \neq i} p(w_j|x) \\&= 1-p(w_i|x) \end{array}$$

So minimizing the average probability of error requires maximization $p(w_i|x)$

> 前面bayes rule , 我们选择后验概率最大的$w_i$作为参数的估计(似然概率最大的$w_i$) , 而这个部分解释了为什么要这么做 , 因为这么取造成的损失的期望是最小的

> 而后面的决策边界 , 则是在确定了$w_i$后, 作决策所用 , 选择一个决策边界使得错误率最小 , 这一步的操作是不会对损失的期望造成影响,因为$w_i$下,已经是最小的

Practice:
![](assets/moshishibiedaolun/2018-03-17-16-49-55.png)

### Classfiers

Discriminant functions :
$$g_(x) = , i = 1 , \cdots , c$$

Decision Rule: x will be assigned to class $w_i$ if $g_i(x) > g_j(x) , \forall j \neq i$

**For Minium-Error-Rate:**
$$g_i(x) = p(w_i|x)$$

if $G$ is a monotonically increasing functions , the decision rule can also be :
Decision Rule: x will be assigned to class $w_i$ if $G(g_i(x)) > G(g_j(x)) , \forall j \neq i$

for example $log$
$$G(g_i(x)) = \ln (g_i(x)) = \ln p(x|w_i) + \ln p(w_i) - ln p(x)$$

**For tow class problem**:
> 二分类问题 , 判别函数作差

$$g(x) = g_1(x) - g_2(x) = \ln \frac{p(x|w_1)}{p(x|w_2)} + \ln \frac{p(w_1)}{p(w_2)}$$

> 左边这个形式就是逻辑回归的表达式 , 即逻辑回归假设$p(w_1) = p(w_2)$
> 由于在样本少的情况下, 贝叶斯方法并不能准确估计 , 因此采用极大似然法(ML) , 即和逻辑回归的表示对应上

### 特例:正态分布

Advantage of Normal Distribute:

1. Analytically traceable continuous
2. A lot of processes are asymptotically Gaussian

Definition:
$$p(x) = \frac{1}{(2\pi)^{\frac{d}{2}} |\Sigma|^\frac{1}{2}}exp\left (-\frac{1}{2}(x - \mu)^T\Sigma^{-1}(x - \mu)\right )$$

Assume $p(x|w_i) \sim N(\mu_i , \Sigma_i)$ 
and use $G(g_i(x)) = \ln (g_i(x)) = \ln p(x|w_i) + \ln p(w_i) - ln p(x)$ as discriminant function,
$$g_i(x) = -\frac{1}{2}(x - \mu_i)^T\Sigma_i^{-1}(x - \mu_i) - \frac{d}{2}\ln (2\pi) - \frac{1}{2} \ln |\Sigma_i| + \ln p(w_i)$$

改写成以下形式:
$$g_i(x) = x^TW_ix + w_ix + w_{i0}$$
其中,$W_i = -\frac{1}{2}\Sigma_i^{-1}$,
$w_i = \Sigma_i^{-1}\mu_i$,
$w_{i0} = -\frac{1}{2}\mu^T\Sigma_i^{-1}\mu - \frac{1}{2}\ln |\Sigma_i| + \ln p(w_i)$

#### Estimation on $p(x| w_i)$

how can we obtain them?

* can be estimated by using training samples
* No problem with estimation of $p(w_i)$
* Sample are often too small for the estimation of $p(x | w_i)$
* even worse for large dimension of feature space

one possible solution for estimating $p(x|w_i)$
assume known forms of distribution, such as normal distribution
$$p(x|w_i) \sim N(\mu_i , \Sigma_i)$$

the advantage of assuming a certain distribution form is that **the problem is now simplified from estimating unknown posterior function to one estimating parameters of a konw distribution function**

**ML Parameter Estimation**

> ML & Bayesian : two common parameter estimation methods
> Parameter such as $\mu_i$ and $\Sigma_i$ for normal distribution are fixed but unknown
> ML gives the best parameter estimation obtained by maximizing probability of obtaining observed samples
> Bayesian view parameter to be estimated as a random having some konwn prior distribution , and observing samples convert this to posterior density.

Assume:

* Sample $\{x_j\}$ in $D$ are i.i.d which are drawn independently according to the probability law $p(x|w_j)$
* $p(x | w_j) \sim N(\mu_j , \Sigma_j)$

$$p(D|\theta) = \displaystyle {\Pi_{k = 1}^n p(x_k | \theta)}$$

> According to beyes decision rule , maximize the posterior probability $p(x_k|\theta)$ will yield minimum error.

似然函数:$$l(\theta) = \ln p(D|\theta) = \sum_{k = 1}^n \ln p(x_k | \theta)$$

目标:$$\hat \theta = arg \max_\theta l(\theta)$$

令 $$\nabla_\theta l = 0$$
得
$$\begin{array}{rl}
\hat \mu &= \displaystyle  \frac{1}{n} \sum_{k = 1}^n x_k\\
\hat \sigma^2 &= \displaystyle \frac{1}{n} \sum_{k = 1}^n (x_k - \hat \mu)^2
\end{array}$$

**How to use ML to train a Classfier?**
Estimate $\mu_i$ and $\Sigma_i$ using $D_i$ for each class

### Tutorial

![](assets/moshishibiedaolun/2018-03-29-22-50-53.png)
![](assets/moshishibiedaolun/2018-03-29-22-51-22.png)
![](assets/moshishibiedaolun/2018-03-29-22-51-40.png)

![](assets/moshishibiedaolun/2018-03-29-22-52-08.png)
![](assets/moshishibiedaolun/2018-03-29-22-52-34.png)