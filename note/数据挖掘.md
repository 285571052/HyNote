# 数据挖掘

## 数据预处理

### 为什么需要进行预处理?

1. 不完整:收集数据时,不同阶段不同考虑, 人/设备/软件等问题
2. 含噪声:包含错误或孤立点,来源于收集阶段和数据传输阶段
3. 不一致:在名称或代码之间存在差异,来源于不同的数据源 , 功能依赖冲突

> 噪声：错误的数据<br>
> 孤立点：偏离正常数据，但真实发生

### 数据与处理的主要任务

1. 数据清洗 data cleaning

    填充遗失的数据, 平滑噪声数据, 辨识或删除孤立点 , 解决不一致性问题
2. 数据集成 data integration

    多个数据库, 数据立方或文件进行继承

3. 数据变换 data transformation

    规范化与聚集

4. 数据约简 data reduction

    得到数据集的压缩表示 ,它小得多,但能够产生相同的(或几乎相同的)分析结果

5. 数据离散化 data discretization

    特别对数字值而言非常重要

### 数据清洗

主要任务:

1. 填充遗失数据
2. 辨识孤立点、平滑噪声数据
3. 修正不一致性数据
4. 解决数据集成造成的数据冗余问题

#### 遗失数据

1. 忽略元组:除非有多个属性缺少之,否则该方法不是很有效
2. 人工填充:费事费力
3. 自动填充
    1. 使用全局常量填充,比如"unkonwn" , 但可能会被误认为是一个新的,有意义的类
    2. 该属性的平均值
    3. 使用于给定元素同一类的所有样本该属性的平均值
    4. 使用可能的值:基于推导的方法,比如beyesian公式或决策树

#### 噪声数据

噪声:是一个策略变量中的随机误差或偏差

1. 分箱方法
    1. 先对数据进行排序,然后把他们划分到箱
    2. 通过箱平均值,箱中值等进行平滑

    该方法对神经网络等不适用

    * 等宽划分(距离)

        根据属性值范围划分成N等宽的区间

        很直接,但孤立点将会对此方法有很大的影响

    * 等深划分(频率)

        划分成N个区间,每个区间含有大约相等的样本数

        具有较好的扩展性

2. 聚类

    探测并去除孤立点

3. 计算机和人工检查结合
4. 回归分析

    让数据拟合一个函数来平滑数据

#### 数据集成

数据集成:将多个数据源中的数据结合起来存放在一个一致的数据存储中(如数据仓库)

可能的需要解决问题:

1. 不同数据源可能具有不同的值
2. 不同的表示方式,不同的刻度
3. 数据冗余问题:可以通过相关性分析检测出来

#### 数据变换

1. 平滑 smoothing

    取出噪声数据

2. 聚集 aggregation

    汇总 , 数据立方构造

3. 概化 generalization

    沿概念层次上升

4. 规范化 normalization

    刻度变换

##### 规范化

1. 最大-最小规范化

    $$v' = \frac{v - min}{max_A - min_A}(max_{new} - min_{new}) + min_{new}$$

    移项之后更好理解:

    $$\frac{v' -  min_{new}}{max_{new} - min_{new}} = \frac{v - min}{max_A - min_A}$$

2. z-score 规范化

    $$v' = \frac{v - mean}{stand}$$

    规范化之后 , $v'$满足均值为$0$ , 方差为$1$

3. 小数定标规范化

    $$v' = \frac{v}{10^j}$$

    其中,$j$是使得$max(|v|) < 1$的最下整数

    规范化之后的结果是所有的值都小于1

#### 数据约简

数据约简：两个方向

1. 降维：减少特征

    1. 决策树归纳

        若只需要其中一些属性即可划分数据, 那么只需要保存这些关键属性即可 ,其他属性可约简掉

2. 采样：减少样本数
3. 数值归约：对样本重新编码，值记录模型与统计信息而无需记录原始数据
    1. 参数化方法：比如只记录解析式的参数而不记录所有样本点

        线性回归 , 对数线性模型

    2. 非参数化方法：对于每个区间，只记录平均值与样本数

        直方图 , 聚类 , 采样

降维对时间复杂度减少的幅度不大于减少样本数

![](assets/shujuwajue/2018-03-15-22-04-32.png)

#### 离散化

属性的三种类型:

1. 离散
    1. 标称性 : 取自无序集合的值
    2. 有序的 : 取自有序集合的值
2. 连续

离散化:

* 把连续型属性的取值范围划分区间
* 通过离散化减小数据集大小

离散化方法:

1. 分箱
2. 直方图分析
3. 聚类分析
4. 基于熵的离散化(决策树,将样本划分成两个部分,选择信息增益最大的作为划分边界)

## 数据仓库

什么是数据仓库?(<font color="red">必考定义)</font>)

数据仓库是面向主题的,集成的,时变的,非易失的数据集合,它用来支持管理部分的决策过程 - W.H.Inmon

1. 联机事务处理 OLTP on-line transaction processing

    日常操作

2. 联机分析处理 OLAP on-line analytical processing

    决策支持

### 建模数据仓库

1. 星型模式

    一个事实与多个直连维表

    ![](assets/shujuwajue/2018-03-15-22-28-27.png)

2. 雪花模型

    在星型模式的基础上,维表也存在层次

    ![](assets/shujuwajue/2018-03-15-22-28-46.png)

3. 事实星座

    多个事实表分享共同的维表

    ![](assets/shujuwajue/2018-03-15-22-29-02.png)

数据仓库一般采用前两种模式(树状,简单 , 而事实星座较为复杂)

### 度量的分类

1. 分布式的 distributive

    一个聚集函数能以分布式进行,称它是分布式的

    比如,min,max,sum,count

2. 代数的 algebraic

    它能由M个参数的代数函数计算 , M是有界的

    如,avg

3. 整体的 holistic

    median(中位数) , mode(众数)

对于代数的和整体的度量,无法采用分布式计算,因此对于常用的部分最好预先计算出来保存

### 数据立方体

数据立方体:在原始数据的基础上, 预先计算更少维度下的综合数据, 以空间换时间

![](assets/shujuwajue/2018-03-15-22-37-41.png)
![](assets/shujuwajue/2018-03-15-22-37-54.png)

1. 3D 数据立方的白色部分 表示原始数据
2. 2D 数据立方的平面部分
3. 1D 数据立方的灰色竖条
4. 0D 数据立方的最深色部分 表示所有数据的总和

### OLAP的操作

1. 上卷 Roll up(drill up)

    通过一个维度的概念分层向上攀升或通过维度归约,在数据立方体上进行聚集

2. 下钻 drill down

    上卷的你操作

3. 切片 Slice 和 切块 dice

    投影与选择

4. 转轴 pivot (rotate)

    是一种目视操作,它转动数据的视角,提供数据的替代表示

### 3个数据仓库模型

1. 企业仓库
2. 数据集市
3. 虚拟仓库

## 关联规则挖掘

### 若干概念

* 关联规则: **频繁模式** Frequent pattern, 在数据库中频繁出现的模式(项集、序列)

    规则形式: $x_1 \land x_2 \land \cdots \land x_n \to y_1 \land y_2 \land \cdots \land y_m$
    读作:如果$x_1 \land x_2 \land \cdots \land x_n$,那么$y_1 \land y_2 \land \cdots \land y_m$

* k-项集:含有k个元素的集合 $X = \{x_1 , x_2 , \cdots , x_k\}$

* 对于规则$X\to Y$,
    * 支持度:$support = P(X \cup Y) = \frac{\text{项集}\{X\cup Y\}\text{的支持度计数}}{\text{事务表中总事务计数}} = \frac{\text{事务表中包含项集}\{X\cup Y\}\text{的事务数}}{\text{事务表中总事务计数}}$
    * 置信度:$confidence = P(Y|X) = \frac{\text{事务表中包含项集}\{X\cup Y\}\text{的事务数}}{\text{事务表中包含项集}\{X\}\text{的事务数}}$
    *   1. 支持度和置信度只有两者都高才有意义
        2. 以投票为例,只有10%的人参与投票, 即使获得全票也无意义
            即支持度低而置信度高
        3. $support \leq confidence$
    * 最小支持度/置信度: 由用户提供的筛选条件

* 支持度频率(支持度计数):模式或项集在DB中出现的频率

* 频繁模式/项集:支持度$\geq$支持度的模式/项集

* 关联规则挖掘的任务: 找出所有满足最小支持度和最小置信度的规则

* 单维布尔规则: if A(x) then A(y)， 涉及到一个谓词， 因此称为单维关联规则， 以 购买 为例， 是涉及买 或者 不买， 因此也称为 布尔关联规则

* 多维关联规则， 有两个以上谓词

* 量化关联： 量化的量与属性之间的关联

### Apriori 算法

**穷举法**

设事物表中有n见不同的商品 $\{x_2. x_2, \cdots, x_n\}$， 可以通过穷举的方法来美剧所有的关联规则， 从2-项集到n-项集，
以2-项集为例子：检查是否存在关联规则$x_i\to x_j$

对于n个元素的集合， 有$2^n$个子集， 而对于我们这个问题， 有$2^n - n - 1$个子集（减去1-项集， 空集）

集合的数目是指数级的，因此穷举法不可取

**Apriori**：一个候选集产生和测试算法

定理： 一个频繁集的任意子集也必须是频繁集

**Apriori裁剪原理**： 对于任意项集， 如果它不是频繁集， 则它的任何超级步用于产生/测试。（若一个集合不是频繁集， 其超级也不是频繁集）


算法过程：


1. 输入： 数据库D的最小支持度
2. D中所有的频繁项集
3. $L_{k -  1}$表示一个k-1项频繁集
4. 从$L_{k -  1}$中利用 **连接操作** 产生候选集 $C_k$
5. 对于$C_k$中每个元素$c$, 扫描数据库检查c是否是k-频繁项集
6. 是则把$C_k$加入到$L_k$
7. k = k+1, 若$L_{k - 1}$不是空， 则继续步骤4
8. 是关于串改用 $L_k$


连接操作： 两个有序k项集， 只有最后一项元素不同时， 则可以连接产生k+1项集， 比如 {A,C,D} 和 {A,C,E}， 连接产生 {A,C,D,E}

算法例子：最小支持度为50%
![](assets/shujuwajue/2018-05-19-15-14-44.png)
* C：统计每个1-项集的频率
* L：移除
1. C1：统计1-项集的频率
2. L1: D的支持度为$\frac{1}{4} < 50\%$
    理解： 对于规则$X\to Y$，$X\cup Y$就是表中的一个条目， 即最终的规则在这个项集内产生
3. C2: 连接操作， 1-项集各不相同， 因此两两连接产生{A,B},{A,C},...
4. C2：统计2-项集的频率
5. L2：{A,B},{A,E}的支持度小于50% (频率至少为2， 因为数据库总条目数为4， 4 * 50% 就是2)
6. C3：连接操作， 只有{B,C}和{B,E}可以连接
7. 算法结束， 最终规则必定从 {B,C,E}中产生


**如何从频繁集中产生关联规则？**
* 是否满足支持度？ 频繁集一定满足（定义）
* 是否满足置信度？
    （穷举法）对于频繁集的每个非空子集， 计算置信度

存在的问题：
1. 多次扫描数据库
2. 产生大量的候选集合

**算法改进**
循环终止条件改进: 满足$size(L_k) \leq k$即可结束算法
因为，若存在$L_{k + 1}$, 则其子集必定存在于$L_k$中，而这样的子集有k+1个， 因此, $L_k$至少有k+1个元素， 即$size(L_k) \geq k + 1$

**算法复杂性**指数复杂度， 对于n的频繁集， 其子集必然已被枚举出来， 因此至少指数级，n一般很小， 另外n也与最小支持度有关

**规则的另一个评价指标：**

$corr(A,B) = \frac{P(A\cup B)}{P(A)P(B)}$
* $> 1$， A与B正相关
* $= 1$， A与独立
* $< 1$， A与B负相关,可能存在的现象：应用规则之后， 效果反而不好了


### 基于频繁模式树的算法

Apriori算法存在的问题：会多次扫描， 并产生大量的候选集合

基于频繁模式树的算法改进：

1. 每一项按频率排序,删除部分非频繁模式
2. 构建频繁模式树
    1. 每一项表示树的一个路径（类似字典树）， 由此构建树
    2. 一个节点表示一个项集的一个元素， 对于相同的节点， 使用有向边连接（构建成链表便于访问）
3. 从频繁模式树种挖掘出频繁模式（见例子）
    1. 链表经过的节点所在路径上的点（例子中红色标注）， 每条路径上的点的任意组合， 取组成元素的频率的最小值为组成的频率
    2. 多条路径的相同的模式频率相加， 求得该模式的总频率
    3. 若总频率满足最小支持度，则是该模式是频繁模式

例子：
![](assets/shujuwajue/2018-05-19-15-51-23.png)
* min_support = 3， 因此，需要删除次数少于3的元素
    TID=100为例子， {d,g,i}在所有条目中统计少于3， 一定是非频繁模式，因此删除
* 除去有向边， 剩下的就是频繁集的结构
* Table 就是各个节点，有向边连接成的链表


包含p的频繁模式：
![](assets/shujuwajue/2018-05-19-15-56-36.png)
* p经过两条路径
    * 对于左边的路径{f,c,a,m,m}的任意组合都是待选的模式， 比如 {pc}，{pa}等， 对应的频率都是2（p出现了两次）
    * 两条路径统计， 只有{pc}的频率大于等于3， 因此{pc}是频繁集

包含m的频繁模式：
![](assets/shujuwajue/2018-05-19-16-08-28.png)
* 第一条路径m下边的p不再管， 这也是为什么从低频率的p开始产生频繁模式
* 包含m的频繁模式最终为{fcam}

包含b的频繁模式：
![](assets/shujuwajue/2018-05-19-16-10-32.png)
* 包含b的频繁模式为空

包含a的频繁模式：
![](assets/shujuwajue/2018-05-19-16-10-59.png)

包含c的频繁模式：
![](assets/shujuwajue/2018-05-19-16-11-29.png)


小结：
1. 通过树的结构来来表示整个数据库的条目，以减少空间消耗， 节点上的值，表示路径上的节点所代表的集合（前缀）的频率
2. 在频繁集的产生部分， 还是通过穷举的方法， 依旧是指数级

### 多维关联规则

基本思路： 与单维布尔关联规则挖掘类似

* 数据预处理： 如对数值数据离散化等
* 构建谓词数据集
* 利用 Apriori 算法挖掘频繁模式
* 得到多维关联规则

例子：

* 数据
    ![](assets/shujuwajue/2018-05-19-16-13-36.png)
* 离散化
    ![](assets/shujuwajue/2018-05-19-16-14-11.png)
* 挖掘频繁谓词模式
    ![](assets/shujuwajue/2018-05-19-16-15-08.png)
    ![](assets/shujuwajue/2018-05-19-16-15-41.png)
    ![](assets/shujuwajue/2018-05-19-16-15-55.png)
    ![](assets/shujuwajue/2018-05-19-16-16-24.png)

### 其他

科学研究的第四范式：2007年，已故的图灵奖得主吉姆·格雷（Jim Gray）在他最后一次演讲中描绘了数据密集型科研“第四范式”的愿景。将大数据科研从第三范式（计算机模拟）中分离出来单独作为一种科研范式，是因为其研究方式不同于基于数学模型的传统研究方式。