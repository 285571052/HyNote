<article>
<h1 class="csdn_top">深度学习（二十九）Batch Normalization 学习笔记</h1>
<div class="article_bar clearfix">
<div class="artical_tag">
<span class="original">
原创                </span>
<span class="time">2016年03月12日 17:00:38</span>
</div>

<ul class="article_tags clearfix csdn-tracking-statistics" data-mod="popu_377" style="display: none;">
<li class="tit">标签：</li>

<!--          [startarticletags]-->
<!--          [endarticletags]-->
</ul>
<ul class="right_bar">
<li><button class="btn-noborder"><i class="icon iconfont icon-read"></i><span class="txt">58862</span></button></li>
<!--                <li><button class="btn-noborder but-comment-topicon"><i class="icon iconfont icon-pinglun"></i><span class="txt">--><!--</span></button></li>-->
<!--                <li><button class="btn-noborder btn-like"><i  class="icon iconfont icon-xihuan-"></i><span class="txt">--><!--</span></button></li>-->
<li class="edit" style="display: none;">
<a class="btn-noborder" href="http://write.blog.csdn.net/postedit/50866313">
<i class="icon iconfont icon-bianji"></i><span class="txt">编辑</span>
</a>
</li>
<li class="del" style="display: none;">
<a class="btn-noborder" onclick="javascript:deleteArticle(fileName);return false;">
<i class="icon iconfont icon-shanchu"></i><span class="txt">删除</span>
</a>
</li>
</ul>
</div>
<div id="article_content" class="article_content csdn-tracking-statistics" data-mod="popu_519" data-dsm="post" style="overflow: hidden;">

<p style="text-align:center"><span style="font-family:Arial"><span style="font-size:24px"><strong>Batch Normalization 学习笔记</strong></span><br>
</span></p>
<p><span style="font-size:18px"><span style="font-family:Arial"><strong>原文地址</strong>：<a target="_blank" href="http://blog.csdn.net/hjimce/article/details/50866313">http://blog.csdn.net/hjimce/article/details/50866313</a></span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial"><strong>作者</strong>：hjimce</span></span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">一、背景意义</span></strong></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">本篇博文主要讲解2015年深度学习领域，非常值得学习的一篇文献：《Batch Normalization: Accelerating Deep Network Training by &nbsp;Reducing Internal Covariate Shift》，这个算法目前已经被大量的应用，最新的文献算法很多都会引用这个算法，进行网络训练，可见其强大之处非同一般啊。</span></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px">近年来深度学习捷报连连、声名鹊起，随机梯度下架成了训练深度网络的主流方法。</span><span style="font-size:18px">尽管随机梯度下降法对于训练深度网络简单高效，但是它有个毛病，就是需要我们人为的去选择参数，比如学习率、参数初始化、权重衰减系数、Drop out比例等。这些参数的选择对训练结果至关重要，以至于我们很多时间都浪费在这些的调参上。那么学完这篇文献之后，你可以不需要那么刻意的慢慢调整参数。BN算法（<span style="font-size:18px">Batch
Normalization）</span>其强大之处如下：</span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">(1)你可以选择比较大的初始学习率，让你的训练速度飙涨。以前还需要慢慢调整学习率，甚至在网络训练到一半的时候，还需要想着学习率进一步调小的比例选择多少比较合适，现在我们可以采用初始很大的学习率，然后学习率的衰减速度也很大，因为这个算法收敛很快。<span style="font-size:18px">当然这个算法即使你选择了较小的学习率，也比以前的收敛速度快，因为它具有快速训练收敛的特性；</span></span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">(2)你再也不用去理会过拟合中drop out、L2正则项参数的选择问题，采用BN算法后，你可以移除这两项了参数，或者可以选择更小的L2正则约束参数了，因为BN具有提高网络泛化能力的特性；</span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">(3)再也不需要使用使用局部响应归一化层了（局部响应归一化是Alexnet网络用到的方法，搞视觉的估计比较熟悉），因为BN本身就是一个归一化网络层；</span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">(4)可以把训练数据彻底打乱（防止每批训练的时候，某一个样本都经常被挑选到，文献说这个可以提高1%的精度，这句话我也是百思不得其解啊）。</span></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px">开始讲解算法前，先来思考一个问题：</span><span style="font-size:18px">我们知道在神经网络训练开始前，都要对输入数据做一个归一化处理，那么具体为什么需要归一化呢？归一化后有什么好处呢？原因在于</span><span style="font-size:18px">神经网络学习过程本质就是为了学习数据分布，一旦训练数据与测试数据的分布不同，那么网络的泛化能力也大大降低；另外一方面，一旦每批训练数据的分布各不相同(batch
梯度下降)，那么网络就要在每次迭代都去学习适应不同的分布，这样将会大大降低网络的训练速度，这也正是为什么我们需要对数据都要做一个归一化预处理的原因。</span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial"><span style="font-size:18px">对于深度网络的训练是一个复杂的过程，只要网络的前面几层发生微小的改变，那么后面几层就会被累积放大下去。一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度。</span><br>
</span></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px">我们知道网络一旦train起来，那么参数就要发生更新，除了输入层的数据外(因为输入层数据，我们已经人为的为每个样本归一化)，后面网络每一层的输入数据分布是一直在发生变化的，因为在训练的时候，前面层训练参数的更新将导致后面层输入数据分布的变化。以网络第二层为例：网络的第二层输入，是由第一层的参数和input计算得到的，而第一层的参数在整个训练过程中一直在变化，因此必然会引起后面每一层输入数据分布的改变。我们把网络中间层在训练过程中，数据分布的改变称之为：“Internal
&nbsp;Covariate&nbsp;Shift”。</span><span style="font-size:18px">Paper</span><span style="font-size:18px">所提出的算法，就是要解决在训练过程中，中间层数据分布发生改变的情况，于是就有了</span><span style="font-size:18px">Batch&nbsp;&nbsp;Normalization</span><span style="font-size:18px">，这个牛逼算法的诞生。</span></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px"></span></span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">二、初识BN(<span style="font-size:18px">Batch&nbsp;&nbsp;Normalization</span>)</span></strong></span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">1、BN概述</span></strong></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px"><span style="font-size:18px; text-align:center">就像激活函数层、卷积层、全连接层、池化层一样，BN(Batch Normalization)也属于网络的一层</span></span><span style="font-size:18px">。在前面我们提到网络除了输出层外，其它层因为低层网络在训练的时候更新了参数，而引起后面层输入数据分布的变化。这个时候我们可能就会想，如果在每一层输入的时候，再加个预处理操作那该有多好啊，比如网络第三层输入数据</span><span style="font-size:18px">X3(X3</span><span style="font-size:18px">表示网络第三层的输入数据</span><span style="font-size:18px">)</span><span style="font-size:18px">把它归一化至：均值</span><span style="font-size:18px">0</span><span style="font-size:18px">、方差为</span><span style="font-size:18px">1，然后再输入第三层计算</span><span style="font-size:18px">，这样我们就可以解决前面所提到的“</span><span style="font-size:18px">Internal&nbsp;Covariate&nbsp;Shift</span><span style="font-size:18px">”的问题了。</span></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px">而事实上，</span><span style="font-size:18px">paper</span><span style="font-size:18px">的算法本质原理就是这样：在网络的每一层输入的时候，又插入了一个归一化层，也就是先做一个归一化处理，然后再进入网络的下一层。不过文献归一化层，可不像我们想象的那么简单，它是一个可学习、有参数的网络层。</span></span><span style="font-family:Arial; font-size:18px">既然说到数据预处理，下面就先来复习一下最强的预处理方法：白化。</span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">2、预处理操作选择</span></strong></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px">说到神经网络输入数据预处理，最好的算法莫过于白化预处理。然而</span><span style="font-size:18px">白化计算量太大了，很不划算，还有就是白化不是处处可微的，所以在深度学习中，其实很少用到白化。经过白化预处理后，数据满足条件：</span><span style="font-size:18px">a</span><span style="font-size:18px">、特征之间的相关性降低，这个就相当于</span><span style="font-size:18px">pca；<span style="font-size:18px">b</span><span style="font-size:18px">、数据均值、标准差归一化，也就是使得<span style="font-size:18px">每一维特征</span>均值为</span><span style="font-size:18px">0，标准差为1</span></span><span style="font-size:18px">。<span style="font-size:18px">如果数据特征维数比较大，要进行</span><span style="font-size:18px">PCA</span><span style="font-size:18px">，也就是实现白化的第1个要求，是需要计算特征向量，计算量非常大，于是为了简化计算，作者忽略了第1个要求，仅仅使用了下面的公式进行预处理，也就是近似白化预处理：</span></span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-family:Arial"><img src="http://img.blog.csdn.net/20160312181715397" alt="" width="158" height="50"><br>
</span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">公式简单粗糙，但是依旧很牛逼。因此后面我们也将用这个公式，对某一个层网络的输入数据做一个归一化处理。需要注意的是，我们训练过程中采用batch 随机梯度下降，上面的E(xk)指的是每一批训练数据神经元xk的平均值；然后分母就是每一批数据神经元xk激活度的一个标准差了。</span></span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">三、BN算法实现</span></strong></span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">1、BN算法概述</span></strong></span></p>
<p><span style="font-family:Arial"><span style="font-size:18px">经过前面简单介绍，这个时候可能我们会想当然的以为：好像很简单的样子，不就是在网络中间层数据做一个归一化处理嘛，这么简单的想法，为什么之前没人用呢？然而其实实现起来并不是那么简单的。</span><span style="font-size:18px">其实如果是仅仅使用上面的归一化公式，对网络某一层A的输出数据做归一化，然后送入网络下一层B，这样是会影响到本层网络A所学习到的特征的。打个比方，比如我网络中间某一层学习到特征数据本身就分布在S型激活函数的两侧，你强制把它给我归一化处理、标准差也限制在了1，把数据变换成分布于s函数的中间部分，这样就相当于我这一层网络所学习到的特征分布被你搞坏了，这可怎么办？</span><span style="font-size:18px">于是文献使出了一招惊天地泣鬼神的招式：变换重构，引入了可学习参数</span><span style="font-size:18px">γ、</span><span style="font-size:18px">β，这就是算法关键之处：</span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-family:Arial"><img src="http://img.blog.csdn.net/20160312190113493" alt="" width="184" height="29">&nbsp;</span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">每一个神经元xk都会有一对这样的参数<span style="font-size:18px">γ、</span><span style="font-size:18px">β。这样其实当：</span></span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><span style="font-family:Arial"><img src="http://img.blog.csdn.net/20160312190323411" alt="" width="150" height="26">、<img src="http://img.blog.csdn.net/20160312190336072" alt="" width="98" height="21"><br>
</span></span></span></p>
<p style="text-align:left"><span style="font-family:Arial"><span style="font-size:18px"><span style="font-size:18px">是可以恢复出原始的某一层所学到的特征的。因此我们引入了这个可学习重构参数<span style="font-size:18px">γ、</span><span style="font-size:18px">β</span>，让我们的网络可以学习恢复出原始网络所要学习的特征分布。最后</span></span><span style="font-size:18px">Batch&nbsp;Normalization网络层的前向传导过程公式就是：</span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-family:Arial">&nbsp;<img src="http://img.blog.csdn.net/20160312190726792" alt="" width="362" height="175"></span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">上面的公式中m指的是mini-batch&nbsp;size。</span></span></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">2、源码实现</span></strong></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial"></span></span></p>
<div class="dp-highlighter bg_python"><div class="bar"><div class="tools"><b>[python]</b> <a href="#" class="ViewSource" title="view plain" onclick="dp.sh.Toolbar.Command('ViewSource',this);return false;">view plain</a><span class="tracking-ad" data-mod="popu_168"> <a href="#" class="CopyToClipboard" title="copy" onclick="dp.sh.Toolbar.Command('CopyToClipboard',this);return false;">copy</a><div style="position: absolute; left: 232px; top: 2562px; width: 16px; height: 16px; z-index: 99;"><embed id="ZeroClipboardMovie_1" src="http://static.blog.csdn.net/scripts/ZeroClipboard/ZeroClipboard.swf" loop="false" menu="false" quality="best" bgcolor="#ffffff" width="16" height="16" name="ZeroClipboardMovie_1" align="middle" allowscriptaccess="always" allowfullscreen="false" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" flashvars="id=1&amp;width=16&amp;height=16" wmode="transparent"></div></span><span class="tracking-ad" data-mod="popu_169"> <a href="#" class="PrintSource" title="print" onclick="dp.sh.Toolbar.Command('PrintSource',this);return false;">print</a></span><a href="#" class="About" title="?" onclick="dp.sh.Toolbar.Command('About',this);return false;">?</a></div></div><ol start="1" class="dp-py"><li class="alt"><span><span>m&nbsp;=&nbsp;K.mean(X,&nbsp;axis=-</span><span class="number">1</span><span>,&nbsp;keepdims=</span><span class="special">True</span><span>)</span><span class="comment">#计算均值</span><span>&nbsp;&nbsp;</span></span></li><li class=""><span>std&nbsp;=&nbsp;K.std(X,&nbsp;axis=-<span class="number">1</span><span>,&nbsp;keepdims=</span><span class="special">True</span><span>)</span><span class="comment">#计算标准差</span><span>&nbsp;&nbsp;</span></span></li><li class="alt"><span>X_normed&nbsp;=&nbsp;(X&nbsp;-&nbsp;m)&nbsp;/&nbsp;(std&nbsp;+&nbsp;<span class="special">self</span><span>.epsilon)</span><span class="comment">#归一化</span><span>&nbsp;&nbsp;</span></span></li><li class=""><span>out&nbsp;=&nbsp;<span class="special">self</span><span>.gamma&nbsp;*&nbsp;X_normed&nbsp;+&nbsp;</span><span class="special">self</span><span>.beta</span><span class="comment">#重构变换</span><span>&nbsp;&nbsp;</span></span></li></ol></div><pre code_snippet_id="1608106" snippet_file_name="blog_20160313_1_181672" name="code" class="python" style="font-weight: bold; display: none;" data-initialized="true" data-gclp-id="0">
m = K.mean(X, axis=-1, keepdims=True)#计算均值
std = K.std(X, axis=-1, keepdims=True)#计算标准差
X_normed = (X - m) / (std + self.epsilon)#归一化
out = self.gamma * X_normed + self.beta#重构变换</pre><span style="font-size:18px">上面的x是一个二维矩阵，对于源码的实现就几行代码而已，轻轻松松。</span><br>
<p></p>
<p><span style="font-size:18px"><strong><span style="font-family:Arial">3、实战使用</span></strong></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">(1)可能学完了上面的算法，你只是知道它的一个训练过程，一个网络一旦训练完了，就没有了min-batch这个概念了。测试阶段我们一般只输入一个测试样本，看看结果而已。因此测试样本，前向传导的时候，上面的均值u、标准差σ&nbsp;要哪里来？其实网络一旦训练完毕，参数都是固定的，这个时候即使是每批训练样本进入网络，那么BN层计算的均值u、和标准差都是固定不变的。我们可以采用这些数值来作为测试样本所需要的均值、标准差，于是最后测试阶段的u和<span style="font-size:18px">σ
计算公式如下：</span></span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><span style="font-family:Arial"><img src="http://img.blog.csdn.net/20160312211219045?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center" alt="" width="156" height="51"><br>
</span></span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-size:18px"><span style="font-family:Arial">上面简单理解就是：对于均值来说直接计算所有batch u值的平均值；然后对于标准偏差采用每个batch&nbsp;<span style="font-size:18px">σB的</span>无偏估计。最后测试阶段，BN的使用公式就是：</span></span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-size:18px"><span style="font-family:Arial"><img src="http://img.blog.csdn.net/20160312212017883" alt="" width="292" height="39"><br>
</span></span></span></p>
<p><span style="font-size:18px"><span style="font-family:Arial">(2)根据文献说，BN可以应用于一个神经网络的任何神经元上。文献主要是把BN变换，置于网络激活函数层的前面。在没有采用BN的时候，激活函数层是这样的：</span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-family:Arial"><span style="font-size:18px; text-align:center">z=g(Wu+b)</span><br>
</span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">也就是我们希望一个激活函数，比如s型函数s(x)的自变量x是经过BN处理后的结果。因此前向传导的计算公式就应该是：</span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-family:Arial">z=g(BN(Wu+b))</span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">其实因为偏置参数b经过BN层后其实是没有用的，最后也会被均值归一化，当然BN层后面还有个<span style="font-size:18px">β参数作为偏置项，所以b这个参数就可以不用了。因此最后把BN层+激活函数层就变成了：</span></span></span></p>
<p style="text-align:center"><span style="font-size:18px"><span style="font-family:Arial">z=g(BN(Wu))</span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial"><strong>四、<span style="font-size:18px">Batch Normalization在CNN中的使用</span></strong></span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">通过上面的学习，我们知道BN层是对于每个神经元做归一化处理，甚至只需要对某一个神经元进行归一化，而不是对一整层网络的神经元进行归一化。既然BN是对单个神经元的运算，那么在CNN中卷积层上要怎么搞？假如某一层卷积层有6个特征图，每个特征图的大小是100*100，这样就相当于这一层网络有6*100*100个神经元，如果采用BN，就会有6*100*100个<span style="font-size:18px">参数</span><span style="font-size:18px">γ、</span><span style="font-size:18px">β，这样岂不是太恐怖了。因此卷积层上的BN使用，其实也是使用了类似权值共享的策略，把一整张特征图当做一个神经元进行处理。</span></span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial"><span style="font-size:18px"></span></span></span></p>
<p><span style="font-family:Arial; font-size:18px">卷积神经网络经过卷积后得到的是一系列的特征图，如果min-batch&nbsp;sizes为m，那么网络某一层输入数据可以表示为四维矩阵(m,f,p,q)，m为min-batch&nbsp;sizes，f为特征图个数，p、q分别为特征图的宽高。在cnn中我们可以把每个特征图看成是一个特征处理（一个神经元），因此在使用Batch&nbsp;Normalization，mini-batch&nbsp;size 的大小就是：m*p*q，于是对于每个特征图都只有一对可学习参数：γ、β。说白了吧，这就是相当于求取<span style="font-family:Arial; font-size:18px">所有样本所对应的</span>一个特征图的所有神经元的平均值、方差，然后对这个特征图神经元做归一化。</span><span style="font-family:Arial; font-size:18px">下面是来自于keras卷积层的BN实现一小段主要源码：</span></p>
<p></p>
<div class="dp-highlighter bg_python"><div class="bar"><div class="tools"><b>[python]</b> <a href="#" class="ViewSource" title="view plain" onclick="dp.sh.Toolbar.Command('ViewSource',this);return false;">view plain</a><span class="tracking-ad" data-mod="popu_168"> <a href="#" class="CopyToClipboard" title="copy" onclick="dp.sh.Toolbar.Command('CopyToClipboard',this);return false;">copy</a><div style="position: absolute; left: 232px; top: 3619px; width: 16px; height: 16px; z-index: 99;"><embed id="ZeroClipboardMovie_2" src="http://static.blog.csdn.net/scripts/ZeroClipboard/ZeroClipboard.swf" loop="false" menu="false" quality="best" bgcolor="#ffffff" width="16" height="16" name="ZeroClipboardMovie_2" align="middle" allowscriptaccess="always" allowfullscreen="false" type="application/x-shockwave-flash" pluginspage="http://www.macromedia.com/go/getflashplayer" flashvars="id=2&amp;width=16&amp;height=16" wmode="transparent"></div></span><span class="tracking-ad" data-mod="popu_169"> <a href="#" class="PrintSource" title="print" onclick="dp.sh.Toolbar.Command('PrintSource',this);return false;">print</a></span><a href="#" class="About" title="?" onclick="dp.sh.Toolbar.Command('About',this);return false;">?</a></div></div><ol start="1" class="dp-py"><li class="alt"><span><span>input_shape&nbsp;=&nbsp;</span><span class="special">self</span><span>.input_shape&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;reduction_axes&nbsp;=&nbsp;list(range(len(input_shape)))&nbsp;&nbsp;</span></li><li class="alt"><span>&nbsp;<span class="keyword">del</span><span>&nbsp;reduction_axes[</span><span class="special">self</span><span>.axis]&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;broadcast_shape&nbsp;=&nbsp;[<span class="number">1</span><span>]&nbsp;*&nbsp;len(input_shape)&nbsp;&nbsp;</span></span></li><li class="alt"><span>&nbsp;broadcast_shape[<span class="special">self</span><span>.axis]&nbsp;=&nbsp;input_shape[</span><span class="special">self</span><span>.axis]&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;<span class="keyword">if</span><span>&nbsp;train:&nbsp;&nbsp;</span></span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;m&nbsp;=&nbsp;K.mean(X,&nbsp;axis=reduction_axes)&nbsp;&nbsp;</span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;brodcast_m&nbsp;=&nbsp;K.reshape(m,&nbsp;broadcast_shape)&nbsp;&nbsp;</span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;std&nbsp;=&nbsp;K.mean(K.square(X&nbsp;-&nbsp;brodcast_m)&nbsp;+&nbsp;<span class="special">self</span><span>.epsilon,&nbsp;axis=reduction_axes)&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;std&nbsp;=&nbsp;K.sqrt(std)&nbsp;&nbsp;</span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;brodcast_std&nbsp;=&nbsp;K.reshape(std,&nbsp;broadcast_shape)&nbsp;&nbsp;</span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;mean_update&nbsp;=&nbsp;<span class="special">self</span><span>.momentum&nbsp;*&nbsp;</span><span class="special">self</span><span>.running_mean&nbsp;+&nbsp;(</span><span class="number">1</span><span>-</span><span class="special">self</span><span>.momentum)&nbsp;*&nbsp;m&nbsp;&nbsp;</span></span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;std_update&nbsp;=&nbsp;<span class="special">self</span><span>.momentum&nbsp;*&nbsp;</span><span class="special">self</span><span>.running_std&nbsp;+&nbsp;(</span><span class="number">1</span><span>-</span><span class="special">self</span><span>.momentum)&nbsp;*&nbsp;std&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span class="special">self</span><span>.updates&nbsp;=&nbsp;[(</span><span class="special">self</span><span>.running_mean,&nbsp;mean_update),&nbsp;&nbsp;</span></span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(<span class="special">self</span><span>.running_std,&nbsp;std_update)]&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_normed&nbsp;=&nbsp;(X&nbsp;-&nbsp;brodcast_m)&nbsp;/&nbsp;(brodcast_std&nbsp;+&nbsp;<span class="special">self</span><span>.epsilon)&nbsp;&nbsp;</span></span></li><li class="alt"><span>&nbsp;<span class="keyword">else</span><span>:&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;brodcast_m&nbsp;=&nbsp;K.reshape(<span class="special">self</span><span>.running_mean,&nbsp;broadcast_shape)&nbsp;&nbsp;</span></span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;brodcast_std&nbsp;=&nbsp;K.reshape(<span class="special">self</span><span>.running_std,&nbsp;broadcast_shape)&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;X_normed&nbsp;=&nbsp;((X&nbsp;-&nbsp;brodcast_m)&nbsp;/&nbsp;&nbsp;</span></li><li class="alt"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(brodcast_std&nbsp;+&nbsp;<span class="special">self</span><span>.epsilon))&nbsp;&nbsp;</span></span></li><li class=""><span>&nbsp;out&nbsp;=&nbsp;K.reshape(<span class="special">self</span><span>.gamma,&nbsp;broadcast_shape)&nbsp;*&nbsp;X_normed&nbsp;+&nbsp;K.reshape(</span><span class="special">self</span><span>.beta,&nbsp;broadcast_shape)&nbsp;&nbsp;</span></span></li></ol></div><pre code_snippet_id="1608106" snippet_file_name="blog_20160313_2_9199369" name="code" class="python" style="display: none;" data-initialized="true" data-gclp-id="1">
input_shape = self.input_shape
reduction_axes = list(range(len(input_shape)))
del reduction_axes[self.axis]
broadcast_shape = [1] * len(input_shape)
broadcast_shape[self.axis] = input_shape[self.axis]
if train:
m = K.mean(X, axis=reduction_axes)
brodcast_m = K.reshape(m, broadcast_shape)
std = K.mean(K.square(X - brodcast_m) + self.epsilon, axis=reduction_axes)
std = K.sqrt(std)
brodcast_std = K.reshape(std, broadcast_shape)
mean_update = self.momentum * self.running_mean + (1-self.momentum) * m
std_update = self.momentum * self.running_std + (1-self.momentum) * std
self.updates = [(self.running_mean, mean_update),
(self.running_std, std_update)]
X_normed = (X - brodcast_m) / (brodcast_std + self.epsilon)
else:
brodcast_m = K.reshape(self.running_mean, broadcast_shape)
brodcast_std = K.reshape(self.running_std, broadcast_shape)
X_normed = ((X - brodcast_m) /
(brodcast_std + self.epsilon))
out = K.reshape(self.gamma, broadcast_shape) * X_normed + K.reshape(self.beta, broadcast_shape)</pre>
<p></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">个人总结：2015年个人最喜欢深度学习的一篇paper就是<span style="font-size:18px">Batch Normalization这篇文献</span>，采用这个方法网络的训练速度快到惊人啊，感觉训练速度是以前的十倍以上，再也不用担心自己这破电脑每次运行一下，训练一下都要跑个两三天的时间。另外这篇文献跟空间变换网络《Spatial
Transformer Networks》的思想神似啊，都是一个变换网络层。</span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">参考文献：</span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">1、<span style="font-size:18px">《Batch Normalization: Accelerating Deep Network Training by &nbsp;Reducing Internal Covariate Shift》</span></span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial">2、<span style="font-size:18px">《</span><span style="font-size:18px">Spatial Transformer Networks</span><span style="font-size:18px">》</span></span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial"><span style="font-size:18px">3、<a target="_blank" href="https://github.com/fchollet/keras">https://github.com/fchollet/keras</a></span></span></span></p>
<p style="text-align:left"><span style="font-size:18px"><span style="font-family:Arial"><span style="color:rgb(51,51,51); line-height:26px; font-size:18px">**********************作者：hjimce &nbsp; 时间：2016.3.12 &nbsp;联系QQ：1393852684</span><span style="color:rgb(51,51,51); line-height:26px; font-size:18px">&nbsp;
&nbsp;原创文章，转载请保留作者、原文地址信息********************</span></span><br>
</span></p>
<p><br>
</p>
</div>
</article>