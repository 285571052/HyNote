<!--toc-->

- [EM算法](#em算法)
	- [从极大似然法说起-一个简单的例子](#从极大似然法说起-一个简单的例子)
	- [从极大似然法说起-一个稍微复杂点的例子-三硬币模型](#从极大似然法说起-一个稍微复杂点的例子-三硬币模型)
	- [EM算法的具体步骤 和 隐变量](#em算法的具体步骤-和-隐变量)
		- [期望](#期望)
	- [更一般形式的描述](#更一般形式的描述)
	- [EM算法的证明](#em算法的证明)
	- [EM算法的应用-高斯混合聚类](#em算法的应用-高斯混合聚类)

<!-- tocstop -->
# EM算法
EM算法(期望最大化算法,Expectation Maximization Algorithm)是一种迭代算法,用于隐变量(hidden variable)的概率模型参数的极大似然估计,或极大后延概率估计。

EM算法由两部组成:
1. E步
    求期望
2. M步
    求极大值

## 从极大似然法说起-一个简单的例子
假设有一枚硬币,抛出硬币,其为正面的概率为p,已知m次试验的结果$\{x_1,x_2,...,x_m\}$,使用极大似然法估计参数p

一次试验的概率为:$P(x) = p^x\times(1-p)^{1-x}$

似然函数为:
$L(x_1,x_2,\ldots,x_m;p) = \displaystyle{\prod_{i = 1}^m} P(x_i) = p^{\displaystyle{\sum_{i = 1}^m} x_i}\cdot(1-p)^{m - \displaystyle{\sum_{i = 1}^m} x_i}$

则令$\frac{\partial L(x_1,x_2,\ldots,x_m)}{\partial p} = 0$
得:$p = \frac{ \displaystyle {\sum_{i = 1}^m} x_i}{m}$

## 从极大似然法说起-一个稍微复杂点的例子-三硬币模型
假设要有3枚硬币,分别记作A,B,C。这些硬币正面出现的概率分别为$\pi,p,q$。进行如下试验:
先抛出硬币A,根据其结果选出硬币B或C,正面选B,反面选C
然后抛出险种的硬币,如果出现正面,记作1,反面记作0

现有m次试验的结果$D = \{(z_1,y_1),(z_2,y_2),\ldots,(z_m,y_m)\}$,
$z_i$表示第i次试验中,抛出硬币A的结果,$y_i$表示第i次试验抛出第二枚硬币的结果,结果正面为1,反面为0
使用极大似然法估计参数$\theta = (\pi, p,q)^T$

一次试验的概率为:$P(y,z) = \pi^zp^{zy}(1-p)^{z(1-y)} \times (1-\pi)^{1-z}q^{(1-z)y}(1-q)^{(1-z)(1-y)}$

似然函数为:
$L(D;\theta) = \displaystyle{\prod_{i = 1}^m} P(y_i,z_i) =  \pi^{\displaystyle {\sum_{i = 1}^m} z_i}p^{\displaystyle {\sum_{i = 1}^m}z_iy_i}(1-p)^{\displaystyle {\sum_{i = 1}^m}z_i(1-y_i)} \times (1-\pi)^{\displaystyle {\sum_{i = 1}^m}(1-z_i)}q^{\displaystyle {\sum_{i = 1}^m}(1-z_i)y_i}(1-q)^{\displaystyle {\sum_{i = 1}^m}(1-z_i)(1-y_i)}$
为简化运算,取对数形式:

$LL(D;\theta) = \bigg (\displaystyle {\sum_{i = 1}^m} z_i\bigg ) \ln \pi +
\bigg (\displaystyle {\sum_{i = 1}^m}z_iy_i\bigg ) \ln p +
\bigg (\displaystyle {\sum_{i = 1}^m}z_i(1-y_i)\bigg ) \ln (1-p) +
\bigg (\displaystyle {\sum_{i = 1}^m}(1-z_i)\bigg ) \ln (1-\pi) +
\bigg (\displaystyle {\sum_{i = 1}^m}(1-z_i)y_i\bigg ) \ln q +
\bigg (\displaystyle {\sum_{i = 1}^m}(1-z_i)(1-y_i)\bigg ) \ln (1-q)$

$\begin{align}
\frac{\partial LL(D;\theta)}{\partial \theta} &= \begin{bmatrix}
\frac{\partial LL(D;\theta)}{\partial \pi}  \\
\frac{\partial LL(D;\theta)}{\partial p}  \\
\frac{\partial LL(D;\theta)}{\partial q}
\end{bmatrix} \\
&= \begin{bmatrix}
\frac{\displaystyle {\sum_{i = 1}^m} z_i}{\pi} - \frac{\displaystyle{\sum_{i = 1}^m} (1 - z_i)}{1 - \pi}  \\
\frac{\displaystyle {\sum_{i = 1}^m}z_iy_i}{p} - \frac{\displaystyle {\sum_{i = 1}^m}z_i(1-y_i)}{1-p}\\
\frac{\displaystyle {\sum_{i = 1}^m}z_iy_i}{q} - \frac{\displaystyle {\sum_{i = 1}^m}z_i(1-y_i)}{1-q}
\end{bmatrix}
\end{align}$

令$\frac{\partial LL(D;\theta)}{\partial \theta}=0$
得:
$\pi = \frac{ \displaystyle {\sum_{i = 1}^m} z_i}{m}$
$p = \frac{ \displaystyle {\sum_{i = 1}^m} z_iy_i}{m}$
$q = \frac{ \displaystyle {\sum_{i = 1}^m} z_iy_i}{m}$

## EM算法的具体步骤 和 隐变量
上面的例子中,隐变量或者说中间变量$z$往往是未知的,那么我们要怎么进行参数的估计?

按照上面的过程,极大似然法无法进行下去,因为我们并不知道输出的结果$y_i$到底是由B硬币生成的,还是C硬币生成的。

使用极大似然法,在求似然函数的时候,需要知道$y_i$和$z_i$(即样本的生成过程信息),但我们不知道$z_i$到底是多少,所以极大似然法无法进行下去。

EM算法的想法就是使用隐变量$z_i$的概率取值来代替$z_i$的真实值(每个取值,以其后验概率作为权重),并且使用 **似然函数的期望** 来代替 **似然函数**
~~具体表现为:使用$z_i$的后验概率$P(z_i|y_i,\theta)$,来猜测$z_i$,这样就避开的$z_i$的实际取值为多少。~~
具体含义见期望的表达式(关于期望见后面的详细信息)。
这样就避开的z的真实取值。

$\begin{align}
E(LL(D;\theta))
&= E\bigg(\ln \displaystyle{\prod_{i = 1}^m} P(y_i,z_i)\bigg) = E\bigg( \displaystyle{\sum_{i = 1}^m} \ln P(y_i,z_i)\bigg)
\\
&= \displaystyle{\sum_{i = 1}^m}  \displaystyle{\sum_{z = 0}^1}
P(z|y_i)
\ln P(y_i,z)
\\
&= \displaystyle{\sum_{i = 1}^m}  \displaystyle{\sum_{z = 0}^1}
P(z|y_i)
\bigg(
z \ln \pi +
zy_i \ln p +
z(1-y_i) \ln (1-p) +
(1-z) \ln (1-\pi) +
(1-z)y_i \ln q +
(1-z)(1-y_i) \ln (1-q) \bigg)
\end{align}$

因为我们不知道$z_i$到底是多少,因此我们从概率的角度来考虑,我们列举了$z_i$的所有可能值,使用后验概率作为其权重。

注意:在EM算法中,$P(z|y_i)$和参数$\theta$有关,但是把$P(z|y_i)$这个量当作一个常量,具体原因见后面EM算法迭代的过程及其证明。我们先继续沿着极大似然法这个思路算下去。

为方便,记$P(z|y_i) = \lambda_{zi}$

令$\frac{\partial E\big(LL(D;\theta)\big)}{\partial \theta} = 0$
得:
$\pi = \frac{ \displaystyle{\sum_{i = 1}^m}\displaystyle{\sum_{z = 0}^1} z\lambda_{zi}}{\displaystyle{\sum_{i = 1}^m}\displaystyle{\sum_{z = 0}^1} \lambda_{zi}}$

$p = \frac{ \displaystyle{\sum_{i = 1}^m}\displaystyle{\sum_{z = 0}^1} zy_i\lambda_{zi}}{\displaystyle{\sum_{i = 1}^m}\displaystyle{\sum_{z = 0}^1} z\lambda_{zi}}$

$q = \frac{ \displaystyle{\sum_{i = 1}^m}\displaystyle{\sum_{z = 0}^1} (1-z)y_i\lambda_{zi}}{\displaystyle{\sum_{i = 1}^m}\displaystyle{\sum_{z = 0}^1} (1-z)\lambda_{zi}}$

$\lambda_{zi} = P(z|y_i) = \frac{P(y_i,z)}{\displaystyle{\sum_{z_j = 0}^1} P(y_i,z_j)}$

EM算法是一个迭代算法,$\lambda_{zi}$为当前参数$\theta^{(i)}$下$z$的后验概率,而上面$\pi,p,q$的等式,则是$\theta^{(i+1)} = \begin{matrix}
argmax\\
\theta
\end{matrix} E\big( LL(D;\theta) \big)$的解
迭代的过程,使用了上一此迭代的结果$\lambda_{zi}$
这也解释了为什么把$\lambda_{zi}$看作常量,因为这个量来自于上一次迭代输出的参数,而和这一次迭代的需要更新的参数无关。

我们整理一下这题EM算法的具体过程:
0. 随机取参数$\theta$
1. E步:计算隐变量$\lambda_{zi}$的值
2. M步:更新参数$\theta$
3. 重复2,3步骤,直到满足停止条件

EM算法有效性的证明后面讲。

为什么不直接使用z的后验概率展开到似然函数中,而非得大费周章地使用似然函数的期望再展开呢?
直接使用后验概率,它的表达式为:
$L(D;\theta) = \displaystyle{\prod_{i = 1}^m} P(y_i,z_i) = \displaystyle{\prod_{i = 1}^m}  \displaystyle{\sum_{z = 0}^1}
P(z|y_i)P(y_i,z)$
取对数之后为:$LL(D;\theta) = \ln \displaystyle{\prod_{i = 1}^m} P(y_i,z_i) = \displaystyle{\sum_{i = 1}^m} \ln \displaystyle{\sum_{z = 0}^1}
P(z|y_i)P(y_i,z)$
而使用似然函数期望对应的表达式为:$E(LL(D;\theta)) = \displaystyle{\sum_{i = 1}^m}  \displaystyle{\sum_{z = 0}^1}
P(z|y_i)
\ln P(y_i,z)$

无论是没取对数(有连乘),还是取了对数(有$\ln \sum$),其求导运算都不好算,而似然函数的期望,则把$\ln$放在了$\sum$的里面,显然运算更加简单。

### 期望
我们先讲一下期望的概念:
在概率论和统计学中,一个离散性随机变量的期望值(或数学期望,或均值,亦简称期望,物理学中称为期待值)是试验中每次可能结果的概率乘以其结果的总和。
采用形式化定义,设$y$是随机变量$X$的函数,$Y=g(x)$那,那么
1. X是离散型随机变量,它的分布律为$P(X = x_k) = p_k,k = 1,2,ldots,$若$\displaystyle{\sum_{k = 1}^\infty} p_kg(x_k)$绝对收敛,则期望计算值为$E(Y) = E(g(X)) = \displaystyle{\sum_{k = 1}^\infty} p_kg(x_k)$

2. X是连续型随机变量,存在相应的概率魔都函数f(x),若积分$\int_{-\infty}^{\infty} g(x)f(x)dx$绝对收敛,则期望值计算为$E(Y) = E(g(X)) = \int_{-\infty}^{\infty} g(x)f(x)dx$

## 更一般形式的描述
有样本$D = \{y_1,y_2,\ldots,y_m\}$,已知每个样本的密度函数均为$P(y,z|\theta)$,估计该密度函数参数的值。
$y$为输出的结果(即样本),$z$为样本生成过程中的中间变量,$\theta$为分布的参数。

由题意,可以得到联合密度(对数形式)为:$LL(D;\theta) = \displaystyle{\sum_{i = 1}^m} \ln P(y_i,z_i|\theta)$
因为我们并不知道 $z_i$的值为多少,我们采用$z_i$的概率取值,并且以其后验概率为权重。
我们希望简化运算,因此我们取似然函数的期望:
$\begin{align}
E\big( LL(D;\theta) \big) &= E\big( \displaystyle{\sum_{i = 1}^m} \ln P(y_i,z_i|\theta) \big) \\
&= \displaystyle{\sum_{i = 1}^m} \displaystyle{\sum_{z}} p(z|y_i)\ln P(y_i,z_i|\theta)
\end{align}$

令$\frac{\partial E\big( LL(D;\theta) \big)}{\partial \theta} = 0$,得到$\theta$关于$p(z|y_i)$的表达式

至此,EM算法的准备工作完毕,下面进行EM算法的迭代过程:
1. 随机取$\theta$的初始值
2. E步:求似然函数期望的表达式
	 即 计算$z_i$的后验概率 $p(z_i|y_i)$
3. M步:最大化参数
	即 $\theta^{(i + 1)} = \begin{matrix} argmax \\ \theta\end{matrix} E\big( LL(D;\theta) \big)_{\theta^{(i)}}$<!--_-->
	具体的计算就是上面,对应参数的导数为0求解。

4. 重复2,3步骤,直到达到停止条件
		停止条件可以是达到最大轮迭代数

至此,就可以得到参数的估计值了。

## EM算法的证明
为什么EM算法这样迭代是有效的?
我们可以通过比较两个

## EM算法的应用-高斯混合聚类
