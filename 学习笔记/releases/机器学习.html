<!DOCTYPE html>
  <html>
    <head>
      <title>机器学习</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="file:///C:\Users\qhy28\.atom\packages\markdown-preview-enhanced\node_modules\@shd101wyy\mume\dependencies\mathjax\MathJax.js"></script>
        
      

      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;padding:2em;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}@media screen and (min-width:914px){html body:not([data-presentation-mode]){width:980px;margin:10px auto}}@media screen and (max-width:400px){html body:not([data-presentation-mode]){font-size:14px;margin:0 auto;padding:15px}}html body .pagebreak,html body .newpage{page-break-before:always}html body pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}html body pre.line-numbers>code{position:relative}html body pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}html body pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}html body pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}html body .mathjax-exps .MathJax_Display{text-align:center !important}html body:not([for="preview"]) .code-chunk .btn-group{display:none}html body:not([for="preview"]) .code-chunk .status{display:none}html body:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px} 
       
      </style>
    </head>
    <body class="mume   ">
    <ul>
<li><a href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5">基本概念</a></li>
<li><a href="#nfl-%E5%AE%9A%E7%90%86">NFL 定理</a></li>
<li><a href="#%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0%E5%92%8C%E9%80%89%E6%8B%A9">模型评估和选择</a>
<ul>
<li><a href="#%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88">过拟合和欠拟合</a></li>
<li><a href="#%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95">评估方法</a>
<ul>
<li><a href="#%E7%95%99%E5%87%BA%E6%B3%95">留出法</a></li>
<li><a href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E6%B3%95">交叉验证法</a></li>
<li><a href="#%E7%95%99%E4%B8%80%E6%B3%95">留一法</a></li>
<li><a href="#%E8%87%AA%E5%8A%A9%E6%B3%95">自助法</a></li>
</ul>
</li>
<li><a href="#%E8%B0%83%E5%8F%82">调参</a>
<ul>
<li><a href="#%E5%8F%82%E6%95%B0%E7%9A%84%E7%B1%BB%E5%9E%8B">参数的类型:</a></li>
<li><a href="#%E6%9C%80%E7%BB%88%E6%A8%A1%E5%9E%8B">最终模型</a></li>
<li><a href="#%E9%AA%8C%E8%AF%81%E9%9B%86-%E5%92%8C-%E6%B5%8B%E8%AF%95%E9%9B%86">验证集 和 测试集</a></li>
</ul>
</li>
<li><a href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F">性能度量</a>
<ul>
<li><a href="#%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE">均方误差</a></li>
<li><a href="#%E9%94%99%E8%AF%AF%E7%8E%87%E4%B8%8E%E7%B2%BE%E5%BA%A6">错误率与精度</a></li>
<li><a href="#%E6%9F%A5%E5%87%86%E7%8E%87%E6%9F%A5%E5%85%A8%E7%8E%87">查准率，查全率</a>
<ul>
<li><a href="#%E6%9F%A5%E5%85%A8%E7%8E%87%E5%92%8C%E6%9F%A5%E5%87%86%E7%8E%87%E7%9A%84%E7%9F%9B%E7%9B%BE">查全率和查准率的矛盾</a></li>
</ul>
</li>
<li><a href="#p-r%E6%9B%B2%E7%BA%BF">P-R曲线</a>
<ul>
<li><a href="#%E6%AF%94%E8%BE%83">比较</a></li>
<li><a href="#macro-prf1-micro-prf1">macro-P/R/F1 micro-P/R/F1</a></li>
</ul>
</li>
<li><a href="#roc%E6%9B%B2%E7%BA%BF%E5%92%8Cauc">ROC曲线和AUC</a>
<ul>
<li><a href="#%E6%AF%94%E8%BE%83-1">比较</a></li>
<li><a href="#auc-%E5%92%8C-ell_rank">AUC 和 <span class="mathjax-exps">$&#x5C;ell_{rank}$</span></a>
<ul>
<li><a href="#%E6%BD%9C%E5%9C%A8%E9%97%AE%E9%A2%98">潜在问题</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E9%9D%9E%E5%9D%87%E7%AD%89%E4%BB%A3%E4%BB%B7">非均等代价</a>
<ul>
<li><a href="#%E4%BB%A3%E4%BB%B7%E7%9F%A9%E9%98%B5">代价矩阵</a></li>
<li><a href="#%E4%BB%A3%E4%BB%B7%E6%95%8F%E6%84%9F%E9%94%99%E8%AF%AF%E7%8E%87">代价敏感错误率</a></li>
<li><a href="#%E4%BB%A3%E4%BB%B7%E6%9B%B2%E7%BA%BF">代价曲线</a></li>
</ul>
</li>
<li><a href="#%E6%AF%94%E8%BE%83%E6%A3%80%E9%AA%8C">比较检验</a>
<ul>
<li><a href="#%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C">假设检验</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95">线性回归算法</a></li>
<li><a href="#%E8%81%9A%E7%B1%BB">聚类</a>
<ul>
<li><a href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-1">性能度量</a>
<ul>
<li><a href="#%E5%A4%96%E9%83%A8%E6%8C%87%E6%A0%87">外部指标</a></li>
<li><a href="#%E5%86%85%E9%83%A8%E6%8C%87%E6%A0%87">内部指标</a></li>
</ul>
</li>
<li><a href="#%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97">距离计算</a>
<ul>
<li><a href="#%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BBminkowski-distance">闵可夫斯基距离(Minkowski distance)</a></li>
<li><a href="#vmd-value-difference-metric">VMD Value Difference Metric</a></li>
<li><a href="#%E9%9D%9E%E5%BA%A6%E9%87%8F%E8%B7%9D%E7%A6%BB">非度量距离</a></li>
</ul>
</li>
<li><a href="#%E5%8E%9F%E5%9E%8B%E8%81%9A%E7%B1%BB">原型聚类</a>
<ul>
<li><a href="#k-means-%E7%AE%97%E6%B3%95">k-means 算法</a></li>
<li><a href="#%E5%AD%A6%E4%B9%A0%E5%90%91%E9%87%8F%E9%87%8F%E5%8C%96-%E7%AE%97%E6%B3%95">学习向量量化 算法</a></li>
<li><a href="#%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB">高斯混合聚类</a></li>
</ul>
</li>
<li><a href="#%E5%AF%86%E5%BA%A6%E8%81%9A%E7%B1%BB">密度聚类</a>
<ul>
<li><a href="#dbscan-%E7%AE%97%E6%B3%95">DBSCAN 算法</a></li>
</ul>
</li>
<li><a href="#%E5%B1%82%E6%AC%A1%E8%81%9A%E7%B1%BB">层次聚类</a>
<ul>
<li><a href="#agnes-%E7%AE%97%E6%B3%95">AGNES 算法</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95">最大似然法</a></li>
<li><a href="#%E6%9C%97%E6%A0%BC%E6%9C%97%E6%97%A5%E4%B9%98%E6%95%B0%E6%B3%95">朗格朗日乘数法</a></li>
<li><a href="#%E7%9F%A9%E9%98%B5%E6%B1%82%E5%AF%BC%E5%BE%AE%E5%88%86">矩阵求导/微分</a></li>
</ul>
<!-- tocstop -->
<h1 class="mume-header" id="&#x57FA;&#x672C;&#x6982;&#x5FF5;">基本概念</h1>

<ol>
<li>
<p>什么是机器学习<br>
机器学习就是一门研究如何通过计算的手段，利用经验来改善系统自身的性能，达到进行预测，分类等目的的学科。<br>
赋予机器学习的能力，通过学习来解决问题，而不是通过特定的编程来解决特定的问题。</p>
<blockquote>
<p>[Mitchell , 1997J 给出了一个更形式化的定义假,设用 P 来评估计算机程序,在某任务类 T 上的性能，若一个程序通过利用经验E 在 T 中任务丰获得了性能改善，则我们就说关于T 和 P ， 该程序对 E 进行了学习</p>
</blockquote>
<blockquote>
<p>机器学习正是这样一门学科，它致力于研究如何通过计算的手段，利用经验来玫善系统自身的性能。</p>
</blockquote>
</li>
<li>
<p>机器学习能干什么</p>
<blockquote>
<p>有了学习算法，我们把经验数据提供给它，它就能基于这些数据产生模型;在面对新的情况时，模型会给我们提供相应的判断</p>
</blockquote>
</li>
<li>
<p>什么是模型/假设？<br>
模型是对某个对象特征的模拟和抽象。<br>
简单的来说，就是学习算法经过对数据的学习之后得到的结果。模型是关于数据的某种潜在规律，因此也称为假设。<br>
这条规律自身，也被称为”真相”或”真实”<br>
所谓的”假设”，我的理解是，某个对象满足了这条规律，我们就有理由相信它就是这一类事物，或者说我们可以假设它就是这一类事物，因此称为”假设”</p>
<blockquote>
<p>本书用&quot;模型&quot;泛指从数据中学得的结果有文献用&quot;模型&quot;指全局性结果(例如一棵决策树)，而用&quot;模式&quot;指局部性结呆(例如 A条规则).</p>
</blockquote>
</li>
<li>
<p>什么是记录/示例/样本/特征向量？<br>
就是关于一个事件或对象的描述，反映事件或对象在某方面的表现或者性质的事项。<br>
一条记录包括若干个属性以及对应的属性值。<br>
从数学表示上更容易理解”特征向量”的含义，我们以一个属性作为一个维度，那么一个样本的属性就可以表示成(x1,x2,..,xn)这样的形式，为n个属性张成的空间中的一个点，或者说一个向量</p>
</li>
<li>
<p>什么是数据集？<br>
就是记录的集合。</p>
</li>
<li>
<p>什么是属性空间/样本空间/输入空间？<br>
属性张成的空间，就是属性空间。<br>
以一个属性作为空间的一维变量，那么数据集中所有的属性组成的空间，就是属性空间。<br>
那么数据集中的一个记录，就可以描述为空间的一个点。因此，一条记录也可以描述为“特征向量”</p>
</li>
<li>
<p>什么是维数？<br>
也就是属性空间的维数，简单的来说，就是数据集中属性种类的数目。一条记录有d个属性，那么d就是这条记录的维数。</p>
</li>
<li>
<p>什么是学习/训练？<br>
从数据中学得模型的过程称为&quot;学习&quot; (learning)或&quot;训练&quot; (training),这个过程通过执行某个学习算法来完成。<br>
学习过程是为了找出或者逼近真相。</p>
</li>
<li>
<p>什么是标记信息，样例，标记空间/输出空间？<br>
样本“结果”信息称为标记信息。<br>
有标记信息的示例成为样例。<br>
标记信息的集合为标记空间。</p>
<pre class="language-text"> &#x6BD4;&#x5982;&#x7ED9;&#x5B9A;&#x4E00;&#x4E2A;&#x74DC;&#x7684;&#x7279;&#x5F81;&#xFF0C;&#x8FD9;&#x4E2A;&#x662F;&#x597D;&#x74DC;&#x3002;
 &#x90A3;&#x4E48;&#x201D;&#x597D;&#x74DC;&#x201D;&#x5C31;&#x662F;&#x6807;&#x8BB0;&#x4FE1;&#x606F;&#x3002;
 &#x201C;&#x4E00;&#x4E2A;&#x542B;&#x6709;&#x67D0;&#x4E2A;&#x7279;&#x5F81;&#x7684;&#x74DC;&#x662F;&#x4E2A;&#x597D;&#x74DC;&#x201D;&#x8FD9;&#x662F;&#x4E00;&#x4E2A;&#x6837;&#x4F8B;&#x3002;
 &#x5982;&#x679C; &#x7ED9;&#x5B9A;&#x67D0;&#x4E2A;&#x74DC;&#x7684;&#x7279;&#x5F81; &#x662F;&#x8F93;&#x5165;&#xFF0C;&#x90A3;&#x4E48;&#x8FD9;&#x4E2A;&#x74DC;&#x662F;&#x4E0D;&#x662F;&#x597D;&#x74DC;&#xFF08;&#x6807;&#x8BB0;&#x4FE1;&#x606F;&#xFF09;&#x5C31;&#x662F;&#x8F93;&#x51FA;&#x3002;
 &#x6240;&#x6709;&#x6807;&#x8BB0;&#x4FE1;&#x606F;&#x7684;&#x96C6;&#x5408;&#x5C31;&#x662F;&#x8F93;&#x51FA;&#x7A7A;&#x95F4;&#x3002;
</pre>
</li>
<li>
<p>什么是分类，回归，聚类？<br>
欲预测的值是离散的学习任务称为“分类”，连续的称为“回归”<br>
通过学习，将训练集中的数据分成若干个组，称为“聚类”</p>
<blockquote>
<p>根据训练数据是否拥有标记信息，学习任务可大致划分为两大类&quot;监督学习&quot; (supervised learning) 和&quot;无监督学习&quot; (unsupervised learning) ，分类和回归是前者的代表，而聚类则是后者的代表.</p>
</blockquote>
<p>分类和回归的区分主要是看目标是离散的还是连续的。<br>
比如，预测明天的温度变化是回归任务，预测明天是晴还是阴，这是分类任务。<br>
分类是根据判断的结果（标记信息）进行区分。<br>
聚类则是根据自身的特性进行区分。<br>
或者说 分类是人为设置标记的结果 ，而聚类则是数据自身特性的结果。</p>
</li>
<li>
<p>什么是 泛化能力？</p>
<blockquote>
<p>学得模型适用于新样本的能力，称为&quot;泛化&quot;(generalization)能力</p>
</blockquote>
</li>
<li>
<p>模型和算法的关系</p>
<blockquote>
<p>从数据中学得模型的过程称为&quot;学习&quot; (learning)或&quot;训练&quot; (training),这个过程通过执行某个学习算法来完成.</p>
</blockquote>
</li>
<li>
<p>假设空间，归纳偏好<br>
能与训练集一致的假设的集合为假设空间。<br>
也就是有多个假设与训练集一致，那么最终采用哪个假设（模型），就是归纳偏好。</p>
<blockquote>
<p>有没有一般性的原则来引导算法确立&quot;正确的&quot;偏好呢? &quot;奥卡姆剃刀&quot; (Occam's razor)是一种常用的、自然科学研究中最基本的原则，即&quot;若有多个假设与观察一致，则选最简单的那个&quot;（但并不绝对）</p>
</blockquote>
</li>
<li>
<p>混淆矩阵<br>
混淆矩阵是数据科学、数据分析和机器学习中总结分类模型预测结果的情形分析表，以矩阵形式将数据集中的记录按照真实的类别与分类模型作出的分类判断两个标准进行汇总。<br>
分类结果的混淆矩阵：<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-f7bc7.png" alt="1"></p>
</li>
<li>
<p>指示函数<br>
<span class="mathjax-exps">$Ⅱ(x)，指示函数,如果x为真，那么值为1，否则值为0$</span><br>
<span class="mathjax-exps">$Ⅱ(x) = &#x5C;begin{cases}1 &amp;,x=true&#x5C;&#x5C;0 &amp;,x=false&#x5C;end{cases}$</span></p>
</li>
</ol>
<h1 class="mume-header" id="nfl-&#x5B9A;&#x7406;">NFL 定理</h1>

<p>&quot;没有免费的午餐&quot;定理 (No Free Lunch Theorem，简称 NFL定理)：在所有问题同等重要的情况下，任意两个机器学习算法期望性能相同。<br>
脱离具体问题来谈哪个机器学习算法更好是毫无意义的。<br>
同时，不同的问题，在不同的情况，评判标准也可能有所不同，因此算法的好坏也不是绝对的。</p>
<p>在没有实际背景下，没有一种算法比随机胡猜的效果好</p>
<h1 class="mume-header" id="&#x6A21;&#x578B;&#x8BC4;&#x4F30;&#x548C;&#x9009;&#x62E9;">模型评估和选择</h1>

<h2 class="mume-header" id="&#x8FC7;&#x62DF;&#x5408;&#x548C;&#x6B20;&#x62DF;&#x5408;">过拟合和欠拟合</h2>

<p>过拟合：把训练样本的自身的特点当作所有潜在样本都会具有的一般特性<br>
欠拟合：指对训练样本的一般特性尚未学习好。<br>
比如：训练样本都是有锯齿的树叶，从而分类没有锯齿的都不是树叶，这是过拟合。欠拟合则是把所有绿色的东西都分类为叶子。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5e779.png" alt="2"></p>
<p>疑问：为什么过拟合无法避免？</p>
<p>疑问：为什么NP难问题是无解的？<br>
有些问题需要考虑全体样本的情况，也就是需要考虑全集，那么这个全集可能是无穷大的，所以这个问题是无解的，我们只能去近似它而不能在多项式时间内解决它。</p>
<h2 class="mume-header" id="&#x8BC4;&#x4F30;&#x65B9;&#x6CD5;">评估方法</h2>

<p>基本思路：我们无法直接获得泛化误差，那么就只能通过测试集上的‘误差测试’来近似泛化误差。</p>
<p>同时，这个测试集是从样本真实分布中独立分布采样而得。</p>
<p>测试集应该尽量不在训练集中出现，未在训练过程中使用过。</p>
<h3 class="mume-header" id="&#x7559;&#x51FA;&#x6CD5;">留出法</h3>

<p>留出法：把数据集划分为两个互斥的集合，一个作为训练集，一个作为测试集。</p>
<p>为保证数据划分分布的一致性，应该采用分层采样。</p>
<p>不足：</p>
<ol>
<li>使用留出法，不同的划分，将导致不同的训练集，所以评估的结果也会有所差别，故应该采取多次划分，取平均值的方法。</li>
<li>我们希望评估的是用D训练出的模型的性能，那么评估准确度<br>
有以下矛盾
<ol>
<li>如果S划分多，那么用于评估的测试集T就会少，那么测试结果就可能不准确</li>
<li>如果S划分少，那么S和D的差别较大，从而降低了结果的保真性。</li>
</ol>
</li>
</ol>
<p>一般是 <span class="mathjax-exps">$&#x5C;frac{2}{3}$</span>~<span class="mathjax-exps">$&#x5C;frac{4}{5}$</span> 的样本用于训练</p>
<h3 class="mume-header" id="&#x4EA4;&#x53C9;&#x9A8C;&#x8BC1;&#x6CD5;">交叉验证法</h3>

<p>交叉验证法：</p>
<ol>
<li>数据集分层采样，划分长k个互斥的子集</li>
<li>每k-1个子集用作训练集，剩下一个用作测试集</li>
<li>多次重复步骤1-2，取平均值</li>
</ol>
<p>注意：如果是k个子集，那么一次交叉验证法就会得到k个模型，当数据集比较大的时候，训练k个模型的开销也会很大。更不用说还要进行多次划分。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-51339.png" alt="3"></p>
<h3 class="mume-header" id="&#x7559;&#x4E00;&#x6CD5;">留一法</h3>

<p>留一法：是交叉验证法的一种特殊情况，m个样本的数据集，划分m个子集，也就是只有一个样本用作测试。</p>
<p>不足：<br>
当数据集比较大的时候，训练m个模型的计算开销可能是难以忍受的，更不用说还要进行多次划分。</p>
<h3 class="mume-header" id="&#x81EA;&#x52A9;&#x6CD5;">自助法</h3>

<p>自助法：从数据集D中，有放回地随机采样m次作为训练集D',以D'作为 训练集，D/D'作为测试集。</p>
<p><span class="mathjax-exps">$lim_{m&#x5C;to &#x5C;infty} (1 - &#x5C;frac{1}{m})^m = &#x5C;frac{1}{e} &#x5C;approx 0.368$</span></p>
<p>也就是初始数据集中，有36.8%的样本从未在训练集中出现过。</p>
<p>自助法在数据集较小，难以有效划分训练/测试集时很有用</p>
<p>不足：自助法产生的数据集改变了初始数据集的分布，会引入误差。</p>
<p>疑问：怎么样才算数据集小?<br>
这个看具体情况。</p>
<h2 class="mume-header" id="&#x8C03;&#x53C2;">调参</h2>

<p>对算法参数进行设定，参数调节。调参需要注意，常用的做法是对每个参数选定一个范围与变化步长，在计算开销与性能估计之间折中。</p>
<p>调参方式：产生多个模型后，基于某种评估方法进行选择。</p>
<h3 class="mume-header" id="&#x53C2;&#x6570;&#x7684;&#x7C7B;&#x578B;">参数的类型:</h3>

<ol>
<li>算法的参数<br>
一般由人工设定多个候选值后产生模型，数目常在10以内</li>
<li>模型的参数<br>
通过学习来产生多个候选模型。大型模型甚至数目可达上百亿。</li>
</ol>
<h3 class="mume-header" id="&#x6700;&#x7EC8;&#x6A21;&#x578B;">最终模型</h3>

<p>对于m个样本的数据集D,训练出模型并选择完成后，学习算法与参数配置都已选定，还需要用数据集D重新训练，这个模型才是我们的最终模型。</p>
<h3 class="mume-header" id="&#x9A8C;&#x8BC1;&#x96C6;-&#x548C;-&#x6D4B;&#x8BD5;&#x96C6;">验证集 和 测试集</h3>

<p>模型评估和选择中用于测试的数据集，为验证集。<br>
验证集用于模型选择与调参。<br>
测试集用于估计模型在使用时的泛化能力</p>
<h2 class="mume-header" id="&#x6027;&#x80FD;&#x5EA6;&#x91CF;">性能度量</h2>

<p>前面提到的都是模型的评估方法，也就是 “怎么做”，而性能度量则是”怎么评”。后面还有怎么比较的问题，并不能直接比较大小。<br>
性能度量：衡量模型泛化能力的评价标准。</p>
<h3 class="mume-header" id="&#x5747;&#x65B9;&#x8BEF;&#x5DEE;">均方误差</h3>

<p>均方差是&quot;误差&quot;的平方的期望值。<br>
误差就是估计值与被估计量的差。<br>
均方误差：回归任务中常用的性能度量。<br>
f：学习器，已经训练好的模型<br>
D：测试集<br>
m：测试集的示例数目<br>
<span class="mathjax-exps">$f(x_i)$</span>:模型对<span class="mathjax-exps">$x_i$</span>预测的结果<br>
<span class="mathjax-exps">$y_i$</span>:示例<span class="mathjax-exps">$x_i$</span>的真实标记<br>
<span class="mathjax-exps">$E(f;D) = &#x5C;frac{1}{m}&#x5C;sum_{i=1}^m (f(x_i) - y_i)^2$</span></p>
<p><span class="mathjax-exps">$&#x5C;mathcal{D}$</span>:数据分布<br>
<span class="mathjax-exps">$p(x)$</span>:概率密度函数<br>
<span class="mathjax-exps">$E(f;&#x5C;mathcal{D}) = &#x5C;int_{x&#x5C;sim &#x5C;mathcal{D}} (f(x) - y)^2p(x)dx$</span></p>
<h3 class="mume-header" id="&#x9519;&#x8BEF;&#x7387;&#x4E0E;&#x7CBE;&#x5EA6;">错误率与精度</h3>

<p>错误率：模型分类错误的样本数占总样本数的比例。<br>
精度：模型分类正确的样本数占总样本数的比例。</p>
<p><span class="mathjax-exps">$Ⅱ(x)，指示函数,如果x为真，那么值为1，否则值为0$</span><br>
<span class="mathjax-exps">$E(f;D) = &#x5C;frac{1}{m}&#x5C;sum_{i = 1}^m Ⅱ(f(x_i)&#x5C;neq y_i)$</span></p>
<p><span class="mathjax-exps">$acc(f;D) = &#x5C;frac{1}{m}&#x5C;sum_{i = 1}^m Ⅱ(f(x_i) =  y_i) = 1 - E(f;D)$</span></p>
<p><span class="mathjax-exps">$E(f;&#x5C;mathcal{D}) = &#x5C;int_{x&#x5C;sim &#x5C;mathcal{D}} Ⅱ(f(x)&#x5C;neq y)p(x)dx$</span></p>
<p><span class="mathjax-exps">$acc(f;&#x5C;mathcal{D}) = &#x5C;int_{x&#x5C;sim &#x5C;mathcal{D}} Ⅱ(f(x) =  y)p(x)dx$</span></p>
<h3 class="mume-header" id="&#x67E5;&#x51C6;&#x7387;&#x67E5;&#x5168;&#x7387;">查准率，查全率</h3>

<p>FP(true positive):真正例<br>
FP(false positive):假正例<br>
TN(true negative):真反例<br>
FN(true negati ve):假反例</p>
<p>查准率R:所有预测为正例的结果中，真正例占的比重。<br>
比如:挑出的西瓜中有多少比例是好瓜<br>
<span class="mathjax-exps">$P = &#x5C;frac{TF}{TP + FP}$</span></p>
<p>查全率R:所有真实情况为正例中，被预测正确（真正例）占的比重。<br>
比如:所有好瓜中有多少比例被挑了出来<br>
<span class="mathjax-exps">$R = &#x5C;frac{TP}{TP + FN}$</span></p>
<p>注意：这里挑出的西瓜的多少，指的应该是预测为正例（好瓜）的数目而不是被预测的瓜的总数</p>
<h4 class="mume-header" id="&#x67E5;&#x5168;&#x7387;&#x548C;&#x67E5;&#x51C6;&#x7387;&#x7684;&#x77DB;&#x76FE;">查全率和查准率的矛盾</h4>

<p>查全率和查准率是一对相互矛盾的度量，查准率高时，查全率往往偏低，查全率高时，查准率往往偏低。</p>
<p>比如:要尽可能多地挑出好瓜（提高查全率），那么尽可能将更多的瓜挑上（比如全都挑走），那么自然好瓜占的比例（查准率）就低了</p>
<p>很多学习器是为样本产生一个实值或概率预测，然后将这个预测值与一个分类阈值进行比较，若大于阈值则为正类，否则为反类。</p>
<p>希望查准率高，则阈值设置大一些，意味着更看重决策的准确性，例如在商品推荐系统，尽量减少错误推荐；</p>
<p>希望查全率高，则阈值设置小一些，意味着“有杀错，冇放过”，例如在罪犯检测过程中。</p>
<p>以上性能度量都是基于某个评判条件（分类阈值）<br>
也就是定义一个边界，<br>
如果预测的值超过这个边界就预测为正例<br>
否则预测为反例<br>
可以理解成 先对结果进行排序，然后在某个地方划分，一边为正例，一边为反例。</p>
<h3 class="mume-header" id="p-r&#x66F2;&#x7EBF;">P-R曲线</h3>

<p>P-R曲线就是以P（查准率）-R（查全率）为坐标得到的曲线。<br>
具体步骤为：</p>
<ol>
<li>根据预测值（可能性）对所有样本进行排序</li>
<li>按顺序逐个把样本作为正例进行预测，得到一个R,P的值</li>
</ol>
<p><s>P-R曲线有什么意义？</s></p>
<h4 class="mume-header" id="&#x6BD4;&#x8F83;">比较</h4>

<ol>
<li>
<p>若一个学习器的P-R曲线被另一个学习器的曲线完全&quot;包住&quot;，则可断言后者的性能优于前者。</p>
</li>
<li>
<p>若两个学习器的P-R曲线发生交叉</p>
<ol>
<li>
<p>P-R曲线包围的面积<br>
它在一定程度上表征了学习器在查准率和查全率上取得双高的比例。<br>
但是这个值不容易估算。</p>
</li>
<li>
<p>平衡点 BEP<br>
BEP是P=R 查全率等于查准率时候的取值。<br>
但是这个还是过于简单。</p>
</li>
<li>
<p>F1<br>
F1是基于查准率和查全率的调和平均。<br>
<span class="mathjax-exps">$F1 = &#x5C;frac{2PR}{P + R} = &#x5C;frac{2 &#x5C;times TP}{2&#x5C;times TP + FN + FP} = &#x5C;frac{2 &#x5C;times TP}{样例总数 + TP - TN}$</span></p>
</li>
<li>
<p><span class="mathjax-exps">$F_&#x5C;beta$</span><br>
更一般的F1，能表达出对查准率和查全率的不同偏好<br>
<span class="mathjax-exps">$F&#x5C;beta = &#x5C;frac{(1 + &#x5C;beta)PR}{&#x5C;beta^2P + R}$</span><br>
<span class="mathjax-exps">$&#x5C;beta &gt; 1$</span>:查全率影响更大<br>
<span class="mathjax-exps">$&#x5C;beta = 1$</span>:退化为F1<br>
<span class="mathjax-exps">$&#x5C;beta &gt; 1$</span>:查准率影响更大</p>
</li>
</ol>
</li>
</ol>
<p>什么是调和平均？<br>
调和平均的定义：<span class="mathjax-exps">$&#x5C;frac{1}{F1} = &#x5C;frac{1}{2}(&#x5C;frac{1}{P} + &#x5C;frac{1}{R})$</span><br>
加权调和平均的定义:<br>
<span class="mathjax-exps">$&#x5C;frac{1}{F_&#x5C;beta} = &#x5C;frac{1}{ 1 + &#x5C;beta^2}(&#x5C;frac{1}{P} + &#x5C;frac{&#x5C;beta^2}{R})$</span></p>
<h4 class="mume-header" id="macro-prf1-micro-prf1">macro-P/R/F1 micro-P/R/F1</h4>

<p>我们希望在n个二分类混淆矩阵上综合考察查全率和查准率</p>
<p>第一种做法是：<br>
分别计算出查全率和查准率，再甲酸平均值,即<br>
<span class="mathjax-exps">$marop&#x5C;mathrm{-}P = &#x5C;frac{1}{n} &#x5C;sum_{i = 1}^n P_i$</span><br>
<span class="mathjax-exps">$macro&#x5C;mathrm{-}R = &#x5C;frac{1}{n} &#x5C;sum_{i = 1}^n R_i$</span><br>
<span class="mathjax-exps">$macro&#x5C;mathrm{-}F1 = &#x5C;frac{2&#x5C;times macro&#x5C;mathrm{-}P &#x5C;times macro&#x5C;mathrm{-}R}{macro&#x5C;mathrm{-}P &#x5C;times macro&#x5C;mathrm{-}R}$</span></p>
<p>第二种做法是：<br>
将各个矩阵对应的元素平均，再基于这些平均值进行查全率/查准率/F1的计算。<br>
<span class="mathjax-exps">$micro&#x5C;mathrm{-}P = &#x5C;frac{&#x5C;overline{TP}}{&#x5C;overline{TP} + &#x5C;overline{FP}}$</span><br>
<span class="mathjax-exps">$micro&#x5C;mathrm{-}R = &#x5C;frac{&#x5C;overline{TP}}{&#x5C;overline{TP} + &#x5C;overline{FN}}$</span><br>
<span class="mathjax-exps">$micro&#x5C;mathrm{-}F1 = &#x5C;frac{2&#x5C;times micro&#x5C;mathrm{-}P &#x5C;times micro&#x5C;mathrm{-}R}{micro&#x5C;mathrm{-}P &#x5C;times micro&#x5C;mathrm{-}R}$</span></p>
<h3 class="mume-header" id="roc&#x66F2;&#x7EBF;&#x548C;auc">ROC曲线和AUC</h3>

<p>疑问：在实际应用中，怎么确定分类阈值到底取多少？</p>
<p>也就是说上面的性能度量都是针对某一种情况而得到的。<br>
而我们更希望评测的是在不同任务下的综合的性能好坏。也就排序本身的质量好坏。</p>
<p>ROC曲线就是考虑从排序本身质量好坏的角度来评估的。(前面的P-R曲线应该也是从这个角度考虑。)</p>
<p>ROC仅适用于二分类问题。</p>
<p>TPR(true positive rate):真正例率<br>
FPR(false positive rate):假正例率<br>
<span class="mathjax-exps">$TPR = &#x5C;frac{TP}{TP + FN}$</span><br>
<span class="mathjax-exps">$FPR = &#x5C;frac{FP}{TN + FP}$</span></p>
<p>具体步骤为：</p>
<ol>
<li>根据学习器的预测结果对样例进行排序</li>
<li>按顺序逐个把样本作为正例进行预测，计算得到TPR和FPR</li>
<li>以FPR为横轴，以TPR为纵轴作图</li>
</ol>
<p><img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7e91d.png" alt="4"></p>
<p>设正例的数目为<span class="mathjax-exps">$m^+$</span>，反例的数目为<span class="mathjax-exps">$m^-$</span>，那么依次将每个样例预测为正例的时候，设前一个标记点的坐标是(x,y),如果是真正例，那么对应的标记点为<span class="mathjax-exps">$(x,y+&#x5C;frac{1}{m^+})$</span>，如果是假正例，那么对应的坐标点就是<span class="mathjax-exps">$(x + &#x5C;frac{1}{m^-},y)$</span>。</p>
<p><span class="mathjax-exps">$TPR = &#x5C;frac{TP}{TP + FN} = &#x5C;frac{TP}{m^+}$</span><br>
<span class="mathjax-exps">$FPR = &#x5C;frac{FP}{TN + FP} = &#x5C;frac{FP}{m^-}$</span><br>
预测为真正例:<span class="mathjax-exps">$TPR&#x27; = &#x5C;frac{TP + 1}{m^+}$</span><br>
预测为假正例:<span class="mathjax-exps">$FPR&#x27; = &#x5C;frac{FP + 1}{m^-}$</span></p>
<ol>
<li>
<p><s>对角线对应为&quot;随机猜想&quot;模型，为什么？<br>
个人觉得不对，如果正例的数目和反例的数目相同的情况下才是对角线吧。</s></p>
</li>
<li>
<p>(0,1)则对应于将所有正例排在所有反例之前的理想模型。<br>
这个应该是指AUC的面积为整个矩形的情况吧。</p>
</li>
<li>
<p>什么是&quot;随机猜想&quot;模型？</p>
</li>
</ol>
<h4 class="mume-header" id="&#x6BD4;&#x8F83;-1">比较</h4>

<ol>
<li>
<p>当一个学习器的ROC曲线被另一个学习器包住时，则可以断言后者的性能优于前者。</p>
</li>
<li>
<p>若两个学习器的ROC曲线发生交叉，较为理想的是比较ROC曲线下的面积 也就是 AUC（Area Under ROC Curve）</p>
</li>
</ol>
<h4 class="mume-header" id="auc-&#x548C;-ell_rank">AUC 和 <span class="mathjax-exps">$&#x5C;ell_{rank}$</span></h4>

<p>设ROC曲线由坐标<span class="mathjax-exps">$&#x5C;{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)&#x5C;}$</span>的点按序链接而形成(<span class="mathjax-exps">$x_1 = 0,x_m = 1$</span>)，则AUC可以估算为：<br>
<span class="mathjax-exps">$AUC = &#x5C;frac{1}{2} &#x5C;sum_{i = 1}^{m - 1}(x_{i+1} - x_i)(y_i + y_{i+1})$</span></p>
<p>给定<span class="mathjax-exps">$m^+$</span>个正例和<span class="mathjax-exps">$m^-$</span>个反例，令<span class="mathjax-exps">$D^+$</span>和<span class="mathjax-exps">$D^-$</span>分别表示正例、反例的集合，则排序“损失”(loss)定义为:<br>
<span class="mathjax-exps">$&#x5C;ell_{rank} = &#x5C;frac{1}{m^+m^-}&#x5C;displaystyle{&#x5C;sum_{x^+ &#x5C;in D^+}}&#x5C;displaystyle{&#x5C;sum_{x^- &#x5C;in D^-}}&#x5C;bigg( I&#x5C;big(f(x^+) &lt; f(x^-)&#x5C;big) + &#x5C;frac{1}{2}I&#x5C;big(f(x^+) = f(x^-)&#x5C;big) &#x5C;bigg)$</span></p>
<p>即 考虑每一对正例、反例，若正例预测值小于反例，则记一个“罚分”，若相等则记0.5个罚分。</p>
<p>疑问：为什么使用ROC和AUC？</p>
<blockquote>
<p>既然已经这么多评价标准，为什么还要使用ROC和AUC呢？因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类不平衡（class imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化。下图是ROC曲线和Precision-Recall曲线5的对比：<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-37682.png" alt="7"><br>
在上图中，(a)和(c)为ROC曲线，(b)和(d)为Precision-Recall曲线。(a)和(b)展示的是分类其在原始测试集（正负样本分布平衡）的结果，(c)和(d)是将测试集中负样本的数量增加到原来的10倍后，分类器的结果。可以明显的看出，ROC曲线基本保持原貌，而Precision-Recall曲线则变化较大。</p>
</blockquote>
<p>疑问：为什么 <span class="mathjax-exps">$AUC = 1 - &#x5C;ell_{rank}$</span>?<br>
若一个正例在ROC曲线上对应的坐标为(x,y)，则x恰是排序在其之前的反例所占的比例，即假正例率，因此有:？？？<br>
<span class="mathjax-exps">$AUC = 1 - &#x5C;ell_{rank}$</span></p>
<p>疑问：同样是点构成的曲线，P-R曲线的面积却说不太容易估算？+</p>
<h5 class="mume-header" id="&#x6F5C;&#x5728;&#x95EE;&#x9898;">潜在问题</h5>

<p>AUC of ROC是机器学习的社群最常使用来比较不同模型优劣的方法<a href="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-5e779.png">2</a> 。然而近来这个做法开始受到质疑，因为有些机器学习的研究指出，AUC的噪声太多，并且很常求不出可信又有效的AUC值（此时便不能保证AUC传达本节开头所述之意义），使得AUC在模型比较时产生的问题比解释的问题更多。</p>
<h2 class="mume-header" id="&#x975E;&#x5747;&#x7B49;&#x4EE3;&#x4EF7;">非均等代价</h2>

<p>前面的性能度量均默认错误的代价是均等的，只考虑错误的次数，但实际情况中，代价往往是不均等的，比如医生看病误诊可能是致命的，但是作业做错的代价却不是致命的。&quot;代价敏感错误率&quot;，&quot;代价曲线&quot;就是&quot;非均等代价&quot;下的性能度量。</p>
<h3 class="mume-header" id="&#x4EE3;&#x4EF7;&#x77E9;&#x9635;">代价矩阵</h3>

<p>描述错误代价的矩阵，<span class="mathjax-exps">$cost_{ij}$</span>表示把第i类样呗预测为第j类样本的代价。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-f1f2d.png" alt="5"><br>
一般情况下，重要的是代价的比值，而不是绝对值（只有有对比才能体现差距不是？）</p>
<h3 class="mume-header" id="&#x4EE3;&#x4EF7;&#x654F;&#x611F;&#x9519;&#x8BEF;&#x7387;">代价敏感错误率</h3>

<p>在非均等代价下，我们所希望的不再是错误的次数最少，而是希望最小化&quot;总体代价&quot;。</p>
<p>令<span class="mathjax-exps">$D^+$</span>和<span class="mathjax-exps">$D^-$</span>分别表示样例集D中的正例集和反例集，则&quot;代价敏感&quot;错误率为:<br>
<span class="mathjax-exps">$E(f;D;cost) = &#x5C;frac{1}{m}&#x5C;bigg(  &#x5C;displaystyle{&#x5C;sum_{x_i &#x5C;in D^+}} I(f(x_i)&#x5C;neq y_i)&#x5C;times cost_{01} + &#x5C;displaystyle{&#x5C;sum_{x_i&#x5C;in D^-}} I(f(x_i)&#x5C;neq y_i)&#x5C;times cost_{10} &#x5C;bigg)$</span></p>
<p>这个东西有什么作用？</p>
<h3 class="mume-header" id="&#x4EE3;&#x4EF7;&#x66F2;&#x7EBF;">代价曲线</h3>

<p>在非均等代价下，ROC曲线不能直接反映出学习器的期望总体代价，而&quot;代价曲线&quot;则可以达到该目的。<br>
代价曲线图的横轴是取值为[0,1]的正例概率代价：p为正例的概率<br>
<span class="mathjax-exps">$P(+)cost = &#x5C;frac{p&#x5C;times cost_{01}}{p&#x5C;times cost_{01} + (1-p)&#x5C;times cost_{10}}$</span></p>
<p>纵轴是取值[0,1]的归一化代价:<br>
<span class="mathjax-exps">$cost_{norm} = &#x5C;frac{FNR&#x5C;times p&#x5C;times cost_{01} + FPR&#x5C;times (1-p) &#x5C;times cost_{10}}{p&#x5C;times cost_{01} + (1-p)&#x5C;times cost_{10}}$</span></p>
<p>上面的FPR为假正例率，FNR为假反例率,FNR = 1 - TPR</p>
<p>代价曲线的绘制：</p>
<ol>
<li>
<p>ROC上的每一个点对应了代价平面的一个线段<br>
设坐标点为(FPR,TPR)则可以计算出FNR</p>
</li>
<li>
<p>在代价平面上绘制 从(0,FPR)到(1,FNR)的线段<br>
则线段下的面积表示了该条件下的期望总体代价。</p>
</li>
<li>
<p>将ROC曲线上的每一个点转换成代价平面上的一条线段<br>
则所有线段的下届围成的面积为在所有条件下学习器的总体期望代价。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-7816a.png" alt="6"></p>
</li>
</ol>
<p>为什么代价曲线能体现非均等代价的期望？</p>
<p>什么是归一化?<br>
规范化是将不同变化范围的值映射到相同的固定的范围中。<br>
归一化是其中的特例，把值映射到[0,1]的范围。</p>
<h2 class="mume-header" id="&#x6BD4;&#x8F83;&#x68C0;&#x9A8C;">比较检验</h2>

<p>为什么不能直接比较性能度量大大小来判断性能的优劣？</p>
<ol>
<li>
<p>我们希望比较的是泛化性能，然而通过实验评估方法，我们获得的是测试集上的性能，两者的对比结果可能未必相同。<br>
怎么理解？不是希望用测试集上的结果来近似总体的泛化性能吗？</p>
</li>
<li>
<p>测试集上的性能与测试集本身的选择有很大的关系，且不论使用大小不同的测试集会得到不同的记过，即便用大小相同的测试集，若包含的测试样例不同，测试的结果也会有不同。</p>
</li>
<li>
<p>很多机器学习算法本身有一定的随机性，即便用享有同的参数设置在同一个测试集上多次运行，其结果也会不同。<br>
结果不同，那么测试皆有有什么意义呢？</p>
</li>
</ol>
<p>反正就是，上面3个条件都理解不能。<br>
意思就是说，性能度量得到的结果也不一定是正确的，还需要进一步检验？</p>
<h3 class="mume-header" id="&#x5047;&#x8BBE;&#x68C0;&#x9A8C;">假设检验</h3>

<p>在现实任务中，我们并不知道学习器的泛化错误率（以错误率为例），只能获知其测试错误率</p>
<h1 class="mume-header" id="&#x7EBF;&#x6027;&#x56DE;&#x5F52;&#x7B97;&#x6CD5;">线性回归算法</h1>

<p>梯度下降算法:<br>
偏导决定了参数变化的方向。<br>
理想的情况，每次更新之后，更新的幅度会越来越小。最后收敛<br>
如果学习速率过大，那么最终可能会导致无法收敛。<br>
大部分情况，cost function 是凸函数，所以没有局部解，只有最优解</p>
<h1 class="mume-header" id="&#x805A;&#x7C7B;">聚类</h1>

<p>聚类：非监督学习<br>
聚类就是依靠数据本身的特性对数据集进行分类。<br>
聚类既能作为一个单独过程，用于找寻数据内在的分布结构，也可作为分类等其他学习任务的前驱过程。</p>
<h2 class="mume-header" id="&#x6027;&#x80FD;&#x5EA6;&#x91CF;-1">性能度量</h2>

<p>对聚类的结果，我们需要某种性能度量来评估其好坏，另一方面，若明确了最终将要使用的性能度量，则可直接将其作为聚类过程的优化目标，从而更好的得到符合要求的聚类结果。</p>
<p>好的聚类结果的 <strong>簇内相似度</strong>(<strong>intra-cluster similarity</strong>)高，且 <strong>簇间相似度</strong>(<strong>inter-cluster similarity</strong>)低。</p>
<p>其性能度量有两种，一种是外部指标，通过和参考模型对比来评估模型。另一种是内部指<br>
标，直接考察聚类的结果而不使用任何模型</p>
<h3 class="mume-header" id="&#x5916;&#x90E8;&#x6307;&#x6807;">外部指标</h3>

<p>将聚类结果和某个 <strong>参考模型</strong> 进行比较</p>
<p>定义：<br>
数据集<span class="mathjax-exps">$D = {x_1,x_2,...,x_m}$</span> ，<br>
通过聚类给出的簇划分，<span class="mathjax-exps">$C = {C_1,C_2,...,C_K}$</span><br>
参考模型给出的簇划分，<span class="mathjax-exps">$C = {C_1^* ,C_2^* ,...,C_K^* }$</span><br>
簇标记：<span class="mathjax-exps">$&#x5C;lambda_j &#x5C;in &#x5C;{1,2,...,k&#x5C;}$</span> ，表示样本<span class="mathjax-exps">$x_j$</span>的 <strong>簇标记</strong>，即 <span class="mathjax-exps">$x_i &#x5C;in C_{&#x5C;lambda_j}$</span></p>
<p>将样本两两配对有：</p>
<p><span class="mathjax-exps">$a = |SS| , SS = &#x5C;{ (x_i , x_j)|&#x5C;lambda_i = &#x5C;lambda_j , &#x5C;lambda_i^* = &#x5C;lambda_j^* , i &lt; j &#x5C;}$</span><br>
a表示 两个模型都将 样本<span class="mathjax-exps">$x_i$</span>和样本<span class="mathjax-exps">$x_j$</span> 预测为同类的 样本对数目</p>
<p><span class="mathjax-exps">$b = |SD| , SD = &#x5C;{ (x_i , x_j)|&#x5C;lambda_i = &#x5C;lambda_j , &#x5C;lambda_i^* &#x5C;neq &#x5C;lambda_j^* , i &lt; j &#x5C;}$</span><br>
b表示，模型将样本<span class="mathjax-exps">$x_i$</span>和样本<span class="mathjax-exps">$x^{(j)}$</span>预测为同一个类，而参考模型预测为不同类的 样本对的数目</p>
<p><span class="mathjax-exps">$c = |DS| , DS = &#x5C;{ (x_i , x_j)|&#x5C;lambda_i &#x5C;neq &#x5C;lambda_j , &#x5C;lambda_i^* = &#x5C;lambda_j^* , i &lt; j &#x5C;}$</span><br>
c表示，模型将样本<span class="mathjax-exps">$x^{(i)}$</span>和样本<span class="mathjax-exps">$x^{(j)}$</span>预测为不同类，而参考模型预测为用一个类的 样本对的数目</p>
<p><span class="mathjax-exps">$d = |DD| , DD = &#x5C;{ (x_i , x_j)|&#x5C;lambda_i &#x5C;neq &#x5C;lambda_j , &#x5C;lambda_i^* &#x5C;neq &#x5C;lambda_j^* , i &lt; j &#x5C;}$</span><br>
d表示 两个模型都将 样本<span class="mathjax-exps">$x_i$</span>和样本<span class="mathjax-exps">$x_j$</span> 预测为不同类的 样本对数目</p>
<p>因为每个样本对仅在上面的集合中出现过一次，并且 i &lt; j，则有<span class="mathjax-exps">$a+b+c+d = &#x5C;frac{m(m-1)}{2}$</span></p>
<p>Jaccard系数(简称 JC)<br>
<span class="mathjax-exps">$JC = &#x5C;frac{a}{a+b+c}$</span></p>
<p>FM指数 （简称 FMI）<br>
<span class="mathjax-exps">$FMI = &#x5C;sqrt{&#x5C;frac{a}{a+b} &#x5C;times &#x5C;frac{a}{a+c}}$</span></p>
<p>Rand指数 （简称 RI）<br>
<span class="mathjax-exps">$RI = &#x5C;frac{2(a+d)}{m(m-1)}$</span></p>
<p>上述性能度量的结果值域为[0,1] ，值越大越好。</p>
<h3 class="mume-header" id="&#x5185;&#x90E8;&#x6307;&#x6807;">内部指标</h3>

<p>直接考察聚类结果而不利用任何 <strong>参考模型</strong><br>
聚类的结果簇划分为：<span class="mathjax-exps">$C = &#x5C;{C_1,C_2,...,C_k&#x5C;}$</span><br>
定义：<br>
<span class="mathjax-exps">$avg(C) = &#x5C;frac{2}{|C|(|C| - 1)}&#x5C;displaystyle{&#x5C;sum_{1 &#x5C;leq i &lt; j &#x5C;leq |C|}} dist(x_i,x_j)$</span><br>
表示 簇内两个样本距离的均值。</p>
<p><span class="mathjax-exps">$diam(C) = &#x5C;displaystyle{max_{1 &#x5C;leq i &lt; j  &#x5C;leq |C|}} dist(x_i,x_j)$</span><br>
表示 簇内两个样本的最远距离</p>
<p><span class="mathjax-exps">$d_{min}(C_i,C_j) = &#x5C;displaystyle{min_{x_i &#x5C;in C_i,x_j&#x5C;in C_j}} dist(x_i,x_j)$</span><br>
表示 两个簇之间样本的最小距离</p>
<p><span class="mathjax-exps">$d_{cen}(C_i,C_j) = dist(&#x5C;mu_i,&#x5C;mu_j)$</span><br>
表示两个簇，中心的距离。</p>
<p>其中<br>
<span class="mathjax-exps">$dist(&#x5C;cdot,&#x5C;cdot)$</span> 表示两个样本之间的距离。<br>
<span class="mathjax-exps">$&#x5C;mu = &#x5C;frac{1}{|C|} &#x5C;displaystyle{&#x5C;sum_{1 &#x5C;leq i &#x5C;leq |C|}} x_i$</span></p>
<p>DB指数（简称 DBI）<br>
<span class="mathjax-exps">$DBI=&#x5C;frac{1}{k} &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;displaystyle{&#x5C;max_{j &#x5C;neq i}} &#x5C;big ( &#x5C;frac{avg(C_i) + avg(C_j)}{d_{cen}(C_i , C_j)} &#x5C;big)$</span><br>
分子：两个簇的样本平均距离之和<br>
分母：两个簇中心的距离<br>
注：分母书本是<span class="mathjax-exps">$d_{cen}(&#x5C;mu_i , &#x5C;mu_j)$</span>应该是<span class="mathjax-exps">$d_{cen}(C_i , C_j)$</span></p>
<p>Dunn指数（简称 DI）<br>
<span class="mathjax-exps">$DI = &#x5C;displaystyle{&#x5C;min_{1 &#x5C;leq i &#x5C;leq k}}&#x5C;big &#x5C;{   &#x5C;displaystyle{&#x5C;min_{j &#x5C;neq i}} &#x5C;big ( &#x5C;frac{d_{min}(C_i,C_j)}{max_{1 &#x5C;leq l &#x5C;leq k} diam(C_l)} &#x5C;big ) &#x5C;big &#x5C;}$</span><br>
分子：两个簇的最短距离<br>
分母：簇内最短距离</p>
<p>DBI越小越好，DI越大越好。</p>
<h2 class="mume-header" id="&#x8DDD;&#x79BB;&#x8BA1;&#x7B97;">距离计算</h2>

<p>聚类的性能度量涉及了 <strong>距离</strong> 的概念。<br>
这个聚类根据属性种类的不同又分为两大种类。<br>
对于属性值有 <strong>序</strong> 的关系，可以使用 <strong>闵可夫斯基距离</strong><br>
对于属性值是 <strong>无序</strong> 的，比如{飞机，火车，轮船}等，直接给每个类定义一个数值，再使<br>
用 <strong>闵可夫斯基距离</strong> 没有很大的意义，对于 这种属性，应该使用 <strong>VDM 距离</strong>。</p>
<p>对函数 <span class="mathjax-exps">$dist(&#x5C;cdot , &#x5C;cdot)$</span>，若它是一个 <strong>距离度量</strong> ，则需满足一些基本性质：</p>
<ol>
<li>非负性：<span class="mathjax-exps">$dist(x^{(i)},x^{(j)}) &#x5C;geq 0$</span>;<br>
两个样本的距离不能为负</li>
<li>同一性：<span class="mathjax-exps">$dist(x^{(i)},x^{(j)}) = 0$</span>当且仅当<span class="mathjax-exps">$x^{(i)}=x^{(j)}$</span><br>
两个样本重叠时距离才为0</li>
<li>对称性：<span class="mathjax-exps">$dist(x^{(i)},x^{(j)}) = dist(x^{(j)},x^{(i)})$</span></li>
<li>直递性：<span class="mathjax-exps">$dist(x^{(i)},x^{(j)}) &#x5C;leq dist(x^{(i)},x^{(k)}) + dist(x^{(k)},x^{(j)})$</span><br>
也称为三角不等式，但是 有一些 <strong>距离度量</strong> 不一定满足这个性质。</li>
</ol>
<h3 class="mume-header" id="&#x95F5;&#x53EF;&#x592B;&#x65AF;&#x57FA;&#x8DDD;&#x79BB;minkowski-distance">闵可夫斯基距离(Minkowski distance)</h3>

<p>给定样本 <span class="mathjax-exps">$x^{(i)} = (x^{(i)}_1;x^{(i)}_2;...;x^{(i)}_n)$</span> 和样本 <span class="mathjax-exps">$x^{(j)} = (x^{(j)}_1;x^{(j)}_2;...;x^{(j)}_n)$</span><br>
<strong>闵可夫斯基距离</strong>(<strong>Minkowski distance</strong>)<br>
<span class="mathjax-exps">$dist_{mk}(x^{(i)},x^{(j)}) = &#x5C;big ( &#x5C;displaystyle{&#x5C;sum_{u = 1}^n} |x^{(i)}_u - x^{(j)}_u|^P  &#x5C;big )^{&#x5C;frac{1}{P}}$</span></p>
<p>也就是 <span class="mathjax-exps">$x^{(i)} - x^{(j)}$</span>的<span class="mathjax-exps">$L_p$</span>范数<span class="mathjax-exps">$||x^{(i)} - x^{(j)}||_p$</span><!--_--></p>
<p>p = 2时，<strong>闵可夫斯基距离</strong> 即 <strong>欧氏距离</strong><br>
<span class="mathjax-exps">$dist_{ed}(x^{(i)},x^{(j)}) = ||x^{(i)} - x^{(j)}||_2 = &#x5C;sqrt{ &#x5C;displaystyle{&#x5C;sum_{u = 1}^n} |x^{(i)}_u - x^{(j)}_u|^2 }$</span></p>
<p>p = 1时，<strong>闵可夫斯基距离</strong> 即 <strong>曼哈顿距离</strong><br>
<span class="mathjax-exps">$dist_{man}(x^{(i)},x^{(j)}) = ||x^{(i)} - x^{(j)}||_1 = &#x5C;displaystyle{&#x5C;sum_{u = 1}^n} |x^{(i)}_u - x^{(j)}_u|$</span></p>
<p><strong>连续属性</strong> ：又叫 <strong>数值属性</strong> ，在定义域上有无穷多个可能的取值。<br>
<strong>离散属性</strong> ：又叫 <strong>列名属性</strong> ，在定义域上是有限个取值。<br>
<strong>有序属性</strong> ：能直接上属性值上计算距离，如：{1,2,3}<br>
<strong>无序属性</strong> ：不能在属性值上计算距离，如：{飞机，火车，轮船}</p>
<p>显然，<strong>闵可夫斯基距离</strong> 是用于有序属性的（无序属性进行这样的运算没有具体意义）。</p>
<h3 class="mume-header" id="vmd-value-difference-metric">VMD Value Difference Metric</h3>

<p>对于 <strong>无序属性</strong> 可以采用 <strong>VMD(Value Difference Metric)</strong></p>
<p>定义：<br>
<span class="mathjax-exps">$m_{u,a}$</span>:表示在属性u上取值为a的样本数<br>
<span class="mathjax-exps">$m_{u,a,i}$</span>:表示在第 <strong>i</strong> 个样本簇中在属性 <strong>u</strong> 上取值为 <strong>a</strong> 的样本数<br>
<span class="mathjax-exps">$k$</span>:表示样本簇数<br>
则属性 <strong>u</strong> 上两个离散值 <strong>a</strong> 与 <strong>b</strong> 之间的 <strong>VMD距离</strong> 为：<br>
<span class="mathjax-exps">$VMD_p(a,b) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;bigg |&#x5C;frac{m_{u,a,i}}{m_{u,a}} - &#x5C;frac{m_{u,b,i}}{m_{u,b}} &#x5C;bigg|$</span></p>
<p>这里实际上就是通过把第i簇的属性u根据其不同取值所占的总比例来 <strong>数值化</strong> 或者说 <strong>有序化</strong> 这个属性。（以属性值的权重来数值化离散的属性值）</p>
<p>于是，将 <strong>闵可夫斯基距离</strong> 和 <strong>VMD距离</strong> 结合即可处理混合属性。<br>
定义：有<span class="mathjax-exps">$n_c$</span>个有序属性，<span class="mathjax-exps">$n-n_c$</span>个无序属性，不失一般性，令有序属性排列在无序属性之前，则<br>
<span class="mathjax-exps">$MinkovDM_p(x^{(i)} , x^{(j)}) = &#x5C;big( &#x5C;displaystyle{&#x5C;sum_{u = 1}^{n_c}} &#x5C;bigg |x^{(i)}_u - x^{(j)}_u &#x5C;bigg|^p + &#x5C;displaystyle{&#x5C;sum_{u = n_c + 1}^{n}} VMD_p(x^{(i)}_u,x^{(j)}_u)   &#x5C;big)^&#x5C;frac{1}{p}$</span></p>
<p>当样本空间的属性的重要性不同时，可使用 <strong>加权距离(weighted distance)</strong>，<br>
以 <strong>闵可夫斯基距离</strong> 为例:<br>
<span class="mathjax-exps">$dist_{wmk}(x^{(i)},x^{(j)}) = &#x5C;big ( w_1&#x5C;cdot|x^{(i)}_1 - x^{(j)}_1|^p + &#x5C;ldots + w_n&#x5C;cdot|x^{(i)}_n - x^{(j)}_n|^p &#x5C;big )^{&#x5C;frac{1}{P}}$</span></p>
<h3 class="mume-header" id="&#x975E;&#x5EA6;&#x91CF;&#x8DDD;&#x79BB;">非度量距离</h3>

<p>不满足 <strong>直递性</strong> 的距离称为非度量距离。</p>
<p>比如 <strong>人马</strong> 分别和 <strong>人</strong> 还有 <strong>马</strong> 相似，<br>
但是 <strong>人</strong> 和 <strong>马</strong> 不相似。</p>
<p><img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-9c582.png" alt="8"></p>
<h2 class="mume-header" id="&#x539F;&#x578B;&#x805A;&#x7C7B;">原型聚类</h2>

<p>原型聚类亦称 <strong>基于原型的聚类 prototype-based clustering</strong>。<br>
此类算法假设类结构能通过一组原型刻画。</p>
<h3 class="mume-header" id="k-means-&#x7B97;&#x6CD5;">k-means 算法</h3>

<p>k-means算法采用一组原型向量来刻画聚类结构。<br>
<strong>k-means算法</strong> 通过设置随机的k个样本作为初始的原型组，每个样本划分为最近那个原型所代表的类<br>
中，而原型更新为其所代表的类中所有样本的中心。</p>
<p>有损压缩的思想是用一个n维的向量来代替其所表示的样本（有损压缩的原理）。<br>
定义：距离样本最近的原型，就表示其所在的类，如果两个样本的距离最近的原型相同，那么这两个样本就是同一个类。</p>
<p>设有k个类，对应的原型(簇的中心)为<span class="mathjax-exps">$&#x5C;mu_1,&#x5C;mu_2,...,&#x5C;mu_k$</span><br>
设第i个样本所在的对应的原型的下标是<span class="mathjax-exps">$c_i$</span>，<br>
也就是说第i个样本对应的原型就是<span class="mathjax-exps">$&#x5C;mu_{c_i}$</span><br>
cost function：<span class="mathjax-exps">$J(c_1,c_2,...,c_n,&#x5C;mu_1,...,&#x5C;mu_k) = &#x5C;frac{1}{m} &#x5C;displaystyle{&#x5C;sum_{i=1}^{n}} ||x^{(i)} - &#x5C;mu_{c_i}||_2^2$</span><!--_--></p>
<p>西瓜书的<span class="mathjax-exps">$C_i$</span>的含义是 <span class="mathjax-exps">$C_i$</span>是原型<span class="mathjax-exps">$&#x5C;mu_i$</span>对应的所有样本的集合。<br>
则代价函数也可以描述为：<span class="mathjax-exps">$E = &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;displaystyle{&#x5C;sum_{&#x5C;boldsymbol x &#x5C;in C_i}} ||&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu_i||^2_2$</span></p>
<p>算法的目的就是最小化上面的代价函数。</p>
<p>算法具体过程为：</p>
<ol>
<li>
<p>首先是初始化问题，可以随机初始化向量<span class="mathjax-exps">$&#x5C;boldsymbol &#x5C;mu$</span>，也可以随机取k个样本作为初始的<span class="mathjax-exps">$&#x5C;boldsymbol &#x5C;mu$</span></p>
</li>
<li>
<p>更新步骤<br>
1. 更新<span class="mathjax-exps">$c_i$</span>，对于每个样本，取最近的<span class="mathjax-exps">$u_j$</span>作为其所在的类，也就是<span class="mathjax-exps">$c_i = j$</span><br>
2. 更新<span class="mathjax-exps">$&#x5C;mu_i$</span>，对于每个类，这个类所有样本的中心作为新的<span class="mathjax-exps">$&#x5C;mu_i$</span></p>
</li>
<li>
<p>为避免运行时间过长，通常设置一个最大运行轮数或最小调整幅度阈值，若达到最大轮数或调整幅度小于阈值，则停止运行。</p>
<blockquote>
<p>input:<span class="mathjax-exps">$D=&#x5C;{x_1,x_2,&#x5C;ldots,x_n&#x5C;}, k$</span><br>
init <span class="mathjax-exps">$&#x5C;mu_1,&#x5C;mu_2,&#x5C;ldots,&#x5C;mu_k$</span><br>
Repeat {<br>
for i = 1 to m<br>
c(i) := index (form 1 to K) of cluster centroid closest to x(i)<br>
for k = 1 to K<br>
μk := average (mean) of points assigned to cluster k<br>
}</p>
</blockquote>
</li>
</ol>
<p>聚类数 k的 值怎么确定？<br>
<strong>肘部法则</strong> 依次取k，若代价函数关于k的图 出现明显的拐点，则k取拐点那个点<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-be200.png" alt="9"></p>
<p><img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-c0cb8.png" alt="10"><br>
k-means算法仅在凸形簇结果上效果好。</p>
<h3 class="mume-header" id="&#x5B66;&#x4E60;&#x5411;&#x91CF;&#x91CF;&#x5316;-&#x7B97;&#x6CD5;">学习向量量化 算法</h3>

<p>学习向量量化 算法 和 k-means算法一样采用一组原型向量来刻画聚类结构。<br>
思想和k-means差不多，只不过，这个算法不是使用样本的中心作为原型的更新，而是通过比较样本的标记信息来决定远离还是靠近样本，同时也没有遍历所有样本。</p>
<p>这个算法要求样本含有标记信息，它的思想就是通过标记信息来指导 <span class="mathjax-exps">$&#x5C;mu$</span>的更新。</p>
<p>算法过程：<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-e56f7.png" alt="12"><br>
结束条件：当达到最大迭代轮数或者原型向量更新很小甚至不更新。<br>
对于任意的样本x，它将被划分到与其距离最近的原型向量所代表的簇中。</p>
<p>为什么这样更新距离是更近？<br>
如第7行所示，<span class="mathjax-exps">$p&#x27; = p_{i*} + &#x5C;eta&#x5C;cdot(x_j - p_{i*})$</span><br>
则<span class="mathjax-exps">$p&#x27;$</span> 和 <span class="mathjax-exps">$x_j$</span> 之间的距离为：<span class="mathjax-exps">$||p&#x27; - x_j||_2 = ||p_{i*} + &#x5C;eta&#x5C;cdot(x_j - p_{i*}) - x_j||_2 = (1 - &#x5C;eta)&#x5C;cdot ||p_{i*} - x_j||_2$</span><!--_--></p>
<p><img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-f6b2d.png" alt="13"></p>
<h3 class="mume-header" id="&#x9AD8;&#x65AF;&#x6DF7;&#x5408;&#x805A;&#x7C7B;">高斯混合聚类</h3>

<p>高斯混合聚类( <strong>Mixture-of-Gaussian</strong> )采用 <strong>概率模型</strong> 来表达聚类原型,簇划分由对应原型对应后验概率确定。</p>
<p>高斯分布：<br>
对n维样本空间X中的随机向量x，若x服从高斯分布，其概率密度函数为：<br>
<span class="mathjax-exps">$p(&#x5C;boldsymbol x) = &#x5C;frac{1}{(2&#x5C;pi)^&#x5C;frac{n}{2}|&#x5C;Sigma|^&#x5C;frac{1}{2}} e^{ -&#x5C;frac{1}{2}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)^T&#x5C;Sigma^{-1}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu) }$</span><br>
其中<span class="mathjax-exps">$&#x5C;boldsymbol &#x5C;mu,&#x5C;boldsymbol x$</span>为n维均值向量，<span class="mathjax-exps">$&#x5C;Sigma$</span>是<span class="mathjax-exps">$n&#x5C;times n$</span>的协方差矩阵,<span class="mathjax-exps">$|&#x5C;Sigma|$</span>为<span class="mathjax-exps">$&#x5C;Sigma$</span>的行列式<br>
为明确显示高斯分布与相应参数的依赖关系，将概率密度函数记为<span class="mathjax-exps">$p(&#x5C;boldsymbol x | &#x5C;boldsymbol &#x5C;mu,&#x5C;Sigma) = p(&#x5C;boldsymbol x)$</span></p>
<p>高斯混合分布：<span class="mathjax-exps">$p_{&#x5C;mathcal{M}}(x) = &#x5C;displaystyle{&#x5C;sum_{i - 1}^k} &#x5C;alpha_i &#x5C;cdot p(&#x5C;boldsymbol x | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i)$</span><br>
该分布由k个混合成分组成，每个混合成分对应一个高斯分布。其中<span class="mathjax-exps">$&#x5C;mu_i$</span>和<span class="mathjax-exps">$&#x5C;Sigma_i$</span>为第i个高斯混合成分的参数，而<span class="mathjax-exps">$&#x5C;alpha_i&gt;0$</span>为相应的 <strong>混合系数(mixture coefficient)</strong> ,且<span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_{i - 1}^k} &#x5C;alpha_i = 1$</span></p>
<p>高斯混合聚类的思想是：<br>
假设样本生成过程由高斯混合分布生成，则一个高斯混合成分就表示一个类，在已知的样本（也就是数据集）的情况下，使用极大似然法来估算这个混合高斯分布的各个参数。</p>
<p>PS:可以通过增大分类的数目k来使得数据的分布更加近似高斯混合分布。</p>
<p>训练集：<span class="mathjax-exps">$D = x_1,x_2,...,x_m$</span><br>
<span class="mathjax-exps">$z_j&#x5C;in &#x5C;{1,2,...,k&#x5C;}$</span>，表示生成样本<span class="mathjax-exps">$x_j$</span>对应的高斯混合成分。</p>
<p>贝叶斯公式：<span class="mathjax-exps">$P(AB) = P(A)P(B|A) = P(B)P(A|B)$</span></p>
<p>那么有：<br>
<span class="mathjax-exps">$P(z_j = j &#x5C;land x_j) = P_{&#x5C;mathcal{M}}(x_j)P(z_j=i|x_j) = P(z_j=i)P(x_j|z_j=i)$</span></p>
<p><span class="mathjax-exps">$P_{&#x5C;mathcal{M}}(x_j)$</span>:高斯混合分布生成样本<span class="mathjax-exps">$x_j$</span>的概率<br>
<span class="mathjax-exps">$P(z_j=i|x_j)$</span>:已知样本<span class="mathjax-exps">$x_j$</span>，样本<span class="mathjax-exps">$x_j$</span>为第i个高斯成分生成的概率<br>
<span class="mathjax-exps">$P(z_j=i)$</span>:样本为第i个高斯分布成分生成的概率，则<span class="mathjax-exps">$P(z_j=i) = &#x5C;alpha_i$</span><br>
<span class="mathjax-exps">$P(x_j|z_j=i)$</span>:第i个高斯成分生成<span class="mathjax-exps">$x_j$</span>的概率，则<span class="mathjax-exps">$P(x_j|z_j=i) = p(x_j|&#x5C;mu_i,&#x5C;Sigma_i)$</span></p>
<p>得：已知样本<span class="mathjax-exps">$x_j$</span>，样本<span class="mathjax-exps">$x_j$</span>为第i个高斯成分生成的概率为：<br>
<span class="mathjax-exps">$P(z_j=i|x_j) = &#x5C;frac{P(z_j=i)P(x_j|z_j=i)}{P_{&#x5C;mathcal{M}}(x_j)} = &#x5C;frac{&#x5C;alpha_i &#x5C;cdot  p(x_j|&#x5C;mu_i,&#x5C;Sigma_i)}{&#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)}$</span></p>
<p>为方便，简记为<span class="mathjax-exps">$P(z_j=i|x_j) = &#x5C;gamma_{ji}$</span><br>
则<span class="mathjax-exps">$x_j$</span>分类的结果为其概率最大的一类中：<span class="mathjax-exps">$&#x5C;lambda_j = &#x5C;begin{matrix}arg max &#x5C;&#x5C;i &#x5C;in &#x5C;{1,2,&#x5C;ldots,k&#x5C;}&#x5C;end{matrix} P(z_j = i|x_j) = &#x5C;begin{matrix} arg max &#x5C;&#x5C; i &#x5C;in &#x5C;{1,2,&#x5C;ldots,k&#x5C;} &#x5C;end{matrix} &#x5C;gamma_{ji}$</span></p>
<p>那么问题来了，模型的参数怎么确定？</p>
<p>已知样本<span class="mathjax-exps">$D = {x_1,x_2,&#x5C;ldots,x_m}$</span>，求 <span class="mathjax-exps">$(&#x5C;boldsymbol &#x5C;mu_i,&#x5C;boldsymbol &#x5C;Sigma_i,&#x5C;alpha_i)$</span><br>
由极大似然法：<br>
m个样本的联合分布为：<br>
<span class="mathjax-exps">$&#x5C;begin{align}L(D) &amp;= &#x5C;displaystyle{	&#x5C;prod_{j = 1}^m} P_{&#x5C;mathcal{M}}(x_j) &#x5C;&#x5C;&amp; = &#x5C;displaystyle{	&#x5C;prod_{j = 1}^m &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)}&#x5C;end{align}$</span><br>
则 求 <span class="mathjax-exps">$&#x5C;boldsymbol &#x5C;mu_i$</span>，由极大似然法：<span class="mathjax-exps">$&#x5C;boldsymbol &#x5C;mu_i = &#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;mu_i&#x5C;end{matrix} = L(d)$</span><br>
也就是要求 <span class="mathjax-exps">$&#x5C;frac{&#x5C;partial L(D)}{&#x5C;boldsymbol &#x5C;mu_i} = 0$</span><br>
为简化运算，取对数<br>
<span class="mathjax-exps">$&#x5C;begin{align}LL(D) &amp; = &#x5C;ln L(D) &#x5C;&#x5C;&amp; = &#x5C;displaystyle{	&#x5C;sum_{j = 1}^m} &#x5C;ln &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)&#x5C;end{align}$</span></p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial LL(D)}{&#x5C;partial &#x5C;boldsymbol &#x5C;mu_i} &amp;= &#x5C;frac{&#x5C;partial &#x5C;displaystyle{	&#x5C;sum_{j = 1}^m} &#x5C;ln &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)}{&#x5C;partial &#x5C;boldsymbol &#x5C;mu_i} &#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{1}{&#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)} &#x5C;cdot &#x5C;frac{ &#x5C;partial &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l) }{ &#x5C;partial &#x5C;mu_i } &amp;,链式法则&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{&#x5C;alpha_i}{&#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)} &#x5C;cdot &#x5C;frac{ &#x5C;partial p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;partial &#x5C;mu_i } &amp;,当l = i的时候，求导的值才不为0&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{&#x5C;alpha_i}{&#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l &#x5C;cdot p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l)} &#x5C;cdot  p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i)&#x5C;cdot &#x5C;Sigma_i^{-1}(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)&amp;,这一步求导见后面&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} p(z_j = i|&#x5C;boldsymbol x_j)&#x5C;cdot &#x5C;Sigma_i^{-1}(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i) &amp;,代入公式&#x5C;&#x5C;&amp;= &#x5C;Sigma_i^{-1} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} p(z_j = i|&#x5C;boldsymbol x_j) &#x5C;cdot (&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i) &amp;,常数提到求和符号前面，&#x5C;Sigma_i的值能由&#x5C;mu_i计算，但不是它的决定式？&#x5C;&#x5C;&amp;= &#x5C;Sigma_i^{-1} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji} &#x5C;cdot (&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i) &amp;,换个符号&#x5C;&#x5C;&amp;=0 &amp;,等号两边左乘一个&#x5C;Sigma_i，消掉&#x5C;Sigma&#x5C;end{align}$</span></p>
<p>解得<span class="mathjax-exps">$&#x5C;mu_i = &#x5C;frac{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji} &#x5C;boldsymbol x_j}{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji}}$</span></p>
<p>求 <span class="mathjax-exps">$&#x5C;frac{ &#x5C;partial p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;partial &#x5C;mu_i }$</span>：<br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{ &#x5C;partial p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;partial &#x5C;mu_i }&amp;= &#x5C;frac{ &#x5C;partial &#x5C;frac{1}{(2&#x5C;pi)^&#x5C;frac{n}{2}|&#x5C;Sigma|^&#x5C;frac{1}{2}} e^{ -&#x5C;frac{1}{2}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)^T&#x5C;Sigma^{-1}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu) } }{ &#x5C;partial &#x5C;mu_i } &#x5C;&#x5C;&amp;= &#x5C;frac{1}{(2&#x5C;pi)^&#x5C;frac{n}{2}|&#x5C;Sigma|^&#x5C;frac{1}{2}} e^{ -&#x5C;frac{1}{2}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)^T&#x5C;Sigma^{-1}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu) } &#x5C;frac{ &#x5C;partial &#x5C;bigg(-&#x5C;frac{1}{2}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)^T&#x5C;Sigma^{-1}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)&#x5C;bigg) }{ &#x5C;partial &#x5C;mu_i } &#x5C;&#x5C;&amp;= p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i)&#x5C;cdot &#x5C;Sigma_i^{-1}(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)&#x5C;end{align}$</span></p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{ &#x5C;partial (&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)^T&#x5C;Sigma^{-1}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu) }{ &#x5C;partial &#x5C;mu_i }&amp;= &#x5C;frac{&#x5C;partial( &#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i )}{&#x5C;partial &#x5C;mu_i}&#x5C;Sigma_i(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i) + &#x5C;frac{&#x5C;partial (&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)}{&#x5C;partial &#x5C;mu_i}(&#x5C;Sigma_i^{-1})^T(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)&#x5C;&#x5C;&amp;= -&#x5C;bigg( &#x5C;Sigma_i^{-1} + (&#x5C;Sigma_i^{-1})^T &#x5C;bigg)(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)&#x5C;&#x5C;&amp;= -2&#x5C;Sigma_i^{-1}(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)&#x5C;end{align}$</span></p>
<blockquote>
<p><span class="mathjax-exps">$&#x5C;begin{cases}&#x5C;frac{&#x5C;partial u^Tv}{&#x5C;partial} = &#x5C;frac{&#x5C;partial u}{&#x5C;partial x}v + &#x5C;frac{&#x5C;partial v}{&#x5C;partial x}u&#x5C;&#x5C;&#x5C;frac{&#x5C;partial a^Tx}{&#x5C;partial x} = &#x5C;frac{&#x5C;partial x^Ta}{&#x5C;partial x} = a&#x5C;&#x5C;(A^T)^{-1} = (A^{-1})^T&#x5C;end{cases}$</span></p>
</blockquote>
<p>同理：<br>
<span class="mathjax-exps">$&#x5C;Sigma_i = &#x5C;frac{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji} &#x5C;boldsymbol  (&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)(&#x5C;boldsymbol x_j - &#x5C;boldsymbol &#x5C;mu_i)^T}{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji}}$</span></p>
<p>对于<span class="mathjax-exps">$&#x5C;alpha_i$</span>，除了最大化<span class="mathjax-exps">$LL(D)$</span>，还有约束条件:<span class="mathjax-exps">$&#x5C;alpha_i &#x5C;geq 0 ,&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} = 1$</span>，因此考虑<span class="mathjax-exps">$LL(D)$</span>的拉格朗日形式：<span class="mathjax-exps">$LL(D) F+ &#x5C;lambda&#x5C;big( &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} -1  &#x5C;big)$</span><br>
对<span class="mathjax-exps">$&#x5C;alpha_i$</span>的导数为0，得到 <span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{ p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l) } + &#x5C;lambda = 0$</span><br>
两边同乘以<span class="mathjax-exps">$&#x5C;alpha_i$</span><br>
对所有的<span class="mathjax-exps">$i$</span>求和，可以得到 <span class="mathjax-exps">$&#x5C;lambda = -m$</span> （见后面）<br>
，得<span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji} + &#x5C;lambda &#x5C;alpha_i = 0$</span></p>
<p>则<span class="mathjax-exps">$&#x5C;alpha_i = &#x5C;frac{1}{m} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;gamma_{ji}$</span></p>
<p>对所有的i求和：<br>
西瓜书上原文是<code>对所有的样本求和</code>，实际上是对所有的 <span class="mathjax-exps">$i$</span> 进行求和：<br>
<span class="mathjax-exps">$&#x5C;begin{matrix} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{&#x5C;alpha_i p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l) } + &#x5C;alpha_i&#x5C;lambda = 0 &amp;,原式&#x5C;&#x5C;&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{&#x5C;alpha_i p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l) } +  &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;alpha_i&#x5C;lambda = 0 &amp;,求和&#x5C;&#x5C; &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;frac{ &#x5C;displaystyle{&#x5C;sum_{i = 1}^k}&#x5C;alpha_i p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i) }{ &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_l p(&#x5C;boldsymbol x_j | &#x5C;boldsymbol &#x5C;mu_l,&#x5C;Sigma_l) } +  &#x5C;lambda = 0 &amp;,求和符号放分子上，且&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;alpha_i = 1 &#x5C;&#x5C;&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} 1 + &#x5C;lambda = 0 &amp;,分子，分母相同约分&#x5C;&#x5C;&#x5C;lambda = -m &#x5C;end{matrix}$</span></p>
<p>由上述的推导即可获得高斯混合模型的EM算法：在每步迭代中，先根据当前参数来计算每一个样本属于每个高斯成分的后验概率<span class="mathjax-exps">$&#x5C;gamma_{ji}$</span>(E步),再根据上面推导的公式来更新模型参数 <span class="mathjax-exps">$&#x5C;{(&#x5C;mu_i,&#x5C;Sigma_i,&#x5C;alpha_i)|1 &#x5C;leq i &#x5C;leq k &#x5C;}$</span>(M步)</p>
<p>具体算法为：<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-c9baa.png" alt="20"></p>
<p>k-means 算法可以看作是高斯聚类在混合成分方差相等，且每个样本仅指派给一个混合成分时的特例。</p>
<p>疑问：协方差矩阵怎么计算？</p>
<h2 class="mume-header" id="&#x5BC6;&#x5EA6;&#x805A;&#x7C7B;">密度聚类</h2>

<p>密度聚类：基于密度的聚类，此类算法假设聚类结构能通过样本的分布的紧密成都确定。<br>
通常情形下，密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于可连接样本不断扩展聚类簇以获得最终的聚类结果，</p>
<h3 class="mume-header" id="dbscan-&#x7B97;&#x6CD5;">DBSCAN 算法</h3>

<p>把每个样本看作一个点，如果两个点可达（距离在一定范围内），则这两个点可以看作属于同一个类。<br>
DBSCAN算法的过程 简单的来说就是 建图和寻找&quot;联通分支&quot;的过程。</p>
<p>DBSCAN算法有两个参数<span class="mathjax-exps">$(&#x5C;epsilon,MinPts)$</span><br>
<span class="mathjax-exps">$&#x5C;epsilon$</span>:描述类内点的紧密程度。<br>
<span class="mathjax-exps">$MinPts$</span>:样本称为核心对象的条件。</p>
<p>一堆堆定义：<br>
<span class="mathjax-exps">$x_i的&#x5C;epsilon 邻域:表示距离这个样本&#x5C;epsilon 的范围内的样本的集合$</span></p>
<p>核心对象:如果一个样本的<span class="mathjax-exps">$&#x5C;epsilon$</span>邻域内样本的数目大于<span class="mathjax-exps">$MinPts$</span>，那么就说这个样本为核心对象。</p>
<p>密度直达:样本和其邻域内的样本相互直达</p>
<p><img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-494ed.png" alt="14"></p>
<p>算法的思想：把相互可达的核心对象及其可达的非核心对象划分为一个类。<br>
对于非核心对象，则根据先来后到的原则来进行瓜分。</p>
<p>感觉就像在求联通分支：每个样本表示一个节点，距离小于<span class="mathjax-exps">$&#x5C;epsilon$</span>则连边，然后求各个连通分支。<br>
（注：核心对象之间是双向边，核心对象和非核心对象为单向边）</p>
<p>优化建议：上面的算法是根据先来后到的原则来瓜分非核心对象，这些非核心对象分配给其最近的类应该更为合适。</p>
<p>算法过程：</p>
<ol>
<li>随机取一个核心对象，加入队列q中</li>
<li>循环处理q，直到q为空<br>
1. 从q取出一个对象（这个对象是非核心对象，因为下面）<br>
2. 如果这个核心对象，就把它邻域内的所有样本都加入到队列中（其中就包括了非核心对象）</li>
<li>上述过程所有取出的样本作为一类，</li>
<li>剩下的样本重复上面的过程，知道划分完毕</li>
</ol>
<p><img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-46eac.png" alt="15"><br>
2-7行：找出所有的核心对象<br>
14行：while循环内只处理核心对象<br>
18行：非核心对象也加入到队列中</p>
<p>简单从图的角度看，<br>
建图：核心对象之间建立双向边，核心对象和非核心对象建立单项边。<br>
找强联通分支：直接根据上述建立的图搜索即可。</p>
<p>输出结果：密度可达的核心对象及其邻域划分为一个簇。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-19638.png" alt="16"></p>
<h2 class="mume-header" id="&#x5C42;&#x6B21;&#x805A;&#x7C7B;">层次聚类</h2>

<p>层次聚类试图在不同层次对数据集进行划分，从而形成树形的聚类结构。</p>
<h3 class="mume-header" id="agnes-&#x7B97;&#x6CD5;">AGNES 算法</h3>

<p>AGNES 算法的过程类似 kruskal算法<br>
算法过程：</p>
<ol>
<li>初始化每个样本作为一个类</li>
<li>取最近的两个类合并成一个类</li>
<li>不断重复这个步骤，直到达到预设的聚类簇个数，或者类间距离大于某个阈值<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-b74b6.png" alt="17"><br>
1-3行：初始化每一个类为一个簇<br>
4-9行：计算类间距离<br>
10-23：两辆合并最近的类</li>
</ol>
<p>关于层次的直观理解：从某一层划分（虚线），则划分出来的每一棵子树代表一个簇。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-e8431.png" alt="18"><br>
每一个中间节点，对应的纵轴表示，它子树所代表的两个簇类的距离。<br>
注：上面那个图之所以那么好看，是因为样本的编号不是顺序排列的。<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-1572f.png" alt="19"></p>
<h1 class="mume-header" id="&#x6700;&#x5927;&#x4F3C;&#x7136;&#x6CD5;">最大似然法</h1>

<p>极大似然法是点估计方法的一种。<br>
所谓参数的点估计，就是找一个合适的统计量，将样本观测值代入到统计量得到的值就作为该参数的估计。<br>
具体来说，就是已知每个样本的分布，根据样本估计这个分布的参数。<br>
极大似然法的思想主要基于&quot;小概率事件在一次实验中几乎不可能发生&quot;。<br>
也就是说，一次实验发生这个已经发生的概率（样本）的概率是最大的。</p>
<p>具体到计算的话，就是</p>
<ol>
<li>假设概率分布函数</li>
<li>求样本的联合分布函数（似然函数）</li>
<li>令似然函数的求导结果为0，得到若干个方程<br>
导数为0的地方，往往是极值点</li>
<li>根据上面的方程求解参数</li>
</ol>
<p>已知样本X1，X2,X3,...,XN,其混合分布为L(X1,X2,X3,...,XN)，则L的参数 应该满足在这个参数下，取得上面的样本的概率为最大。则可以使用<span class="mathjax-exps">$偏导 = 0$</span> 来解得参数</p>
<h1 class="mume-header" id="&#x6717;&#x683C;&#x6717;&#x65E5;&#x4E58;&#x6570;&#x6CD5;">朗格朗日乘数法</h1>

<p>当在求某个函数的极值的时候，题目往往还带有约束条件，比如上面的极大似然法的参数可能还有其他的约束条件。<br>
那么可以考虑使用拉格朗日乘数法。<br>
具体的做法是：把约束条件也加入到目标函数中去，使得约束条件成为目标函数的一部分，进而把问题转换成了无约束条件的最优化问题</p>
<h1 class="mume-header" id="&#x77E9;&#x9635;&#x6C42;&#x5BFC;&#x5FAE;&#x5206;">矩阵求导/微分</h1>

<p>根据wiki上的说法，矩阵求导至今为止还没有公认的一种定义（约定），甚至有时候一篇文章也可能同时使用了不同的矩阵求导的定义（约定）。<br>
PS:这里说的矩阵求导，指的是矩阵对矩阵求导，而标量对标量的求导可以看成是1×1矩阵的求导。</p>
<p>矩阵求导的有两种比较普遍的约定：<br>
对于 <span class="mathjax-exps">$&#x5C;frac{&#x5C;partial &#x5C;boldsymbol Y}{&#x5C;partial &#x5C;boldsymbol X}$</span>，主要有两种处理方法：</p>
<ol>
<li>先展开X，转置，再展开Y（展开的意思见下面）</li>
<li>先展开Y，转置，再展开X<br>
这里所谓的展开，指的是把求导运算代入到被展开矩阵中的每一个元素上。</li>
</ol>
<p>上面的两种约定得到的结果，刚好互为转置。<br>
以两个列向量<span class="mathjax-exps">$&#x5C;boldsymbol y &#x5C;in &#x5C;mathbb{R}^m$</span> 和 <span class="mathjax-exps">$&#x5C;boldsymbol x &#x5C;in &#x5C;mathbb{R}^n$</span>为例,采用第二种约定:<br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial &#x5C;boldsymbol y}{&#x5C;partial &#x5C;boldsymbol x} &amp;= &#x5C;begin{bmatrix}&#x5C;frac{&#x5C;partial y_1}{&#x5C;partial &#x5C;boldsymbol x}&#x5C;&#x5C;&#x5C;frac{&#x5C;partial y_2}{&#x5C;partial &#x5C;boldsymbol x}&#x5C;&#x5C;&#x5C;vdots &#x5C;&#x5C;&#x5C;frac{&#x5C;partial y_m}{&#x5C;partial &#x5C;boldsymbol x}&#x5C;end{bmatrix}^T &amp;,展开&#x5C;boldsymbol y，并转置&#x5C;&#x5C;&amp;= &#x5C;begin{bmatrix}&#x5C;frac{&#x5C;partial y_1}{&#x5C;partial &#x5C;boldsymbol x}&#x5C;frac{&#x5C;partial y_2}{&#x5C;partial &#x5C;boldsymbol x}&#x5C;cdots&#x5C;frac{&#x5C;partial y_m}{&#x5C;partial &#x5C;boldsymbol x}&#x5C;end{bmatrix}&#x5C;&#x5C;&amp;= &#x5C;begin{bmatrix}&#x5C;frac{&#x5C;partial y_1}{&#x5C;partial  x_1} &amp;&#x5C;frac{&#x5C;partial y_2}{&#x5C;partial  x_1} &amp;&#x5C;cdots &amp;&#x5C;frac{&#x5C;partial y_m}{&#x5C;partial  x_1}&#x5C;&#x5C;&#x5C;frac{&#x5C;partial y_1}{&#x5C;partial  x_2} &amp;&#x5C;frac{&#x5C;partial y_2}{&#x5C;partial  x_2} &amp;&#x5C;cdots  &amp;&#x5C;frac{&#x5C;partial y_m}{&#x5C;partial  x_2}&#x5C;&#x5C;&#x5C;vdots &amp;&#x5C;vdots &amp;&#x5C;ddots &amp;&#x5C;vdots&#x5C;&#x5C;&#x5C;frac{&#x5C;partial y_1}{&#x5C;partial  x_n} &amp;&#x5C;frac{&#x5C;partial y_2}{&#x5C;partial  x_n} &amp;&#x5C;cdots &amp;&#x5C;frac{&#x5C;partial y_m}{&#x5C;partial  x_n}&#x5C;end{bmatrix} &amp;,展开&#x5C;boldsymbol x&#x5C;end{align}$</span></p>
<p>以上是矩阵求导的基本定义，然后还有一些定理和公式，比如链式法则，两个矩阵相乘再求导等，目前还没有进行证明。</p>

    </body>
    
    
    <script>
(function bindTaskListEvent() {
  var taskListItemCheckboxes = document.body.getElementsByClassName('task-list-item-checkbox')
  for (var i = 0; i < taskListItemCheckboxes.length; i++) {
    var checkbox = taskListItemCheckboxes[i]
    var li = checkbox.parentElement
    if (li.tagName !== 'LI') li = li.parentElement
    if (li.tagName === 'LI') {
      li.classList.add('task-list-item')
    }
  }
}())    
</script>
  </html>