<!DOCTYPE html>
  <html>
    <head>
      <title>机器学习-EM算法</title>
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width, initial-scale=1.0">
      
      
        <script type="text/x-mathjax-config">
          MathJax.Hub.Config({"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"],"messageStyle":"none","tex2jax":{"processEnvironments":false,"processEscapes":true,"inlineMath":[["$","$"],["\\(","\\)"]],"displayMath":[["$$","$$"],["\\[","\\]"]]},"TeX":{"extensions":["AMSmath.js","AMSsymbols.js","noErrors.js","noUndefined.js"]},"HTML-CSS":{"availableFonts":["TeX"]}});
        </script>
        <script type="text/javascript" async src="file:///C:\Users\qhy28\.atom\packages\markdown-preview-enhanced\node_modules\@shd101wyy\mume\dependencies\mathjax\MathJax.js"></script>
        
      

      
      

      <style> 
      /**
 * prism.js Github theme based on GitHub's theme.
 * @author Sam Clarke
 */
code[class*="language-"],
pre[class*="language-"] {
  color: #333;
  background: none;
  font-family: Consolas, "Liberation Mono", Menlo, Courier, monospace;
  text-align: left;
  white-space: pre;
  word-spacing: normal;
  word-break: normal;
  word-wrap: normal;
  line-height: 1.4;

  -moz-tab-size: 8;
  -o-tab-size: 8;
  tab-size: 8;

  -webkit-hyphens: none;
  -moz-hyphens: none;
  -ms-hyphens: none;
  hyphens: none;
}

/* Code blocks */
pre[class*="language-"] {
  padding: .8em;
  overflow: auto;
  /* border: 1px solid #ddd; */
  border-radius: 3px;
  /* background: #fff; */
  background: #f5f5f5;
}

/* Inline code */
:not(pre) > code[class*="language-"] {
  padding: .1em;
  border-radius: .3em;
  white-space: normal;
  background: #f5f5f5;
}

.token.comment,
.token.blockquote {
  color: #969896;
}

.token.cdata {
  color: #183691;
}

.token.doctype,
.token.punctuation,
.token.variable,
.token.macro.property {
  color: #333;
}

.token.operator,
.token.important,
.token.keyword,
.token.rule,
.token.builtin {
  color: #a71d5d;
}

.token.string,
.token.url,
.token.regex,
.token.attr-value {
  color: #183691;
}

.token.property,
.token.number,
.token.boolean,
.token.entity,
.token.atrule,
.token.constant,
.token.symbol,
.token.command,
.token.code {
  color: #0086b3;
}

.token.tag,
.token.selector,
.token.prolog {
  color: #63a35c;
}

.token.function,
.token.namespace,
.token.pseudo-element,
.token.class,
.token.class-name,
.token.pseudo-class,
.token.id,
.token.url-reference .token.variable,
.token.attr-name {
  color: #795da3;
}

.token.entity {
  cursor: help;
}

.token.title,
.token.title .token.punctuation {
  font-weight: bold;
  color: #1d3e81;
}

.token.list {
  color: #ed6a43;
}

.token.inserted {
  background-color: #eaffea;
  color: #55a532;
}

.token.deleted {
  background-color: #ffecec;
  color: #bd2c00;
}

.token.bold {
  font-weight: bold;
}

.token.italic {
  font-style: italic;
}


/* JSON */
.language-json .token.property {
  color: #183691;
}

.language-markup .token.tag .token.punctuation {
  color: #333;
}

/* CSS */
code.language-css,
.language-css .token.function {
  color: #0086b3;
}

/* YAML */
.language-yaml .token.atrule {
  color: #63a35c;
}

code.language-yaml {
  color: #183691;
}

/* Ruby */
.language-ruby .token.function {
  color: #333;
}

/* Markdown */
.language-markdown .token.url {
  color: #795da3;
}

/* Makefile */
.language-makefile .token.symbol {
  color: #795da3;
}

.language-makefile .token.variable {
  color: #183691;
}

.language-makefile .token.builtin {
  color: #0086b3;
}

/* Bash */
.language-bash .token.keyword {
  color: #0086b3;
}html body{font-family:"Helvetica Neue",Helvetica,"Segoe UI",Arial,freesans,sans-serif;font-size:16px;line-height:1.6;color:#333;background-color:#fff;overflow:initial;padding:2em;box-sizing:border-box;word-wrap:break-word}html body>:first-child{margin-top:0}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{line-height:1.2;margin-top:1em;margin-bottom:16px;color:#000}html body h1{font-size:2.25em;font-weight:300;padding-bottom:.3em}html body h2{font-size:1.75em;font-weight:400;padding-bottom:.3em}html body h3{font-size:1.5em;font-weight:500}html body h4{font-size:1.25em;font-weight:600}html body h5{font-size:1.1em;font-weight:600}html body h6{font-size:1em;font-weight:600}html body h1,html body h2,html body h3,html body h4,html body h5{font-weight:600}html body h5{font-size:1em}html body h6{color:#5c5c5c}html body strong{color:#000}html body del{color:#5c5c5c}html body a:not([href]){color:inherit;text-decoration:none}html body a{color:#08c;text-decoration:none}html body a:hover{color:#00a3f5;text-decoration:none}html body img{max-width:100%}html body>p{margin-top:0;margin-bottom:16px;word-wrap:break-word}html body>ul,html body>ol{margin-bottom:16px}html body ul,html body ol{padding-left:2em}html body ul.no-list,html body ol.no-list{padding:0;list-style-type:none}html body ul ul,html body ul ol,html body ol ol,html body ol ul{margin-top:0;margin-bottom:0}html body li{margin-bottom:0}html body li.task-list-item{list-style:none}html body li>p{margin-top:0;margin-bottom:0}html body .task-list-item-checkbox{margin:0 .2em .25em -1.8em;vertical-align:middle}html body .task-list-item-checkbox:hover{cursor:pointer}html body blockquote{margin:16px 0;font-size:inherit;padding:0 15px;color:#5c5c5c;border-left:4px solid #d6d6d6}html body blockquote>:first-child{margin-top:0}html body blockquote>:last-child{margin-bottom:0}html body hr{height:4px;margin:32px 0;background-color:#d6d6d6;border:0 none}html body table{margin:10px 0 15px 0;border-collapse:collapse;border-spacing:0;display:block;width:100%;overflow:auto;word-break:normal;word-break:keep-all}html body table th{font-weight:bold;color:#000}html body table td,html body table th{border:1px solid #d6d6d6;padding:6px 13px}html body dl{padding:0}html body dl dt{padding:0;margin-top:16px;font-size:1em;font-style:italic;font-weight:bold}html body dl dd{padding:0 16px;margin-bottom:16px}html body code{font-family:Menlo,Monaco,Consolas,'Courier New',monospace;font-size:.85em !important;color:#000;background-color:#f0f0f0;border-radius:3px;padding:.2em 0}html body code::before,html body code::after{letter-spacing:-0.2em;content:"\00a0"}html body pre>code{padding:0;margin:0;font-size:.85em !important;word-break:normal;white-space:pre;background:transparent;border:0}html body .highlight{margin-bottom:16px}html body .highlight pre,html body pre{padding:1em;overflow:auto;font-size:.85em !important;line-height:1.45;border:#d6d6d6;border-radius:3px}html body .highlight pre{margin-bottom:0;word-break:normal}html body pre code,html body pre tt{display:inline;max-width:initial;padding:0;margin:0;overflow:initial;line-height:inherit;word-wrap:normal;background-color:transparent;border:0}html body pre code:before,html body pre tt:before,html body pre code:after,html body pre tt:after{content:normal}html body p,html body blockquote,html body ul,html body ol,html body dl,html body pre{margin-top:0;margin-bottom:16px}html body kbd{color:#000;border:1px solid #d6d6d6;border-bottom:2px solid #c7c7c7;padding:2px 4px;background-color:#f0f0f0;border-radius:3px}@media print{html body{background-color:#fff}html body h1,html body h2,html body h3,html body h4,html body h5,html body h6{color:#000;page-break-after:avoid}html body blockquote{color:#5c5c5c}html body pre{page-break-inside:avoid}html body table{display:table}html body img{display:block;max-width:100%;max-height:100%}html body pre,html body code{word-wrap:break-word;white-space:pre}}@media screen and (min-width:914px){html body:not([data-presentation-mode]){width:980px;margin:10px auto}}@media screen and (max-width:400px){html body:not([data-presentation-mode]){font-size:14px;margin:0 auto;padding:15px}}html body .pagebreak,html body .newpage{page-break-before:always}html body pre.line-numbers{position:relative;padding-left:3.8em;counter-reset:linenumber}html body pre.line-numbers>code{position:relative}html body pre.line-numbers .line-numbers-rows{position:absolute;pointer-events:none;top:1em;font-size:100%;left:0;width:3em;letter-spacing:-1px;border-right:1px solid #999;-webkit-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none}html body pre.line-numbers .line-numbers-rows>span{pointer-events:none;display:block;counter-increment:linenumber}html body pre.line-numbers .line-numbers-rows>span:before{content:counter(linenumber);color:#999;display:block;padding-right:.8em;text-align:right}html body .mathjax-exps .MathJax_Display{text-align:center !important}html body:not([for="preview"]) .code-chunk .btn-group{display:none}html body:not([for="preview"]) .code-chunk .status{display:none}html body:not([for="preview"]) .code-chunk .output-div{margin-bottom:16px} 
       
      </style>
    </head>
    <body class="mume   ">
    <ul>
<li><a href="#em%E7%AE%97%E6%B3%95">EM算法</a>
<ul>
<li><a href="#%E4%BB%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95%E8%AF%B4%E8%B5%B7-%E4%B8%80%E4%B8%AA%E7%AE%80%E5%8D%95%E7%9A%84%E4%BE%8B%E5%AD%90">从极大似然法说起-一个简单的例子</a></li>
<li><a href="#%E4%BB%8E%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E6%B3%95%E8%AF%B4%E8%B5%B7-%E4%B8%80%E4%B8%AA%E7%A8%8D%E5%BE%AE%E5%A4%8D%E6%9D%82%E7%82%B9%E7%9A%84%E4%BE%8B%E5%AD%90-%E4%B8%89%E7%A1%AC%E5%B8%81%E6%A8%A1%E5%9E%8B">从极大似然法说起-一个稍微复杂点的例子-三硬币模型</a></li>
<li><a href="#em%E7%AE%97%E6%B3%95%E7%9A%84%E5%85%B7%E4%BD%93%E6%AD%A5%E9%AA%A4-%E5%92%8C-%E9%9A%90%E5%8F%98%E9%87%8F">EM算法的具体步骤 和 隐变量</a></li>
<li><a href="#em%E7%AE%97%E6%B3%95%E7%9A%84%E8%AF%81%E6%98%8E">EM算法的证明</a></li>
<li><a href="#%E5%85%B6%E4%BB%96">其他</a></li>
<li><a href="#em%E7%AE%97%E6%B3%95%E7%9A%84%E5%BA%94%E7%94%A8-%E9%AB%98%E6%96%AF%E6%B7%B7%E5%90%88%E8%81%9A%E7%B1%BB">EM算法的应用-高斯混合聚类</a></li>
</ul>
</li>
</ul>
<h1 class="mume-header" id="em&#x7B97;&#x6CD5;">EM算法</h1>

<p>EM算法(期望最大化算法,Expectation Maximization Algorithm)是一种迭代算法,用于隐变量(hidden variable)的概率模型参数的极大似然估计,或极大后延概率估计。</p>
<p>在存在隐变量的情况,由于隐变量未知,以下为似然函数<br>
<span class="mathjax-exps">$LL(X|&#x5C;theta) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln &#x5C;displaystyle{&#x5C;sum_z} P(X,z|&#x5C;theta)$</span><br>
令其导数为0,由于<span class="mathjax-exps">$&#x5C;ln$</span> 后面的 <span class="mathjax-exps">$&#x5C;Sigma$</span> 和 <span class="mathjax-exps">$P$</span> 本身的复杂性,导致解析解难以求出。</p>
<p>而在完全数据的情况下,我们能很轻易地计算出解析解。<br>
<span class="mathjax-exps">$LL(X,Z|&#x5C;theta) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(X,Z|&#x5C;theta) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(Z)P(X|&#x5C;theta,Z)$</span></p>
<p>EM算法的核心思想是:使用<span class="mathjax-exps">$z_i$</span>的期望(或者使用所用的<span class="mathjax-exps">$z_i$</span>,不同的<span class="mathjax-exps">$z_i$</span>赋予不同的权重)来代替缺失的<span class="mathjax-exps">$Z$</span>隐变量的数据,这样就能使用完全数据的计算方法来计算。<br>
(这么近似处理之后,表达式就变成了似然函数关于<span class="mathjax-exps">$Z$</span>的后验概率的期望)</p>
<p>EM算法有几个要点:</p>
<ol>
<li>是上面提到的,通过构造<span class="mathjax-exps">$z_i$</span>,把计算方法转换到完全数据上来</li>
<li>使用<span class="mathjax-exps">$Z$</span>的后验概率,而不是其他</li>
<li>迭代求解</li>
</ol>
<p>EM算法主要分两个步骤:</p>
<ol>
<li>
<p>E步<br>
这一步实际上就是求似然函数在新的参数下的表达式,具体就是求隐变量<span class="mathjax-exps">$Z$</span>的后验概率</p>
</li>
<li>
<p>M步<br>
最大化似然函数的期望,具体方法就是令似然函数(或其拉格朗日形式)的导数为0</p>
</li>
</ol>
<p>EM算法对初始值敏感,不同的初始值可能会得到不同的结果。<br>
常用的做法是,使用多几组初始值进行迭代<br>
(对于高斯混合模型)使用<span class="mathjax-exps">$k-means$</span>算法进行预处理,其结果作为EM算法的初始值</p>
<p>k-means算法可以看成是高斯混合聚类在混合成分 方差均等、且每个样本仅指派给一个混合成分的特例加。</p>
<p>EM算法证明示意图:<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-EM%E7%AE%97%E6%B3%95-96f58.png" alt="2"></p>
<p>疑问:关于PRML上的关于奇异点的描述没看明白。</p>
<h2 class="mume-header" id="&#x4ECE;&#x6781;&#x5927;&#x4F3C;&#x7136;&#x6CD5;&#x8BF4;&#x8D77;-&#x4E00;&#x4E2A;&#x7B80;&#x5355;&#x7684;&#x4F8B;&#x5B50;">从极大似然法说起-一个简单的例子</h2>

<p>假设有一枚硬币,抛出硬币,其为正面的概率为p,已知m次试验的结果<span class="mathjax-exps">$&#x5C;{x_1,x_2,...,x_m&#x5C;}$</span>,使用极大似然法估计参数p</p>
<p>一次试验的概率为:<span class="mathjax-exps">$P(x) = p^x&#x5C;times(1-p)^{1-x}$</span></p>
<p>似然函数为:<br>
<span class="mathjax-exps">$L(x_1,x_2,&#x5C;ldots,x_m;p) = &#x5C;displaystyle{&#x5C;prod_{i = 1}^m} P(x_i) = p^{&#x5C;displaystyle{&#x5C;sum_{i = 1}^m} x_i}&#x5C;cdot(1-p)^{m - &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} x_i}$</span></p>
<p>则令<span class="mathjax-exps">$&#x5C;frac{&#x5C;partial L(x_1,x_2,&#x5C;ldots,x_m;p)}{&#x5C;partial p} = 0$</span><br>
得:<span class="mathjax-exps">$p = &#x5C;frac{ &#x5C;displaystyle {&#x5C;sum_{i = 1}^m} x_i}{m}$</span></p>
<h2 class="mume-header" id="&#x4ECE;&#x6781;&#x5927;&#x4F3C;&#x7136;&#x6CD5;&#x8BF4;&#x8D77;-&#x4E00;&#x4E2A;&#x7A0D;&#x5FAE;&#x590D;&#x6742;&#x70B9;&#x7684;&#x4F8B;&#x5B50;-&#x4E09;&#x786C;&#x5E01;&#x6A21;&#x578B;">从极大似然法说起-一个稍微复杂点的例子-三硬币模型</h2>

<p>假设要有3枚硬币,分别记作A,B,C。这些硬币正面出现的概率分别为<span class="mathjax-exps">$&#x5C;pi,p,q$</span>。进行如下试验:<br>
先抛出硬币A,根据其结果选出硬币B或C,正面选B,反面选C<br>
然后抛出选中的硬币,如果出现正面,记作1,反面记作0</p>
<p>现有m次试验的结果<span class="mathjax-exps">$(Z,Y) = &#x5C;{(z_1,y_1),(z_2,y_2),&#x5C;ldots,(z_m,y_m)&#x5C;}$</span>,<br>
<span class="mathjax-exps">$z_i$</span>表示第i次试验中,抛出硬币A的结果,<span class="mathjax-exps">$y_i$</span>表示第i次试验抛出第二枚硬币的结果,结果正面为1,反面为0<br>
使用极大似然法估计参数<span class="mathjax-exps">$&#x5C;theta = (&#x5C;pi, p,q)^T$</span></p>
<p>一次试验的概率为:<span class="mathjax-exps">$P(y,z|&#x5C;theta) = &#x5C;pi^zp^{zy}(1-p)^{z(1-y)} &#x5C;times (1-&#x5C;pi)^{1-z}q^{(1-z)y}(1-q)^{(1-z)(1-y)}$</span></p>
<p>似然函数为:<br>
<span class="mathjax-exps">$&#x5C;begin{align}L(&#x5C;theta|Y,Z) &amp;= &#x5C;displaystyle{&#x5C;prod_{i = 1}^m} P(y_i,z_i|&#x5C;theta) &#x5C;&#x5C;&amp;=  &#x5C;pi^{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m} z_i}p^{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_iy_i}(1-p)^{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_i(1-y_i)}&#x5C;&#x5C;&amp;&#x5C;quad&#x5C;times (1-&#x5C;pi)^{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}(1-z_i)}q^{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}(1-z_i)y_i}(1-q)^{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}(1-z_i)(1-y_i)}&#x5C;end{align}$</span></p>
<p>为简化运算,取对数形式:<br>
<span class="mathjax-exps">$&#x5C;begin{align}LL(&#x5C;theta|Y,Z) &amp;= &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(y_i,z_i|&#x5C;theta) &#x5C;&#x5C;&amp;= &#x5C;bigg (&#x5C;displaystyle {&#x5C;sum_{i = 1}^m} z_i&#x5C;bigg ) &#x5C;ln &#x5C;pi +&#x5C;bigg (&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_iy_i&#x5C;bigg ) &#x5C;ln p +&#x5C;bigg (&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_i(1-y_i)&#x5C;bigg ) &#x5C;ln (1-p) &#x5C;&#x5C;&amp;&#x5C;quad +&#x5C;bigg (&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}(1-z_i)&#x5C;bigg ) &#x5C;ln (1-&#x5C;pi) +&#x5C;bigg (&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}(1-z_i)y_i&#x5C;bigg ) &#x5C;ln q +&#x5C;bigg (&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}(1-z_i)(1-y_i)&#x5C;bigg ) &#x5C;ln (1-q)&#x5C;end{align}$</span></p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial LL(&#x5C;theta|Y,Z)}{&#x5C;partial &#x5C;theta} &amp;= &#x5C;begin{bmatrix}&#x5C;frac{&#x5C;partial LL(&#x5C;theta|Y,Z)}{&#x5C;partial &#x5C;pi}  &#x5C;&#x5C;&#x5C;frac{&#x5C;partial LL(&#x5C;theta|Y,Z)}{&#x5C;partial p}  &#x5C;&#x5C;&#x5C;frac{&#x5C;partial LL(&#x5C;theta|Y,Z)}{&#x5C;partial q}&#x5C;end{bmatrix} &#x5C;&#x5C;&amp;= &#x5C;begin{bmatrix}&#x5C;frac{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m} z_i}{&#x5C;pi} - &#x5C;frac{&#x5C;displaystyle{&#x5C;sum_{i = 1}^m} (1 - z_i)}{1 - &#x5C;pi}  &#x5C;&#x5C;&#x5C;frac{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_iy_i}{p} - &#x5C;frac{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_i(1-y_i)}{1-p}&#x5C;&#x5C;&#x5C;frac{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_iy_i}{q} - &#x5C;frac{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m}z_i(1-y_i)}{1-q}&#x5C;end{bmatrix}&#x5C;end{align}$</span></p>
<p>令<span class="mathjax-exps">$&#x5C;frac{&#x5C;partial LL(&#x5C;theta|Y,Z)}{&#x5C;partial &#x5C;theta}=0$</span><br>
得:<br>
<span class="mathjax-exps">$&#x5C;pi = &#x5C;frac{ &#x5C;displaystyle {&#x5C;sum_{i = 1}^m} z_i}{m}$</span><br>
<span class="mathjax-exps">$p = &#x5C;frac{ &#x5C;displaystyle {&#x5C;sum_{i = 1}^m} z_iy_i}{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m} z_i}$</span><br>
<span class="mathjax-exps">$q = &#x5C;frac{ &#x5C;displaystyle {&#x5C;sum_{i = 1}^m}  (1-z_i)y_i}{&#x5C;displaystyle {&#x5C;sum_{i = 1}^m} (1-z_i)}$</span></p>
<h2 class="mume-header" id="em&#x7B97;&#x6CD5;&#x7684;&#x5177;&#x4F53;&#x6B65;&#x9AA4;-&#x548C;-&#x9690;&#x53D8;&#x91CF;">EM算法的具体步骤 和 隐变量</h2>

<p>原log似然函数:<span class="mathjax-exps">$LL(&#x5C;theta|Y,Z) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(y_i,z_i|&#x5C;theta)$</span><br>
上面的例子中,隐变量或者说中间变量<span class="mathjax-exps">$z_i$</span>是已知的, 但<span class="mathjax-exps">$z_i$</span>如果是未知的,那么我们要怎么进行参数的估计?</p>
<p>按照上面的过程,极大似然法无法进行下去,因为我们并不知道输出的结果<span class="mathjax-exps">$y_i$</span>到底是由B硬币生成的,还是C硬币生成的。</p>
<p>使用极大似然法,在完整数据求似然函数的时候,需要知道<span class="mathjax-exps">$y_i$</span>和<span class="mathjax-exps">$z_i$</span>(即样本的生成过程信息),但我们不知道<span class="mathjax-exps">$z_i$</span>到底是多少,所以极大似然法无法进行下去。</p>
<p>在不完全数据下的计算过程:<br>
似然函数为:<span class="mathjax-exps">$LL(X|&#x5C;theta) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln &#x5C;displaystyle{&#x5C;sum_z} P(X,z|&#x5C;theta)$</span><br>
令这个似然函数的导数为0,并不能得到一个解析解。(PRML说的,但至少足够难运算)</p>
<p>那么有什么方法,既可以避开<span class="mathjax-exps">$z_i$</span>真实值的问题,而又能使得极大似然法得以进行下去?<br>
一个简单地想法就是使用<span class="mathjax-exps">$z_i$</span>的期望,或者是<span class="mathjax-exps">$z_i$</span>的概率取值来代替真实值。<br>
这样我们就能近似得到<span class="mathjax-exps">$Z$</span>的值,使得可以使用完全数据下的似然函数的方法来计算下去。</p>
<p>注意是<span class="mathjax-exps">$P(X,Z) = &#x5C;displaystyle{&#x5C;sum_z} q(z)P(X,z)$</span><br>
或者是<span class="mathjax-exps">$P(X,Z) = P(X,E(Z))$</span><br>
而不是指 <span class="mathjax-exps">$P(X) = &#x5C;displaystyle{&#x5C;sum_z} P(z)P(X|z)$</span></p>
<p>那么<span class="mathjax-exps">$z_i$</span>怎么取?<br>
这里我们采用第一种策略,并且取<span class="mathjax-exps">$z_i$</span>的概率分布为其后验概率。(后面会证明只能是后验概率。)</p>
<p>如果我们知道<span class="mathjax-exps">$&#x5C;theta$</span>的值,那么我们可以轻易地得到<span class="mathjax-exps">$z_i$</span>的分布,如果我们知道<span class="mathjax-exps">$z_i$</span>的值,那么也可以使用极大似然法直接得到<span class="mathjax-exps">$&#x5C;theta$</span>的参数估计。</p>
<p>EM算法就是使用通过迭代上面提到的两个步骤来实现参数的估计的。</p>
<p>也就是说EM算法取当前<span class="mathjax-exps">$&#x5C;theta^{(t)}$</span>下<span class="mathjax-exps">$z_i$</span>的后验概率作为<span class="mathjax-exps">$z_i$</span>的概率分布。(具体表示见后面的表达式。)<br>
然后在<span class="mathjax-exps">$z_i$</span>的后验概率下,计算<span class="mathjax-exps">$&#x5C;theta^{(t+1)}$</span></p>
<p><s>EM算法的想法就是使用隐变量<span class="mathjax-exps">$z_i$</span>的概率取值来代替<span class="mathjax-exps">$z_i$</span>的真实值(每个取值,以其后验概率作为权重),并且使用 <strong>似然函数的期望</strong> 来代替 <strong>似然函数</strong></s><br>
<s>具体表现为:使用<span class="mathjax-exps">$z_i$</span>的后验概率<span class="mathjax-exps">$P(z_i|y_i,&#x5C;theta)$</span>,来猜测<span class="mathjax-exps">$z_i$</span>,这样就避开的<span class="mathjax-exps">$z_i$</span>的实际取值为多少。</s><br>
<s>具体含义见期望的表达式(关于期望见后面的详细信息)。</s><br>
<s>这样就避开的z的真实取值。</s></p>
<p>基于上面规避<span class="mathjax-exps">$z_i$</span>的方法,<br>
我们得到:<br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;theta^{(t+1)} &amp;= &#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} LL(&#x5C;theta|Y,Z) &#x5C;&#x5C;&amp;=&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(y_i,z_i|&#x5C;theta) &amp;(1)&#x5C;&#x5C;&amp;&#x5C;dot{=} &#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;displaystyle{&#x5C;sum_{z=0}^1} q(z) &#x5C;ln P(y_i,z|&#x5C;theta) &amp;(2)&#x5C;&#x5C;&amp;=&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} &#x5C;displaystyle{&#x5C;sum_{z=0}^1}&#x5C;displaystyle{&#x5C;sum_{i = 1}^m}  q(z) &#x5C;ln P(y_i,z|&#x5C;theta)&amp;(3)&#x5C;&#x5C;&amp;=&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} &#x5C;displaystyle{&#x5C;sum_{z=0}^1} P(z|y_i,&#x5C;theta^{(t)})&#x5C;bigg(&#x5C;displaystyle{&#x5C;sum_{i = 1}^m}   &#x5C;ln P(y_i,z|&#x5C;theta)&#x5C;bigg)&amp;(4)&#x5C;&#x5C;&amp;=&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;bigg(LL(&#x5C;theta|Y,Z)&#x5C;bigg) &amp;(5)&#x5C;end{align}$</span></p>
<p>(1):完整数据下的似然函数<br>
(2):使用z的概率取值来近似<br>
(3):调整一下两个求和符号的位置<br>
(4):<span class="mathjax-exps">$q(Z)$</span>取值为<span class="mathjax-exps">$Z$</span>的后验概率<br>
(5):上面的形式,可以理解成似然函数的期望</p>
<blockquote>
<p>期望:<span class="mathjax-exps">$E&#x5C;bigg(&#x5C;ln&#x5C;big(g(x)&#x5C;big)&#x5C;bigg) = &#x5C;displaystyle{&#x5C;sum_z}P(z)&#x5C;ln&#x5C;big(g(x)&#x5C;big)$</span></p>
</blockquote>
<p>即从<span class="mathjax-exps">$&#x5C;theta^{(t+1)} =&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} LL(&#x5C;theta|Y)$</span><br>
变成了<br>
<span class="mathjax-exps">$&#x5C;theta^{(t+1)} =&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;bigg(LL(&#x5C;theta|Y,Z)&#x5C;bigg)$</span></p>
<p>后面证明两者是等价的,我们先看EM算法的具体过程:<br>
<span class="mathjax-exps">$&#x5C;begin{align}E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;bigg(LL(&#x5C;theta|Y,Z)&#x5C;bigg)&amp; = E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;bigg( &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(y_i,z_i|&#x5C;theta)&#x5C;bigg)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{i = 1}^m}  &#x5C;displaystyle{&#x5C;sum_{z = 0}^1}P(z|y_i,&#x5C;theta^{(t)})&#x5C;ln P(y_i,z|&#x5C;theta)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{i = 1}^m}  &#x5C;displaystyle{&#x5C;sum_{z = 0}^1}P(z|y_i,&#x5C;theta^{(t)})&#x5C;bigg(z &#x5C;ln &#x5C;pi +zy_i &#x5C;ln p +z(1-y_i) &#x5C;ln (1-p) +(1-z) &#x5C;ln (1-&#x5C;pi) +(1-z)y_i &#x5C;ln q +(1-z)(1-y_i) &#x5C;ln (1-q) &#x5C;bigg)&#x5C;end{align}$</span></p>
<p>因为我们不知道<span class="mathjax-exps">$z_i$</span>到底是多少,因此我们从概率的角度来考虑,我们列举了<span class="mathjax-exps">$z_i$</span>的所有可能值,使用后验概率作为其权重。</p>
<p><s>注意:在EM算法中,<span class="mathjax-exps">$p(z|y_i,&#x5C;theta^{(t)})$</span>和参数<span class="mathjax-exps">$&#x5C;theta$</span>有关,但是把<span class="mathjax-exps">$p(z|y_i,&#x5C;theta^{(t)})$</span>这个量当作一个常量,具体原因见后面EM算法迭代的过程及其证明。我们先继续沿着极大似然法这个思路算下去。</s></p>
<p>为方便,记<span class="mathjax-exps">$P(z|y_i,&#x5C;theta^{(t)}) = &#x5C;lambda_{zi}$</span></p>
<p>令<span class="mathjax-exps">$&#x5C;frac{&#x5C;partial E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;big(LL(&#x5C;theta|Y,Z)&#x5C;big)}{&#x5C;partial &#x5C;theta} = 0$</span>(有约束条件,则使用其拉格朗日形式)<br>
得:<br>
<span class="mathjax-exps">$&#x5C;pi = &#x5C;frac{ &#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;displaystyle{&#x5C;sum_{z = 0}^1} z&#x5C;lambda_{zi}}{&#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;displaystyle{&#x5C;sum_{z = 0}^1} &#x5C;lambda_{zi}}$</span></p>
<p><span class="mathjax-exps">$p = &#x5C;frac{ &#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;displaystyle{&#x5C;sum_{z = 0}^1} zy_i&#x5C;lambda_{zi}}{&#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;displaystyle{&#x5C;sum_{z = 0}^1} z&#x5C;lambda_{zi}}$</span></p>
<p><span class="mathjax-exps">$q = &#x5C;frac{ &#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;displaystyle{&#x5C;sum_{z = 0}^1} (1-z)y_i&#x5C;lambda_{zi}}{&#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;displaystyle{&#x5C;sum_{z = 0}^1} (1-z)&#x5C;lambda_{zi}}$</span></p>
<p><span class="mathjax-exps">$&#x5C;lambda_{zi} = P(z|y_i,&#x5C;theta^{(t)}) = &#x5C;frac{P(y_i,z|&#x5C;theta^{(t)})}{ P(y_i|&#x5C;theta^{(t)})} = &#x5C;frac{P(y_i,z|&#x5C;theta^{(t)})}{&#x5C;displaystyle{&#x5C;sum_{z_j = 0}^1} P(y_i,z_j|&#x5C;theta^{(t)})}$</span></p>
<blockquote>
<p><span class="mathjax-exps">$P(X) = &#x5C;displaystyle{&#x5C;sum_z} P(z)P(X|z) = &#x5C;displaystyle{&#x5C;sum_z} P(X,z)$</span></p>
</blockquote>
<p>EM算法是一个迭代算法<br>
<span class="mathjax-exps">$&#x5C;lambda_{zi}$</span>为当前参数<span class="mathjax-exps">$&#x5C;theta^{(t)}$</span>下<span class="mathjax-exps">$z$</span>的后验概率<br>
上面<span class="mathjax-exps">$&#x5C;pi,p,q$</span>的等式,则是<span class="mathjax-exps">$&#x5C;theta^{(t+1)} = &#x5C;begin{matrix}argmax&#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;big( LL(&#x5C;theta|Y,Z) &#x5C;big)$</span>的解<br>
<s>迭代的过程,使用了上一次迭代的结果<span class="mathjax-exps">$&#x5C;lambda_{zi}$</span><br>
这也解释了为什么把<span class="mathjax-exps">$&#x5C;lambda_{zi}$</span>看作常量,因为这个量来自于上一次迭代输出的参数,而和这一次迭代的需要更新的参数无关。</s></p>
<p>我们整理一下这题EM算法的具体过程:<br>
0. 随机取参数<span class="mathjax-exps">$&#x5C;theta$</span> (对于高斯混合模型 取k-means的结果作为初始参数)<br>
采用k-means的时候,<br>
均值作为初始化的均值,<br>
协方差矩阵初始化为各类的样本的协方差<br>
混合系数初始化为对应类别中的数据点所占的比例</p>
<ol>
<li>
<p>E步:求似然函数期望的表达式<br>
即 计算<span class="mathjax-exps">$z_i$</span>的后验概率 <span class="mathjax-exps">$p(z_i|y_i,&#x5C;theta^{(t)})$</span>(也就是<span class="mathjax-exps">$&#x5C;lambda_{zi}$</span>)</p>
</li>
<li>
<p>M步:更新参数<span class="mathjax-exps">$&#x5C;theta$</span><br>
即 <span class="mathjax-exps">$&#x5C;theta^{(t + 1)} = &#x5C;begin{matrix} argmax &#x5C;&#x5C; &#x5C;theta&#x5C;end{matrix} E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;big( LL(&#x5C;theta|Y,Z) &#x5C;big)$</span></p>
</li>
<li>
<p>重复2,3步骤,直到达到停止条件<br>
停止条件可以是达到最大轮迭代数,期望变化,参数变化达到阈值<br>
<span class="mathjax-exps">$|&#x5C;theta^{(t+1)} - &#x5C;theta^{(t)}| &lt; &#x5C;epsilon_1$</span> 或<span class="mathjax-exps">$&#x5C;bigg|E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;big( LL(&#x5C;theta^{(t+1)}|Y,Z) &#x5C;big) - E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;big( LL(&#x5C;theta^{(t)}|Y,Z) &#x5C;big)&#x5C;bigg|&lt; &#x5C;epsilon_2$</span></p>
</li>
</ol>
<p>疑问:采用上述<span class="mathjax-exps">$z_i$</span>真实值的规避策略,可以有:<br>
<span class="mathjax-exps">$L(&#x5C;theta|Y,Z) = &#x5C;displaystyle{&#x5C;prod_{i = 1}^m} P(y_i,z_i|&#x5C;theta) = &#x5C;displaystyle{&#x5C;prod_{i = 1}^m}  &#x5C;displaystyle{&#x5C;sum_{z = 0}^1}p(z|y_i,&#x5C;theta^{(t)})P(y_i,z)$</span><br>
和<br>
<span class="mathjax-exps">$L(&#x5C;theta|Y,Z) = &#x5C;displaystyle{&#x5C;prod_{i = 1}^m} P(y_i,z_i|&#x5C;theta) = &#x5C;displaystyle{&#x5C;sum_{z = 0}^1}p(z|y_i,&#x5C;theta^{(t)})&#x5C;displaystyle{&#x5C;prod_{i = 1}^m}  P(y_i,z)$</span><br>
两种选择,为什么采用后者?</p>
<p>前者取对数之后为:<span class="mathjax-exps">$LL(&#x5C;theta|Y,Z) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(y_i,z_i|&#x5C;theta) =&#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln &#x5C;displaystyle{&#x5C;sum_{z = 0}^1}p(z|y_i,&#x5C;theta^{(t)})P(y_i,z)$</span></p>
<p>后者取对数之后为::$<span class="mathjax-exps">$LL(&#x5C;theta|Y,Z) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^m} &#x5C;ln P(y_i,z_i|&#x5C;theta) =   &#x5C;displaystyle{&#x5C;sum_{z = 0}^1}p(z|y_i,&#x5C;theta^{(t)})&#x5C;bigg(&#x5C;displaystyle{&#x5C;sum_{i = 1}^m}&#x5C;ln P(y_i,z)&#x5C;bigg)$</span></p>
<p>这里赢是指似然函数和对数似然函数分别进行z的替换之后的结果,而不是似然函数替换z之后再取对数</p>
<p>无论是没取对数(有连乘),还是前者取了对数(有<span class="mathjax-exps">$&#x5C;ln &#x5C;sum$</span>),其求导运算都不好算,而后者取对数之后,则把<span class="mathjax-exps">$&#x5C;ln$</span>放在了<span class="mathjax-exps">$&#x5C;sum$</span>的里面,显然运算更加简单。</p>
<h2 class="mume-header" id="em&#x7B97;&#x6CD5;&#x7684;&#x8BC1;&#x660E;">EM算法的证明</h2>

<p>为什么EM算法这样迭代是有效的?<br>
<span class="mathjax-exps">$&#x5C;begin{align}LL(&#x5C;theta|Y) = &#x5C;ln P(Y|&#x5C;theta) &amp;= &#x5C;ln&#x5C;frac{ P(Y,Z|&#x5C;theta)}{P(Z|Y,&#x5C;theta)} &amp;,贝叶斯定理&#x5C;&#x5C;&#x5C;&#x5C;&amp;= &#x5C;ln P(Y,Z|&#x5C;theta) - &#x5C;ln P(Z|,Y,&#x5C;theta) &#x5C;&#x5C;&#x5C;&#x5C;&#x5C;displaystyle{&#x5C;sum_z}P(Z|Y,&#x5C;theta^{(t)}) ln P(Y|&#x5C;theta)&amp;= &#x5C;displaystyle{&#x5C;sum_z}P(Z|Y,&#x5C;theta^{(t)})&#x5C;ln P(Y,Z|&#x5C;theta) - &#x5C;displaystyle{&#x5C;sum_z}P(Z|Y,&#x5C;theta^{(t)})&#x5C;ln P(Z|,Y,&#x5C;theta)&amp; ,(1)&#x5C;&#x5C;&#x5C;&#x5C;ln P(Y|&#x5C;theta)&amp;= Q(&#x5C;theta|&#x5C;theta^{t}) +  H(&#x5C;theta|&#x5C;theta^{t})&amp;,(2)&#x5C;end{align}$</span></p>
<p>(1):等号两边乘上一个系数,这个系数值为1,即<span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_z}P(Z|Y,&#x5C;theta^{(t)}) = 1$</span><br>
(2):为方便,定义两个符号</p>
<p>令<span class="mathjax-exps">$&#x5C;theta = &#x5C;theta^{(t)}$</span><br>
有<span class="mathjax-exps">$&#x5C;ln P(Y|&#x5C;theta^{(t)}) = Q(&#x5C;theta^{(t)}|&#x5C;theta^{(t)}) +  H(&#x5C;theta^{(t)}|&#x5C;theta^{(t)})$</span></p>
<p><span class="mathjax-exps">$Q(&#x5C;theta|&#x5C;theta^{(t)})$</span>就是<span class="mathjax-exps">$E_{Z|Y,&#x5C;theta^{(t)}}&#x5C;big( LL(&#x5C;theta|Y,Z) &#x5C;big)$</span></p>
<p>则上面两个方程式相减有:<br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;ln P(Y|&#x5C;theta) - &#x5C;ln P(Y|&#x5C;theta^{(t))}) &amp;= &#x5C;bigg (Q(&#x5C;theta|&#x5C;theta^{t})  - Q(&#x5C;theta^{t}|&#x5C;theta^{t}) &#x5C;bigg) + &#x5C;bigg(H(&#x5C;theta|&#x5C;theta^{t})  - H(&#x5C;theta^{t}|&#x5C;theta^{t})&#x5C;bigg) &#x5C;&#x5C;&amp;&#x5C;geq &#x5C;bigg(H(&#x5C;theta|&#x5C;theta^{t})  - H(&#x5C;theta^{t}|&#x5C;theta^{t})&#x5C;bigg) &#x5C;&#x5C;&amp;&#x5C;geq 0&#x5C;end{align}$</span></p>
<p>也就是说,EM算法迭代之后,Q的值要么增大,要么不变(不变则说明迭代结束),所以EM算法是有效的。</p>
<p>证明:<span class="mathjax-exps">$Q(&#x5C;theta|&#x5C;theta^{t})  - Q(&#x5C;theta^{t}|&#x5C;theta^{t}) &#x5C;geq 0$</span><br>
<span class="mathjax-exps">$&#x5C;theta^{(t+1)} =&#x5C;begin{matrix}argmax &#x5C;&#x5C;&#x5C;theta&#x5C;end{matrix} Q(&#x5C;theta|&#x5C;theta^{t})$</span><br>
也就是说<span class="mathjax-exps">$Q(&#x5C;theta^{(t+1)}|&#x5C;theta^{t}) = &#x5C;max_&#x5C;theta(Q(&#x5C;theta|&#x5C;theta^{t})) &#x5C;geq Q(&#x5C;theta^{t}|&#x5C;theta^{t})$</span></p>
<p>证明:<span class="mathjax-exps">$H(&#x5C;theta|&#x5C;theta^{t})  - H(&#x5C;theta^{t}|&#x5C;theta^{t}) &#x5C;geq 0$</span><br>
<span class="mathjax-exps">$H(&#x5C;theta|&#x5C;theta^{t}) = - &#x5C;displaystyle{&#x5C;sum_z}P(Z|Y,&#x5C;theta^{(t)})&#x5C;ln P(Z|,Y,&#x5C;theta)$</span><br>
由吉布斯不等式(<strong>gibbs' inequality</strong>),<span class="mathjax-exps">$&#x5C;bigg(H(&#x5C;theta|&#x5C;theta^{t})  - H(&#x5C;theta^{t}|&#x5C;theta^{t})&#x5C;bigg) &#x5C;geq 0$</span>,</p>
<blockquote>
<p>吉布斯不等式(<strong>gibbs' inequality</strong>)<br>
若<span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_i} p_i = &#x5C;displaystyle{&#x5C;sum_i} q_i = 1,且p_i,q_i &#x5C;in (0,1],则 -&#x5C;displaystyle{&#x5C;sum_i} p_i&#x5C;ln p_i &#x5C;leq -&#x5C;displaystyle{&#x5C;sum_i} p_i &#x5C;ln q_i$</span>,当且仅当<span class="mathjax-exps">$p_i = q_i$</span>时等式成立</p>
</blockquote>
<p>这里要求<span class="mathjax-exps">$p_i = q_i$</span>,否则无法比较<span class="mathjax-exps">$H(&#x5C;theta|&#x5C;theta^{t})$</span>和<span class="mathjax-exps">$H(&#x5C;theta^{t}|&#x5C;theta^{t})$</span>的大小,这也解释了为什么选择z的后验概率,而不是其他。</p>
<p>从整体上来看,只有<span class="mathjax-exps">$&#x5C;theta = &#x5C;theta{(t)}$</span>的时候,整个式子才能取等号。<br>
因此随着迭代,似然函数的值不断变大(除非参数不再变化,收敛)</p>
<p>图示理解:<br>
<img src="assets/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-EM%E7%AE%97%E6%B3%95-e8373.png" alt="1"></p>
<p>PRML上的证明类似,只不过构造的函数形式不太一样。</p>
<ul>
<li><input type="checkbox" class="task-list-item-checkbox"> : PRML上的证明</li>
</ul>
<h2 class="mume-header" id="&#x5176;&#x4ED6;">其他</h2>

<p>EM算法可能走到鞍点,故需要多次进行,<br>
也就是说EM算法对初始值敏感,常用的做法是使用k-means的结果作为初始值。</p>
<p>EM算法实际上有两种:</p>
<ol>
<li>
<p>soft EM<br>
也就是前面我们用的,枚举所有的z,以其后验概率作为每个z的权重,综合考虑</p>
</li>
<li>
<p>hard EM<br>
<span class="mathjax-exps">$z_i$</span>未知,则取<span class="mathjax-exps">$z_i$</span>的期望作为真实值的近似。(对于离散型不可用,因为期望这个值不一定在可选范围内,比如前面的例子,可选择的值只有0和1,而期望则不是0或者1)</p>
</li>
</ol>
<p>em算法有3个关键的地方:</p>
<ol>
<li>
<p>使用Z的概率取值(或者期望)来代替Z的真实值,这样就避免了Z真实值的需求<br>
注意是<span class="mathjax-exps">$P(X,Z) = &#x5C;displaystyle{&#x5C;sum_z} q(z)P(X,z)$</span><br>
或者是<span class="mathjax-exps">$P(X,Z) = P(X,E(Z))$</span><br>
而不是 <span class="mathjax-exps">$P(X) = &#x5C;displaystyle{&#x5C;sum_z} q(z)P(X|z)$</span></p>
</li>
<li>
<p>使用z的后验概率而不是其他</p>
</li>
<li>
<p>迭代求解</p>
</li>
</ol>
<h2 class="mume-header" id="em&#x7B97;&#x6CD5;&#x7684;&#x5E94;&#x7528;-&#x9AD8;&#x65AF;&#x6DF7;&#x5408;&#x805A;&#x7C7B;">EM算法的应用-高斯混合聚类</h2>

<p>高斯分布的定义:<br>
对n维样本空间X中的随机向量x，若x服从高斯分布，其概率密度函数为：<br>
<span class="mathjax-exps">$p(&#x5C;boldsymbol x) = &#x5C;frac{1}{(2&#x5C;pi)^&#x5C;frac{n}{2}|&#x5C;Sigma|^&#x5C;frac{1}{2}} e^{ -&#x5C;frac{1}{2}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu)^T&#x5C;Sigma^{-1}(&#x5C;boldsymbol x - &#x5C;boldsymbol &#x5C;mu) }$</span><br>
其中<span class="mathjax-exps">$&#x5C;boldsymbol &#x5C;mu,&#x5C;boldsymbol x$</span>为n维均值向量，<span class="mathjax-exps">$&#x5C;Sigma$</span>是<span class="mathjax-exps">$n&#x5C;times n$</span>的协方差矩阵,<span class="mathjax-exps">$|&#x5C;Sigma|$</span>为<span class="mathjax-exps">$&#x5C;Sigma$</span>的行列式<br>
为明确显示高斯分布与相应参数的依赖关系，将概率密度函数记为<span class="mathjax-exps">$p(&#x5C;boldsymbol x | &#x5C;boldsymbol &#x5C;mu,&#x5C;Sigma) = p(&#x5C;boldsymbol x)$</span></p>
<p>高斯混合分布的定义:<br>
高斯混合分布：<span class="mathjax-exps">$p_{&#x5C;mathcal{M}}(x) = &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;alpha_i &#x5C;cdot p(&#x5C;boldsymbol x | &#x5C;boldsymbol &#x5C;mu_i,&#x5C;Sigma_i)$</span><br>
该分布由k个混合成分组成，每个混合成分对应一个高斯分布。其中<span class="mathjax-exps">$&#x5C;mu_i$</span>和<span class="mathjax-exps">$&#x5C;Sigma_i$</span>为第i个高斯混合成分的参数，而<span class="mathjax-exps">$&#x5C;alpha_i&gt;0$</span>为相应的 <strong>混合系数(mixture coefficient)</strong> ,且<span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;alpha_i = 1$</span></p>
<p>高斯混合分布样本的生成过程:<br>
首先,根据<span class="mathjax-exps">$&#x5C;alpha_1,&#x5C;alpha_2,&#x5C;ldots,&#x5C;alpha_k$</span>定义的先验分布选择高斯混合成分,其中<span class="mathjax-exps">$&#x5C;alpha_i$</span>为第i个混合成分的概率;<br>
然后,根据被选择的混合成分的概率密度进行采样,从而生成相应的样本。</p>
<p>定义:<br>
样本:<span class="mathjax-exps">$X = &#x5C;{x_1,x_2,&#x5C;ldots,x_m&#x5C;}$</span>,<span class="mathjax-exps">$x_i$</span>表示第i个样本的值<br>
隐变量:<span class="mathjax-exps">$Z = &#x5C;{z_1,z_2,&#x5C;ldots,z_m&#x5C;}$</span>,<span class="mathjax-exps">$z_i$</span>表示第i个样本真实来自的混合高斯成分。<br>
参数:<span class="mathjax-exps">$&#x5C;theta_i = (&#x5C;alpha_i,&#x5C;mu_i,&#x5C;Sigma_i)$</span>:表示第i个高斯混合成分的参数。</p>
<p>使用下标 <strong>M</strong> 以区别高斯混合模型和高斯混合成分</p>
<p>在已知<span class="mathjax-exps">$X$</span>和<span class="mathjax-exps">$Z$</span>的情况,其概率为:<span class="mathjax-exps">$P_&#x5C;mathcal{M}(x,z|&#x5C;theta) = &#x5C;alpha_zP(x|&#x5C;theta) = &#x5C;alpha_zP(x|&#x5C;mu,&#x5C;Sigma)$</span><br>
其似然函数为:<span class="mathjax-exps">$LL(&#x5C;theta|X,Z) = &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} P_&#x5C;mathcal{M}(x_j,z_j|&#x5C;theta_j)$</span></p>
<p><span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} p_{&#x5C;mathcal{M}}(z_j = l|x_j,&#x5C;theta^{(t)}) &#x5C;ln P_{&#x5C;mathcal{M}}(x_j.z_j = l|&#x5C;theta_l)$</span></p>
<p>期望为:<span class="mathjax-exps">$&#x5C;begin{align}E_{Z|X,&#x5C;theta^{(t)}}&#x5C;big(LL(&#x5C;theta|X,Z)&#x5C;big) &amp;=  &#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  &#x5C;ln P_{&#x5C;mathcal{M}}(x_j,z_j|&#x5C;theta_j) &amp;(1)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} p_{&#x5C;mathcal{M}}(z_j = l|x_j,&#x5C;theta^{(t)}) &#x5C;ln P_{&#x5C;mathcal{M}}(x_j,z_j = l|&#x5C;theta_l) &amp;(2)&#x5C;end{align}$</span><br>
(1)是定义<br>
(2)是使用期望的公式展开隐变量,使用后验概率作为每个隐变量取值的权重</p>
<p>隐变量z的后验概率为:<span class="mathjax-exps">$&#x5C;begin{align}p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)}) &amp;= &#x5C;frac{ p_{&#x5C;mathcal{M}}(z_j = i,x_j|&#x5C;theta^{(t)})}{p_{&#x5C;mathcal{M}}(x_j|&#x5C;theta^{(t)})} &amp;(1) &#x5C;&#x5C;&amp;= &#x5C;frac{ p_{&#x5C;mathcal{M}}(z_j = i,x_j|&#x5C;theta^{(t)})}{&#x5C;displaystyle{&#x5C;sum_{l = 1}^k}p_{&#x5C;mathcal{M}}(x_j,z_j = l|&#x5C;theta^{(t)})} &amp;(2)&#x5C;&#x5C;&amp;= &#x5C;frac{	&#x5C;alpha_ip(x_j|&#x5C;mu_i,&#x5C;Sigma_i)}{&#x5C;displaystyle{&#x5C;sum_{l = 1}^k} &#x5C;alpha_lP(x_j|&#x5C;mu_l,&#x5C;Sigma_l)}&#x5C;end{align}$</span><br>
(1)贝叶斯公式<br>
(2)隐变量z使用其概率分布展开</p>
<p>则<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial LL(&#x5C;theta|X,Z)}{&#x5C;partial &#x5C;mu_i}&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;displaystyle{&#x5C;sum_{l = 1}^k}  p_{&#x5C;mathcal{M}}(z_j = l|x_j,&#x5C;theta^{(t)})&#x5C;frac{&#x5C;alpha_l}{P_{&#x5C;mathcal{M}}(x_j,z_j = l|&#x5C;theta_l)} &#x5C;cdot&#x5C;frac{&#x5C;partial P(x_j|&#x5C;mu_l,&#x5C;Sigma_l) }{&#x5C;partial &#x5C;mu_i} &amp;(1)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)})&#x5C;frac{&#x5C;alpha_i}{P_{&#x5C;mathcal{M}}(x_j,z_j = i|&#x5C;theta_i)} &#x5C;cdot&#x5C;frac{&#x5C;partial P(x_j|&#x5C;mu_i,&#x5C;Sigma_i) }{&#x5C;partial &#x5C;mu_i} &amp;(2)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)})&#x5C;frac{&#x5C;alpha_i}{P_{&#x5C;mathcal{M}}(x_j,z_j = i|&#x5C;theta_i)} &#x5C;cdot P(x_j|&#x5C;mu_i,&#x5C;Sigma_i)&#x5C;Sigma_{-1}(x_j-&#x5C;mu_i) &amp;(3) &#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)}) &#x5C;cdot&#x5C;Sigma_{-1}(x_j-&#x5C;mu_i) &amp;(4)&#x5C;&#x5C;&amp;= 0&#x5C;end{align}$</span></p>
<p>得:<span class="mathjax-exps">$&#x5C;mu_i = &#x5C;frac{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)})x_j }{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)}) }$</span></p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial LL(&#x5C;theta|X,Z)}{&#x5C;partial &#x5C;Sigma_i}&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;displaystyle{&#x5C;sum_{l = 1}^k}  p_{&#x5C;mathcal{M}}(z_j = l|x_j,&#x5C;theta^{(t)})&#x5C;frac{&#x5C;alpha_l}{P_{&#x5C;mathcal{M}}(x_j,z_j = l|&#x5C;theta_l)} &#x5C;cdot&#x5C;frac{&#x5C;partial P(x_j|&#x5C;mu_l,&#x5C;Sigma_l) }{&#x5C;partial &#x5C;Sigma_i} &amp;(1)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)})&#x5C;frac{&#x5C;alpha_i}{P_{&#x5C;mathcal{M}}(x_j,z_j = i|&#x5C;theta_i)} &#x5C;cdot&#x5C;frac{&#x5C;partial P(x_j|&#x5C;mu_i,&#x5C;Sigma_i) }{&#x5C;partial &#x5C;Sigma_i} &amp;(2)&#x5C;&#x5C;&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)})&#x5C;frac{&#x5C;alpha_i}{P_{&#x5C;mathcal{M}}(x_j,z_j = i|&#x5C;theta_i)} &#x5C;cdot(&#x5C;frac{1}{2})[(&#x5C;Sigma_i^{-1})^T - (x_j - &#x5C;mu_i)(x_j - &#x5C;mu_i)^T&#x5C;Sigma_i^{-2}] &amp;(3)&#x5C;&#x5C;&amp; = 0&#x5C;end{align}$</span></p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial p(x_j|&#x5C;mu_i,&#x5C;Sigma_i)}{&#x5C;partial &#x5C;Sigma_i}&amp;= &#x5C;frac{&#x5C;partial &#x5C;frac{1}{t}e^p}{&#x5C;partial &#x5C;Sigma_i} &amp;(1)&#x5C;&#x5C;&amp;= e^p&#x5C;frac{&#x5C;partial &#x5C;frac{1}{t}}{&#x5C;partial &#x5C;Sigma_i} + &#x5C;frac{1}{t}&#x5C;frac{&#x5C;partial e^p}{&#x5C;partial &#x5C;Sigma_i} &amp;(2)&#x5C;&#x5C;&amp;=e^p&#x5C;frac{1}{(2&#x5C;pi)^{&#x5C;frac{&#x5C;pi}{2}}} (-&#x5C;frac{1}{2})|&#x5C;Sigma_i|^{-&#x5C;frac{3}{2}}&#x5C;frac{&#x5C;partial |&#x5C;Sigma_i|}{&#x5C;partial &#x5C;Sigma_i} + e^p (-&#x5C;frac{1}{2})&#x5C;frac{&#x5C;partial (x_j - &#x5C;mu_i)^T&#x5C;Sigma_i^{-1}(X_j - &#x5C;mu_i)} {&#x5C;partial &#x5C;Sigma_i}&amp;(3)&#x5C;&#x5C;&amp;= p(x_j|&#x5C;mu_i,&#x5C;Sigma_i)((E^{-1})^T - (x_j - &#x5C;mu_i)(x_j - &#x5C;mu_i)^T&#x5C;Sigma_i^{-2})&amp;(4)&#x5C;&#x5C;&#x5C;end{align}$</span></p>
<p><span class="mathjax-exps">$&#x5C;frac{&#x5C;partial |&#x5C;Sigma_i|}{&#x5C;partial &#x5C;Sigma_i} = |&#x5C;Sigma_i|(&#x5C;Sigma_i^{-1})^T$</span></p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial( x_j - &#x5C;mu_i)^T&#x5C;Sigma_i^{-1}(x_j - &#x5C;mu_i)}{ &#x5C;partial}&amp;= &#x5C;frac{&#x5C;partial (x_j - &#x5C;mu_i)^T&#x5C;Sigma_i^{-1}(x_j - &#x5C;mu_i)}{&#x5C;partial &#x5C;Sigma_i^{-1}(x_j - &#x5C;mu_i)}&#x5C;frac{&#x5C;partial &#x5C;Sigma_i^{-1}(x_j - &#x5C;mu_i)}{&#x5C;partial &#x5C;Sigma_i^{-1}}&#x5C;frac{&#x5C;partial &#x5C;Sigma_i^{-1}}{&#x5C;partial &#x5C;Sigma_i} &amp;(1)&#x5C;&#x5C;&amp;=(x_j - &#x5C;mu_i)&#x5C;frac{&#x5C;partial &#x5C;Sigma_i^{-1}(x_j - &#x5C;mu_i)}{&#x5C;partial &#x5C;Sigma_i^{-1}}&#x5C;frac{&#x5C;partial &#x5C;Sigma_i^({-1}}{&#x5C;partial &#x5C;Sigma_i} &amp;(2)&#x5C;&#x5C;&amp;=(x_j - &#x5C;mu_i)(x_j-&#x5C;mu_i)^T&#x5C;frac{&#x5C;partial &#x5C;Sigma_i^({-1}}{&#x5C;partial &#x5C;Sigma_i} &amp;(3)&#x5C;&#x5C;&amp;=(x_j - &#x5C;mu_i)(x_j-&#x5C;mu_i)^T(-1)&#x5C;Sigma_i^{-2}&#x5C;end{align}$</span></p>
<p>得:<br>
<span class="mathjax-exps">$&#x5C;begin{align}(&#x5C;Sigma_i^{-1})^T&#x5C;Sigma_i^2 &amp;= &#x5C;Sigma_i &amp;(1)&#x5C;&#x5C;&amp;=&#x5C;frac{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)})(x_j - &#x5C;mu_i)(x_j - &#x5C;mu_i)^T }{&#x5C;displaystyle{&#x5C;sum_{j = 1}^m}  p_{&#x5C;mathcal{M}}(z_j = i|x_j,&#x5C;theta^{(t)}) }&amp;(2)&#x5C;end{align}$</span></p>
<p>(1):<span class="mathjax-exps">$&#x5C;Sigma_i$</span>是对称矩阵(<span class="mathjax-exps">$A^T = A$</span>),并且<span class="mathjax-exps">$(A^{-1})^T = (A^T)^{-1}$</span><br>
(2):参数<span class="mathjax-exps">$&#x5C;mu_i$</span>是更新之后的值,而不是原来的值。</p>
<p><span class="mathjax-exps">$&#x5C;begin{align}&#x5C;frac{&#x5C;partial &#x5C;bigg(LL(&#x5C;theta|X,Z) + &#x5C;lambda(&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;alpha_i - 1)&#x5C;bigg)}{&#x5C;partial &#x5C;alpha_i}&amp;= &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} &#x5C;displaystyle{&#x5C;sum_{l = 1}^k} P_&#x5C;mathcal{M}(z_j = l | x_j,&#x5C;theta^{(t)}) &#x5C;frac{p(x_j|&#x5C;mu_i,&#x5C;Sigma_i)}{P_&#x5C;mathcal{M}(x_j,z_j = l |&#x5C;theta&#x5C;_l)}&#x5C;frac{&#x5C;partial &#x5C;alpha_l}{&#x5C;partial &#x5C;alpha_i} + &#x5C;lambda &amp;(1)&#x5C;&#x5C;&amp;=&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} P_&#x5C;mathcal{M}(z_j = i | x_j,&#x5C;theta^{(t)}) &#x5C;frac{p(x_j|&#x5C;mu_i,&#x5C;Sigma_i)}{P_&#x5C;mathcal{M}(x_j,z_j = i |&#x5C;theta&#x5C;_i)} + &#x5C;lambda &amp;(2)&#x5C;&#x5C;&amp;= 0&#x5C;end{align}$</span></p>
<p>等号两边同乘<span class="mathjax-exps">$&#x5C;alpha_i$</span>,得:<span class="mathjax-exps">$&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} P_&#x5C;mathcal{M}(z_j = i | x_j,&#x5C;theta^{(t)}) + &#x5C;lambda &#x5C;alpha_i = 0$</span></p>
<p>全部k个表达式求和:得到<br>
<span class="mathjax-exps">$&#x5C;begin{align}&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} P_&#x5C;mathcal{M}(z_j = i | x_j,&#x5C;theta^{(t)}) + &#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;lambda &#x5C;alpha_i &amp;= 0&#x5C;&#x5C;&#x5C;displaystyle{&#x5C;sum_{i = 1}^k} &#x5C;displaystyle{&#x5C;sum_{j = 1}^m} P_&#x5C;mathcal{M}(z_j = i | x_j,&#x5C;theta^{(t)}) +  &#x5C;lambda &amp;= 0&#x5C;&#x5C;m + &#x5C;lambda &amp;= 0 &#x5C;&#x5C;&#x5C;lambda &amp;= -m&#x5C;end{align}$</span></p>
<p>则<span class="mathjax-exps">$&#x5C;alpha_i = &#x5C;frac{1}{m}&#x5C;displaystyle{&#x5C;sum_{j = 1}^m} P_&#x5C;mathcal{M}(z_j = i | x_j,&#x5C;theta^{(t)})$</span></p>
<p>至此,关于高斯混合聚类模型参数的推导完成。</p>
<p>下面只需要进行迭代即可。</p>
<p>EM算法的结果对初始值敏感,对于高斯混合模型,一般先采用k-means算法的结果作为其初始值</p>
<p>如果有k个混合成分,那么就应该有<span class="mathjax-exps">$k!$</span>个等价的解,为什么EM算法只会得到其中一个?<br>
根据参数初始值,一开始就决定了最终会导向哪一个形式的解。</p>

    </body>
    
    
    <script>
(function bindTaskListEvent() {
  var taskListItemCheckboxes = document.body.getElementsByClassName('task-list-item-checkbox')
  for (var i = 0; i < taskListItemCheckboxes.length; i++) {
    var checkbox = taskListItemCheckboxes[i]
    var li = checkbox.parentElement
    if (li.tagName !== 'LI') li = li.parentElement
    if (li.tagName === 'LI') {
      li.classList.add('task-list-item')
    }
  }
}())    
</script>
  </html>