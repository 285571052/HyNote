# 深度神经网络的分布式训练


神经网络的分布式训练可以通过两种方式实现：数据并行化和模型并行化。

> **数据并行化** 的目标是将数据集均等地分配到系统的各个节点（node），其中每个节点都有该神经网络的一个副本及其本地的权重。每个节点都会处理该数据集的一个不同子集并更新其本地权重集。这些本地权重会在整个集群中共享，从而通过一个累积算法计算出一个新的全局权重集。这些全局权重又会被分配至所有节点，然后节点会在此基础上处理下一批数据。
> **模型并行化** 则是通过将该模型的架构切分到不同的节点上来实现训练的分布化。AlexNet [2] 是使用模型并行化的最早期模型之一，其方法是将网络分摊到 2 个 GPU 上以便模型能放入内存中。

- [深度神经网络的分布式训练概述：常用方法和技巧全面总结 - 知乎](https://zhuanlan.zhihu.com/p/48590894?hmsr=toutiao.io&utm_medium=toutiao.io&utm_source=toutiao.io)
- [Spark-mllib源码分析之逻辑回归(Logistic Regression) - JokerDuuuu的博客 - CSDN博客](https://blog.csdn.net/u011724402/article/details/79089257)
